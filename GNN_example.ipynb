{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e9288a-3f21-4855-bcb6-3acc260ccd7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py, os, optuna, torch\n",
    "from scipy.spatial import KDTree\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric as torchg\n",
    "import torch_scatter as torchs\n",
    "\n",
    "import Graph_Functions as graph\n",
    "import Merger_Tree_Script as mts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe1f8e1-e010-48a6-b85f-92103497502b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Walking the merger tree for all boxes in the DREAMS WDM SB4 MW zoom-in suite. SnapNum, SubhaloID, and DescendantID are\n",
    "# required to construct the merger trees. Add any additional tags for data features you want to use. Documentation can\n",
    "# be found at: https://www.tng-project.org/data/docs/specifications/#sec2b\n",
    "\n",
    "boxes = range(30) # For all the boxes, do range(1024)\n",
    "datalist = []\n",
    "for box in boxes:\n",
    "    try:\n",
    "        data = mts.walk_tree(0,\"SnapNum\",\"SubhaloID\",\"DescendantID\")\n",
    "        datalist.append(data)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb65efa-eeb2-45ea-8db8-14ca1fb73408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish path to parameter file\n",
    "\n",
    "param_path = 'WDM_TNG_MW_SB4_parameters.txt'\n",
    "params = []\n",
    "boxes = range(30)\n",
    "for box in boxes:\n",
    "    try:\n",
    "        param = graph.get_params(param_path)[box]\n",
    "        params.append(param)\n",
    "    except:\n",
    "        print(box)\n",
    "\n",
    "params = np.array(params)\n",
    "nparams = graph.norm_params(params)\n",
    "params = nparams[:,0:1] # Change to the column you want to use for parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38f29d0e-7381-413b-8a86-18986f1658eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph objects with the data features and the parameters\n",
    "\n",
    "dataset = []\n",
    "for i in range(len(datalist)):\n",
    "    dataset.append(graph.create_dataset(datalist[i], params[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bd782e0-02a1-40e6-a618-a1e0bb62d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "\n",
    "train_size = 0.8\n",
    "valid_size = 0.1\n",
    "test_size  = 0.1\n",
    "batch_size = 32\n",
    "\n",
    "train_data, valid_data, test_data = graph.split_dataset(dataset, train_size, valid_size, test_size)\n",
    "train_loader = graph.DataLoader(train_data, batch_size=batch_size, shuffle=True) \n",
    "valid_loader = graph.DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = graph.DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec407215-1f3e-4b9b-adcd-902592e50e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters object, GNN object\n",
    "\n",
    "class Hyperparameters():\n",
    "    \"\"\"\n",
    "    This object acts as a container for the hyperparameters that are used during training.\n",
    "    This object is also used to name files that are stored during training and testing.\n",
    "    \"\"\"\n",
    "    def __init__(self, lr, wd, nl, hc, rl, ne, name):\n",
    "        \n",
    "        self.learning_rate = lr\n",
    "        self.weight_decay = wd\n",
    "        self.n_layers = nl\n",
    "        self.hidden_channels = hc\n",
    "        self.r_link = rl\n",
    "        self.n_epochs = ne #set small at first\n",
    "        self.study_name = name\n",
    "        self.outmode = 'cosmo'\n",
    "        self.pred_params = 1\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"lr {self.learning_rate:.2e}; wd {self.weight_decay:.2e}; nl {self.n_layers}; hc {self.hidden_channels}; rl {self.r_link:.2e}\"\n",
    "    \n",
    "    def name_model(self):\n",
    "        return f\"{name}_lr_{self.learning_rate:.2e}_wd_{self.weight_decay:.2e}_nl_{self.n_layers}_hc_{self.hidden_channels}_rl_{self.r_link:.2e}\"\n",
    "\n",
    "class EdgeModel(torch.nn.Module):\n",
    "    def __init__(self, node_in, node_out, edge_in, edge_out, hid_channels, residuals=True, norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residuals = residuals\n",
    "        self.norm = norm\n",
    "\n",
    "        layers = [torch.nn.Linear(node_in*2 + edge_in, hid_channels),\n",
    "                  torch.nn.ReLU(),\n",
    "                  torch.nn.Linear(hid_channels, edge_out)]\n",
    "        \n",
    "        if self.norm:  \n",
    "            layers.append(torchg.nn.LayerNorm(edge_out))\n",
    "\n",
    "        self.edge_mlp = torch.nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, src, dest, edge_attr, u, batch):\n",
    "\n",
    "        out = torch.cat([src, dest, edge_attr], dim=1)\n",
    "        out = self.edge_mlp(out)\n",
    "        if self.residuals:\n",
    "            out = out + edge_attr\n",
    "        return out\n",
    "\n",
    "class NodeModel(torch.nn.Module):\n",
    "    def __init__(self, node_in, node_out, edge_in, edge_out, hid_channels, residuals=True, norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residuals = residuals\n",
    "        self.norm = norm\n",
    "\n",
    "        layers = [torch.nn.Linear(node_in + 3*edge_out + 1, hid_channels),\n",
    "                  torch.nn.ReLU(),\n",
    "                  torch.nn.Linear(hid_channels, node_out)]\n",
    "        \n",
    "        if self.norm:  \n",
    "            layers.append(torchg.nn.LayerNorm(node_out))\n",
    "\n",
    "        self.node_mlp = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        row, col = edge_index\n",
    "        out = edge_attr\n",
    "\n",
    "        # Multipooling layer\n",
    "        out1 = torchs.scatter_add(out, col, dim=0, dim_size=x.size(0))\n",
    "        out2 = torchs.scatter_max(out, col, dim=0, dim_size=x.size(0))[0]\n",
    "        out3 = torchs.scatter_mean(out, col, dim=0, dim_size=x.size(0))\n",
    "        out = torch.cat([x, out1, out2, out3, u[batch]], dim=1)\n",
    "\n",
    "        out = self.node_mlp(out)\n",
    "        if self.residuals:\n",
    "            out = out + x\n",
    "        return out\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, node_features, n_layers, hidden_channels, linkradius, dim_out, only_positions, residuals=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.link_r = linkradius\n",
    "        self.dim_out = dim_out\n",
    "        self.only_positions = only_positions\n",
    "\n",
    "        # Number of input node features (0 if only_positions is used)\n",
    "        node_in = node_features\n",
    "        # Input edge features: |p_i-p_j|, p_i*p_j, p_i*(p_i-p_j)\n",
    "        edge_in = 3\n",
    "        node_out = hidden_channels\n",
    "        edge_out = hidden_channels\n",
    "        hid_channels = hidden_channels\n",
    "        \n",
    "        layers = []\n",
    "\n",
    "        # Encoder graph block\n",
    "        inlayer = torchg.nn.MetaLayer(node_model=NodeModel(node_in, node_out, edge_in, edge_out, hid_channels, residuals=False),\n",
    "                                      edge_model=EdgeModel(node_in, node_out, edge_in, edge_out, hid_channels, residuals=False))\n",
    "\n",
    "        layers.append(inlayer)\n",
    "\n",
    "        # Change input node and edge feature sizes\n",
    "        node_in = node_out\n",
    "        edge_in = edge_out\n",
    "\n",
    "        # Hidden graph blocks\n",
    "        for i in range(n_layers-1):\n",
    "\n",
    "            lay = torchg.nn.MetaLayer(node_model=NodeModel(node_in, node_out, edge_in, edge_out, hid_channels, residuals=residuals),\n",
    "                                      edge_model=EdgeModel(node_in, node_out, edge_in, edge_out, hid_channels, residuals=residuals))\n",
    "            layers.append(lay)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList(layers)\n",
    "\n",
    "        # Final aggregation layer\n",
    "        self.outlayer = torch.nn.Sequential(torch.nn.Linear(3*node_out+1, hid_channels),\n",
    "                              torch.nn.ReLU(),\n",
    "                              torch.nn.Linear(hid_channels, hid_channels),\n",
    "                              torch.nn.ReLU(),\n",
    "                              torch.nn.Linear(hid_channels, hid_channels),\n",
    "                              torch.nn.ReLU(),\n",
    "                              torch.nn.Linear(hid_channels, self.dim_out))\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        h, edge_index, edge_attr, u, batch = data.x, data.edge_index, data.edge_attr, data.u, data.batch\n",
    "\n",
    "        # Message passing layers\n",
    "        for layer in self.layers:\n",
    "            h, edge_attr, _ = layer(h, edge_index, edge_attr, u, data.batch)\n",
    "\n",
    "        # Multipooling layer\n",
    "        addpool = torchg.nn.global_add_pool(h, batch)\n",
    "        meanpool = torchg.nn.global_mean_pool(h, batch)\n",
    "        maxpool = torchg.nn.global_max_pool(h, batch)\n",
    "    \n",
    "        out = torch.cat([addpool,meanpool,maxpool,u], dim=1)\n",
    "\n",
    "        # Final linear layer\n",
    "        out = self.outlayer(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f446d5d-c951-4d32-82d6-63d30ae22c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 7.00e-04; wd 2.00e-08; nl 2; hc 512; rl 3.00e+02\n"
     ]
    }
   ],
   "source": [
    "# Create GNN\n",
    "\n",
    "base_name = \"gnn_test\" # Name that the files will be saved as. Make it different for every unique GNN you train.\n",
    "\n",
    "name = base_name           \n",
    "boxes = range(1024)        # Which simulations you include, there are 1024 simulations\n",
    "prediction = [0]   \n",
    "\n",
    "# Initial hyperparameters (will be tuned)\n",
    "\n",
    "rl = 300       # Linking radius\n",
    "lr = 7e-4      # Learning rate\n",
    "wd = 2e-8      # Weight decay\n",
    "nl = 2         # Number of layers\n",
    "hc = 512       # Hidden channels (power of 2)\n",
    "n_epochs = 100  # Number of epochs\n",
    "hparams = Hyperparameters(lr, wd, nl, hc, rl, n_epochs, name)\n",
    "print(hparams)\n",
    "\n",
    "## Create GNN based on hyperparameters\n",
    "model = GNN(node_features=dataset[0].x.shape[1],\n",
    "            n_layers=hparams.n_layers,\n",
    "            hidden_channels=hparams.hidden_channels,\n",
    "            linkradius=hparams.r_link,\n",
    "            dim_out=len(prediction)*2,\n",
    "            only_positions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac9a4ca7-1352-4593-9d92-068bc5f8101d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Move to gpu/cpu (highly recommend gpu)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda') #gpu\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9758a095-e22f-4d1c-9d90-700cf6e213d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define training functions\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, hparams):\n",
    "    \"\"\"\n",
    "    This is the main loop for training the GNN. For each epoch, the GNN is given data from the training set to update its parameters and is then tested on the validation set to see if the model has improved.\n",
    "    If the model has improved, the GNN is saved in a file which can be reloaded later.\n",
    "    \n",
    "    Inputs\n",
    "     - model - the instantiated and untrained GNN\n",
    "     - train_loader - a pytorch_geometric DataLoader object containing the training dataset\n",
    "     - valid_loader - a pytorch_geometric DataLoader object containing the validation dataset\n",
    "     - hparams - a Hyperparameters object containing the hyperparameters to be used in this training session\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=hparams.learning_rate, weight_decay=hparams.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=hparams.learning_rate, max_lr=1.e-3, cycle_momentum=False, step_size_up=500)\n",
    "    \n",
    "    train_losses, valid_losses = [], []\n",
    "    valid_loss_min, err_min = 1000., 1000.\n",
    "    \n",
    "    for epoch in range(1, hparams.n_epochs+1):\n",
    "        train_loss = train(train_loader, model, hparams, optimizer, scheduler)\n",
    "        valid_loss, err = test(valid_loader, model, hparams)\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        # Save model if it has improved\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            torch.save(model.state_dict(), \"Models/\"+hparams.name_model())\n",
    "            valid_loss_min = valid_loss\n",
    "            err_min = err\n",
    "            print(f\"Epoch {epoch:03d} Train loss {train_loss:.2e} Valid loss {valid_loss:.2e} Error: {err:.2e} (B)\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch:03d} Train loss {train_loss:.2e} Valid loss {valid_loss:.2e} Error: {err:.2e}\")\n",
    "            \n",
    "    return train_losses, valid_losses\n",
    "\n",
    "def train(loader, model, hparams, optimizer, scheduler):\n",
    "    \"\"\"\n",
    "    This function loops over all data in the training dataset, calculates the loss, and updates the GNN parameters appropriately.\n",
    "    \n",
    "    Inputs\n",
    "     - loader - a pytorch_geometric DataLoader object containing the training dataset\n",
    "     - model - the partially trained GNN object\n",
    "     - hparams - a Hyperparameters object containing the hyperparameters to be used in this training session\n",
    "     - optimizer - the pytorch optimizer used to update the GNN\n",
    "     - scheduler - the pytorch scheduler used to vary the training rates / momentum\n",
    "     \n",
    "    Returns\n",
    "     - loss - the average log loss from this epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    loss_tot = 0\n",
    "    \n",
    "    for data in loader:  # Iterate in batches over the training dataset.\n",
    "\n",
    "        data.to(device)\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        out = model(data)  # Perform a single forward pass.\n",
    "        \n",
    "        y_out, err_out = out[:,:hparams.pred_params], out[:,hparams.pred_params:2*hparams.pred_params]     # Take mean and standard deviation of the output\n",
    "        \n",
    "        # Compute loss as sum of two terms for likelihood-free inference\n",
    "        loss_mse = torch.mean(torch.sum((y_out - data.y)**2., axis=1) , axis=0)\n",
    "        loss_lfi = torch.mean(torch.sum(((y_out - data.y)**2. - err_out**2.)**2., axis=1) , axis=0)\n",
    "        loss = torch.log(loss_mse) + torch.log(loss_lfi)\n",
    "\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        scheduler.step()\n",
    "        loss_tot += loss.item()\n",
    "\n",
    "    return loss_tot/len(loader)\n",
    "\n",
    "def test(loader, model, hparams):\n",
    "    \"\"\"\n",
    "    This function loops over all data in the given (validation or testing) dataset and calculates the loss. \n",
    "    The parameters of the model are not updated in this function.\n",
    "    \n",
    "    Inputs\n",
    "     - loader - a pytorch_geometric DataLoader object containing the validation or testing dataset\n",
    "     - model - the partially trained GNN object\n",
    "     - hparams - a Hyperparameters object containing the hyperparameters to be used in this training session\n",
    "     \n",
    "    Returns\n",
    "     - loss - the average log loss from this epoch\n",
    "     - errs - the average absolute error from the GNN predictions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    trueparams = np.zeros((0,hparams.pred_params))\n",
    "    outparams = np.zeros((0,hparams.pred_params))\n",
    "    outerrparams = np.zeros((0,hparams.pred_params))\n",
    "\n",
    "    errs = []\n",
    "    loss_tot = 0\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        with torch.no_grad():\n",
    "\n",
    "            data.to(device)\n",
    "            out = model(data)  # Perform a single forward pass.\n",
    "\n",
    "            # If cosmo parameters are predicted, perform likelihood-free inference to predict also the standard deviation\n",
    "            y_out, err_out = out[:,:hparams.pred_params], out[:,hparams.pred_params:2*hparams.pred_params]     # Take mean and standard deviation of the output\n",
    "            \n",
    "            # Compute loss as sum of two terms for likelihood-free inference\n",
    "            loss_mse = torch.mean(torch.sum((y_out - data.y)**2., axis=1) , axis=0)\n",
    "            loss_lfi = torch.mean(torch.sum(((y_out - data.y)**2. - err_out**2.)**2., axis=1) , axis=0)\n",
    "            loss = torch.log(loss_mse) + torch.log(loss_lfi)\n",
    "\n",
    "            err = (y_out - data.y)#/data.y\n",
    "            errs.append( np.abs(err.detach().cpu().numpy()).mean() )\n",
    "            loss_tot += loss.item()\n",
    "\n",
    "            # Append true values and predictions\n",
    "            trueparams = np.append(trueparams, data.y.detach().cpu().numpy(), 0)\n",
    "            outparams = np.append(outparams, y_out.detach().cpu().numpy(), 0)\n",
    "            outerrparams  = np.append(outerrparams, err_out.detach().cpu().numpy(), 0)\n",
    "                \n",
    "    \n",
    "    # Save true values and predictions\n",
    "    np.save(\"Outputs/trues_\"+hparams.name_model()+\".npy\",trueparams)\n",
    "    np.save(\"Outputs/outputs_\"+hparams.name_model()+\".npy\",outparams)\n",
    "    np.save(\"Outputs/errors_\"+hparams.name_model()+\".npy\",outerrparams)\n",
    "\n",
    "    return loss_tot/len(loader), np.array(errs).mean(axis=0)\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    This function is given to optuna to tune hyperparameters. Given the current trial, optuna will suggest new hyperparameters for this training session.\n",
    "    \n",
    "    Inputs\n",
    "     - trial - an optuna object containing information on the current training session\n",
    "     \n",
    "    Returns\n",
    "     - test_loss - the log loss from the testing dataset, used to compare trials and choose hyperparameters\n",
    "    \"\"\"\n",
    "    hparams.learning_rate = trial.suggest_float(\"learning_rate\", 1e-7, 1e-3, log=True)\n",
    "    hparams.weight_decay = trial.suggest_float(\"weight_decay\", 1e-9, 1e-6, log=True)\n",
    "    hparams.n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    hparams.hidden_channels = trial.suggest_categorical(\"hidden_channels\", [64, 128, 256, 512])\n",
    "    hparams.r_link = trial.suggest_int(\"r_link\", 0, 300, log=False)   # can delete\n",
    "    \n",
    "    dataset = []\n",
    "    for i in range(len(datalist)):\n",
    "        dataset.append(graph.create_dataset(datalist[i], params[i]))\n",
    "        \n",
    "    model = GNN(node_features=dataset[0].x.shape[1],\n",
    "            n_layers=hparams.n_layers,\n",
    "            hidden_channels=hparams.hidden_channels,\n",
    "            linkradius=hparams.r_link,\n",
    "            dim_out=len(prediction)*2,\n",
    "            only_positions=False)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda') #gpu\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    model.to(device)\n",
    "        \n",
    "    train_data, valid_data, test_data = graph.split_dataset(dataset, train_size, valid_size, test_size)\n",
    "    train_loader = graph.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = graph.DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = graph.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    train_losses, valid_losses = train_model(model, train_loader, valid_loader, hparams)\n",
    "    \n",
    "    np.save(f\"Outputs/train_loss_{hparams.name_model()}\", train_losses)\n",
    "    np.save(f\"Outputs/valid_loss_{hparams.name_model()}\", valid_losses)\n",
    "    \n",
    "    state_dict = torch.load(\"Models/\"+hparams.name_model(), map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    test_loss, err = test(test_loader, model, hparams)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aff564c2-fd0b-4c7d-b801-6a52d782f959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 Train loss 4.54e+01 Valid loss 8.09e+01 Error: 7.89e+05 (B)\n",
      "Epoch 002 Train loss 8.09e+01 Valid loss 7.87e+01 Error: 9.35e+05 (B)\n",
      "Epoch 003 Train loss 7.87e+01 Valid loss 8.83e+01 Error: 2.53e+06\n",
      "Epoch 004 Train loss 8.83e+01 Valid loss 9.04e+01 Error: 3.53e+06\n",
      "Epoch 005 Train loss 9.04e+01 Valid loss 9.07e+01 Error: 3.70e+06\n",
      "Epoch 006 Train loss 9.07e+01 Valid loss 8.89e+01 Error: 2.72e+06\n",
      "Epoch 007 Train loss 8.89e+01 Valid loss 8.28e+01 Error: 1.07e+06\n",
      "Epoch 008 Train loss 8.28e+01 Valid loss 7.76e+01 Error: 1.99e+05 (B)\n",
      "Epoch 009 Train loss 7.76e+01 Valid loss 7.47e+01 Error: 2.55e+05 (B)\n",
      "Epoch 010 Train loss 7.47e+01 Valid loss 8.32e+01 Error: 1.32e+06\n",
      "Epoch 011 Train loss 8.32e+01 Valid loss 8.63e+01 Error: 1.23e+06\n",
      "Epoch 012 Train loss 8.63e+01 Valid loss 8.73e+01 Error: 1.00e+06\n",
      "Epoch 013 Train loss 8.73e+01 Valid loss 8.60e+01 Error: 4.35e+05\n",
      "Epoch 014 Train loss 8.60e+01 Valid loss 8.54e+01 Error: 4.13e+05\n",
      "Epoch 015 Train loss 8.54e+01 Valid loss 8.29e+01 Error: 1.87e+05\n",
      "Epoch 016 Train loss 8.29e+01 Valid loss 8.44e+01 Error: 8.75e+05\n",
      "Epoch 017 Train loss 8.44e+01 Valid loss 8.14e+01 Error: 1.45e+06\n",
      "Epoch 018 Train loss 8.14e+01 Valid loss 8.55e+01 Error: 7.20e+05\n",
      "Epoch 019 Train loss 8.55e+01 Valid loss 8.41e+01 Error: 2.21e+05\n",
      "Epoch 020 Train loss 8.41e+01 Valid loss 8.69e+01 Error: 8.63e+05\n",
      "Epoch 021 Train loss 8.69e+01 Valid loss 8.75e+01 Error: 1.58e+06\n",
      "Epoch 022 Train loss 8.75e+01 Valid loss 8.63e+01 Error: 1.99e+06\n",
      "Epoch 023 Train loss 8.63e+01 Valid loss 8.57e+01 Error: 2.30e+06\n",
      "Epoch 024 Train loss 8.57e+01 Valid loss 8.52e+01 Error: 2.03e+06\n",
      "Epoch 025 Train loss 8.52e+01 Valid loss 8.19e+01 Error: 1.44e+06\n",
      "Epoch 026 Train loss 8.19e+01 Valid loss 8.29e+01 Error: 1.30e+06\n",
      "Epoch 027 Train loss 8.29e+01 Valid loss 8.11e+01 Error: 9.00e+05\n",
      "Epoch 028 Train loss 8.11e+01 Valid loss 7.49e+01 Error: 4.21e+05\n",
      "Epoch 029 Train loss 7.49e+01 Valid loss 7.65e+01 Error: 2.38e+05\n",
      "Epoch 030 Train loss 7.65e+01 Valid loss 8.26e+01 Error: 9.64e+05\n",
      "Epoch 031 Train loss 8.26e+01 Valid loss 7.93e+01 Error: 5.73e+05\n",
      "Epoch 032 Train loss 7.93e+01 Valid loss 7.28e+01 Error: 1.97e+05 (B)\n",
      "Epoch 033 Train loss 7.28e+01 Valid loss 7.65e+01 Error: 3.47e+05\n",
      "Epoch 034 Train loss 7.65e+01 Valid loss 7.34e+01 Error: 2.08e+05\n",
      "Epoch 035 Train loss 7.34e+01 Valid loss 6.88e+01 Error: 6.89e+04 (B)\n",
      "Epoch 036 Train loss 6.88e+01 Valid loss 7.12e+01 Error: 1.75e+05\n",
      "Epoch 037 Train loss 7.12e+01 Valid loss 7.46e+01 Error: 1.46e+05\n",
      "Epoch 038 Train loss 7.46e+01 Valid loss 7.29e+01 Error: 3.26e+05\n",
      "Epoch 039 Train loss 7.29e+01 Valid loss 7.29e+01 Error: 1.55e+04\n",
      "Epoch 040 Train loss 7.29e+01 Valid loss 7.82e+01 Error: 2.52e+05\n",
      "Epoch 041 Train loss 7.82e+01 Valid loss 7.87e+01 Error: 3.79e+05\n",
      "Epoch 042 Train loss 7.87e+01 Valid loss 7.87e+01 Error: 5.00e+05\n",
      "Epoch 043 Train loss 7.87e+01 Valid loss 7.66e+01 Error: 6.60e+05\n",
      "Epoch 044 Train loss 7.66e+01 Valid loss 8.00e+01 Error: 5.14e+05\n",
      "Epoch 045 Train loss 8.00e+01 Valid loss 8.04e+01 Error: 3.77e+05\n",
      "Epoch 046 Train loss 8.04e+01 Valid loss 7.94e+01 Error: 2.21e+05\n",
      "Epoch 047 Train loss 7.94e+01 Valid loss 7.47e+01 Error: 2.36e+04\n",
      "Epoch 048 Train loss 7.47e+01 Valid loss 7.90e+01 Error: 4.25e+05\n",
      "Epoch 049 Train loss 7.90e+01 Valid loss 7.88e+01 Error: 7.17e+05\n",
      "Epoch 050 Train loss 7.88e+01 Valid loss 8.00e+01 Error: 7.89e+05\n",
      "Epoch 051 Train loss 8.00e+01 Valid loss 7.90e+01 Error: 7.50e+05\n",
      "Epoch 052 Train loss 7.90e+01 Valid loss 7.75e+01 Error: 6.25e+05\n",
      "Epoch 053 Train loss 7.75e+01 Valid loss 7.38e+01 Error: 5.88e+05\n",
      "Epoch 054 Train loss 7.38e+01 Valid loss 8.09e+01 Error: 7.54e+05\n",
      "Epoch 055 Train loss 8.09e+01 Valid loss 8.17e+01 Error: 8.27e+05\n",
      "Epoch 056 Train loss 8.17e+01 Valid loss 8.17e+01 Error: 8.24e+05\n",
      "Epoch 057 Train loss 8.17e+01 Valid loss 8.11e+01 Error: 7.61e+05\n",
      "Epoch 058 Train loss 8.11e+01 Valid loss 7.98e+01 Error: 6.61e+05\n",
      "Epoch 059 Train loss 7.98e+01 Valid loss 7.71e+01 Error: 5.20e+05\n",
      "Epoch 060 Train loss 7.71e+01 Valid loss 7.67e+01 Error: 3.15e+05\n",
      "Epoch 061 Train loss 7.67e+01 Valid loss 7.61e+01 Error: 1.64e+05\n",
      "Epoch 062 Train loss 7.61e+01 Valid loss 7.10e+01 Error: 1.68e+04\n",
      "Epoch 063 Train loss 7.10e+01 Valid loss 7.24e+01 Error: 2.60e+05\n",
      "Epoch 064 Train loss 7.24e+01 Valid loss 7.89e+01 Error: 5.18e+05\n",
      "Epoch 065 Train loss 7.89e+01 Valid loss 8.01e+01 Error: 6.30e+05\n",
      "Epoch 066 Train loss 8.01e+01 Valid loss 7.97e+01 Error: 6.09e+05\n",
      "Epoch 067 Train loss 7.97e+01 Valid loss 7.78e+01 Error: 4.94e+05\n",
      "Epoch 068 Train loss 7.78e+01 Valid loss 7.39e+01 Error: 3.27e+05\n",
      "Epoch 069 Train loss 7.39e+01 Valid loss 7.40e+01 Error: 3.01e+05\n",
      "Epoch 070 Train loss 7.40e+01 Valid loss 7.12e+01 Error: 2.02e+05\n",
      "Epoch 071 Train loss 7.12e+01 Valid loss 6.95e+01 Error: 5.61e+04\n",
      "Epoch 072 Train loss 6.95e+01 Valid loss 6.78e+01 Error: 5.30e+04 (B)\n",
      "Epoch 073 Train loss 6.78e+01 Valid loss 6.44e+01 Error: 5.78e+04 (B)\n",
      "Epoch 074 Train loss 6.44e+01 Valid loss 6.66e+01 Error: 6.92e+04\n",
      "Epoch 075 Train loss 6.66e+01 Valid loss 5.86e+01 Error: 1.99e+03 (B)\n",
      "Epoch 076 Train loss 5.86e+01 Valid loss 6.79e+01 Error: 9.49e+04\n",
      "Epoch 077 Train loss 6.79e+01 Valid loss 6.22e+01 Error: 1.00e+05\n",
      "Epoch 078 Train loss 6.22e+01 Valid loss 7.67e+01 Error: 3.56e+05\n",
      "Epoch 079 Train loss 7.67e+01 Valid loss 7.95e+01 Error: 5.73e+05\n",
      "Epoch 080 Train loss 7.95e+01 Valid loss 8.11e+01 Error: 7.60e+05\n",
      "Epoch 081 Train loss 8.11e+01 Valid loss 8.20e+01 Error: 8.97e+05\n",
      "Epoch 082 Train loss 8.20e+01 Valid loss 8.24e+01 Error: 9.83e+05\n",
      "Epoch 083 Train loss 8.24e+01 Valid loss 8.20e+01 Error: 9.87e+05\n",
      "Epoch 084 Train loss 8.20e+01 Valid loss 7.97e+01 Error: 8.17e+05\n",
      "Epoch 085 Train loss 7.97e+01 Valid loss 7.82e+01 Error: 4.68e+05\n",
      "Epoch 086 Train loss 7.82e+01 Valid loss 7.57e+01 Error: 2.17e+05\n",
      "Epoch 087 Train loss 7.57e+01 Valid loss 6.86e+01 Error: 9.28e+04\n",
      "Epoch 088 Train loss 6.86e+01 Valid loss 7.50e+01 Error: 2.74e+05\n",
      "Epoch 089 Train loss 7.50e+01 Valid loss 7.51e+01 Error: 2.80e+05\n",
      "Epoch 090 Train loss 7.51e+01 Valid loss 6.96e+01 Error: 1.21e+05\n",
      "Epoch 091 Train loss 6.96e+01 Valid loss 7.30e+01 Error: 1.94e+05\n",
      "Epoch 092 Train loss 7.30e+01 Valid loss 7.22e+01 Error: 1.67e+05\n",
      "Epoch 093 Train loss 7.22e+01 Valid loss 5.64e+01 Error: 6.51e+03 (B)\n",
      "Epoch 094 Train loss 5.64e+01 Valid loss 7.97e+01 Error: 9.72e+05\n",
      "Epoch 095 Train loss 7.97e+01 Valid loss 8.47e+01 Error: 1.55e+06\n",
      "Epoch 096 Train loss 8.47e+01 Valid loss 8.73e+01 Error: 2.09e+06\n",
      "Epoch 097 Train loss 8.73e+01 Valid loss 8.85e+01 Error: 2.45e+06\n",
      "Epoch 098 Train loss 8.85e+01 Valid loss 8.87e+01 Error: 2.67e+06\n",
      "Epoch 099 Train loss 8.87e+01 Valid loss 8.75e+01 Error: 2.80e+06\n",
      "Epoch 100 Train loss 8.75e+01 Valid loss 8.79e+01 Error: 3.29e+06\n",
      "72.15962219238281 56.447265625 56.447227478027344\n"
     ]
    }
   ],
   "source": [
    "# Initial training, without tuned hyperparameters\n",
    "\n",
    "train_losses, valid_losses = train_model(model, train_loader, valid_loader, hparams)\n",
    "\n",
    "state_dict = torch.load(\"Models/\"+hparams.name_model(), map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "test_loss, err = test(test_loader, model, hparams)\n",
    "valid_loss = np.min(valid_losses)\n",
    "train_loss = train_losses[np.argmin(valid_losses)]\n",
    "print(train_loss, valid_loss, test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93d91196-f70d-4d4a-9279-04ea88aa6204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-23 19:58:11,995] Using an existing study with name 'gnn_test_trials' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 Train loss 5.86e+01 Valid loss 5.85e+01 Error: 1.72e+04 (B)\n",
      "Epoch 002 Train loss 5.85e+01 Valid loss 5.79e+01 Error: 1.55e+04 (B)\n",
      "Epoch 003 Train loss 5.79e+01 Valid loss 5.65e+01 Error: 1.25e+04 (B)\n",
      "Epoch 004 Train loss 5.65e+01 Valid loss 5.40e+01 Error: 8.26e+03 (B)\n",
      "Epoch 005 Train loss 5.40e+01 Valid loss 4.71e+01 Error: 3.34e+03 (B)\n",
      "Epoch 006 Train loss 4.71e+01 Valid loss 4.61e+01 Error: 7.15e+02 (B)\n",
      "Epoch 007 Train loss 4.61e+01 Valid loss 4.84e+01 Error: 1.69e+03\n",
      "Epoch 008 Train loss 4.84e+01 Valid loss 4.70e+01 Error: 2.37e+03\n",
      "Epoch 009 Train loss 4.70e+01 Valid loss 4.99e+01 Error: 4.09e+03\n",
      "Epoch 010 Train loss 4.99e+01 Valid loss 4.42e+01 Error: 4.11e+03 (B)\n",
      "Epoch 011 Train loss 4.42e+01 Valid loss 5.33e+01 Error: 7.15e+03\n",
      "Epoch 012 Train loss 5.33e+01 Valid loss 5.45e+01 Error: 9.55e+03\n",
      "Epoch 013 Train loss 5.45e+01 Valid loss 5.38e+01 Error: 1.10e+04\n",
      "Epoch 014 Train loss 5.38e+01 Valid loss 5.50e+01 Error: 1.13e+04\n",
      "Epoch 015 Train loss 5.50e+01 Valid loss 5.70e+01 Error: 1.23e+04\n",
      "Epoch 016 Train loss 5.70e+01 Valid loss 5.81e+01 Error: 1.35e+04\n",
      "Epoch 017 Train loss 5.81e+01 Valid loss 5.88e+01 Error: 1.50e+04\n",
      "Epoch 018 Train loss 5.88e+01 Valid loss 5.93e+01 Error: 1.63e+04\n",
      "Epoch 019 Train loss 5.93e+01 Valid loss 5.97e+01 Error: 1.77e+04\n",
      "Epoch 020 Train loss 5.97e+01 Valid loss 5.99e+01 Error: 1.93e+04\n",
      "Epoch 021 Train loss 5.99e+01 Valid loss 5.99e+01 Error: 2.09e+04\n",
      "Epoch 022 Train loss 5.99e+01 Valid loss 5.97e+01 Error: 2.27e+04\n",
      "Epoch 023 Train loss 5.97e+01 Valid loss 5.89e+01 Error: 2.52e+04\n",
      "Epoch 024 Train loss 5.89e+01 Valid loss 5.64e+01 Error: 2.87e+04\n",
      "Epoch 025 Train loss 5.64e+01 Valid loss 5.90e+01 Error: 2.62e+04\n",
      "Epoch 026 Train loss 5.90e+01 Valid loss 5.99e+01 Error: 2.49e+04\n",
      "Epoch 027 Train loss 5.99e+01 Valid loss 6.02e+01 Error: 2.42e+04\n",
      "Epoch 028 Train loss 6.02e+01 Valid loss 6.02e+01 Error: 2.39e+04\n",
      "Epoch 029 Train loss 6.02e+01 Valid loss 5.99e+01 Error: 2.38e+04\n",
      "Epoch 030 Train loss 5.99e+01 Valid loss 5.94e+01 Error: 2.40e+04\n",
      "Epoch 031 Train loss 5.94e+01 Valid loss 5.80e+01 Error: 2.49e+04\n",
      "Epoch 032 Train loss 5.80e+01 Valid loss 5.84e+01 Error: 2.76e+04\n",
      "Epoch 033 Train loss 5.84e+01 Valid loss 5.93e+01 Error: 2.77e+04\n",
      "Epoch 034 Train loss 5.93e+01 Valid loss 5.87e+01 Error: 2.61e+04\n",
      "Epoch 035 Train loss 5.87e+01 Valid loss 5.54e+01 Error: 2.27e+04\n",
      "Epoch 036 Train loss 5.54e+01 Valid loss 5.90e+01 Error: 1.47e+04\n",
      "Epoch 037 Train loss 5.90e+01 Valid loss 5.90e+01 Error: 8.70e+03\n",
      "Epoch 038 Train loss 5.90e+01 Valid loss 5.71e+01 Error: 2.74e+03\n",
      "Epoch 039 Train loss 5.71e+01 Valid loss 5.82e+01 Error: 4.40e+03\n",
      "Epoch 040 Train loss 5.82e+01 Valid loss 5.94e+01 Error: 8.60e+03\n",
      "Epoch 041 Train loss 5.94e+01 Valid loss 5.98e+01 Error: 1.14e+04\n",
      "Epoch 042 Train loss 5.98e+01 Valid loss 5.97e+01 Error: 1.34e+04\n",
      "Epoch 043 Train loss 5.97e+01 Valid loss 5.93e+01 Error: 1.54e+04\n",
      "Epoch 044 Train loss 5.93e+01 Valid loss 5.81e+01 Error: 1.72e+04\n",
      "Epoch 045 Train loss 5.81e+01 Valid loss 5.49e+01 Error: 1.92e+04\n",
      "Epoch 046 Train loss 5.49e+01 Valid loss 5.78e+01 Error: 1.43e+04\n",
      "Epoch 047 Train loss 5.78e+01 Valid loss 5.82e+01 Error: 1.11e+04\n",
      "Epoch 048 Train loss 5.82e+01 Valid loss 5.79e+01 Error: 8.14e+03\n",
      "Epoch 049 Train loss 5.79e+01 Valid loss 5.70e+01 Error: 4.94e+03\n",
      "Epoch 050 Train loss 5.70e+01 Valid loss 5.29e+01 Error: 7.56e+02\n",
      "Epoch 051 Train loss 5.29e+01 Valid loss 5.65e+01 Error: 8.68e+03\n",
      "Epoch 052 Train loss 5.65e+01 Valid loss 5.61e+01 Error: 1.56e+04\n",
      "Epoch 053 Train loss 5.61e+01 Valid loss 5.82e+01 Error: 1.88e+04\n",
      "Epoch 054 Train loss 5.82e+01 Valid loss 5.88e+01 Error: 2.04e+04\n",
      "Epoch 055 Train loss 5.88e+01 Valid loss 5.86e+01 Error: 2.03e+04\n",
      "Epoch 056 Train loss 5.86e+01 Valid loss 5.75e+01 Error: 1.92e+04\n",
      "Epoch 057 Train loss 5.75e+01 Valid loss 5.54e+01 Error: 1.67e+04\n",
      "Epoch 058 Train loss 5.54e+01 Valid loss 5.51e+01 Error: 1.72e+04\n",
      "Epoch 059 Train loss 5.51e+01 Valid loss 5.67e+01 Error: 1.40e+04\n",
      "Epoch 060 Train loss 5.67e+01 Valid loss 5.69e+01 Error: 1.20e+04\n",
      "Epoch 061 Train loss 5.69e+01 Valid loss 5.61e+01 Error: 1.02e+04\n",
      "Epoch 062 Train loss 5.61e+01 Valid loss 5.35e+01 Error: 8.85e+03\n",
      "Epoch 063 Train loss 5.35e+01 Valid loss 5.48e+01 Error: 9.39e+03\n",
      "Epoch 064 Train loss 5.48e+01 Valid loss 5.21e+01 Error: 7.55e+03\n",
      "Epoch 065 Train loss 5.21e+01 Valid loss 5.37e+01 Error: 1.83e+03\n",
      "Epoch 066 Train loss 5.37e+01 Valid loss 5.76e+01 Error: 6.84e+03\n",
      "Epoch 067 Train loss 5.76e+01 Valid loss 5.91e+01 Error: 1.21e+04\n",
      "Epoch 068 Train loss 5.91e+01 Valid loss 5.96e+01 Error: 1.65e+04\n",
      "Epoch 069 Train loss 5.96e+01 Valid loss 5.88e+01 Error: 2.11e+04\n",
      "Epoch 070 Train loss 5.88e+01 Valid loss 5.91e+01 Error: 2.72e+04\n",
      "Epoch 071 Train loss 5.91e+01 Valid loss 5.98e+01 Error: 2.85e+04\n",
      "Epoch 072 Train loss 5.98e+01 Valid loss 5.79e+01 Error: 2.68e+04\n",
      "Epoch 073 Train loss 5.79e+01 Valid loss 6.03e+01 Error: 2.02e+04\n",
      "Epoch 074 Train loss 6.03e+01 Valid loss 6.09e+01 Error: 1.47e+04\n",
      "Epoch 075 Train loss 6.09e+01 Valid loss 6.06e+01 Error: 9.76e+03\n",
      "Epoch 076 Train loss 6.06e+01 Valid loss 5.93e+01 Error: 4.64e+03\n",
      "Epoch 077 Train loss 5.93e+01 Valid loss 5.73e+01 Error: 1.78e+03\n",
      "Epoch 078 Train loss 5.73e+01 Valid loss 5.53e+01 Error: 6.98e+02\n",
      "Epoch 079 Train loss 5.53e+01 Valid loss 6.03e+01 Error: 1.23e+04\n",
      "Epoch 080 Train loss 6.03e+01 Valid loss 5.92e+01 Error: 2.24e+04\n",
      "Epoch 081 Train loss 5.92e+01 Valid loss 6.17e+01 Error: 3.35e+04\n",
      "Epoch 082 Train loss 6.17e+01 Valid loss 6.35e+01 Error: 4.05e+04\n",
      "Epoch 083 Train loss 6.35e+01 Valid loss 6.43e+01 Error: 4.54e+04\n",
      "Epoch 084 Train loss 6.43e+01 Valid loss 6.46e+01 Error: 4.78e+04\n",
      "Epoch 085 Train loss 6.46e+01 Valid loss 6.47e+01 Error: 4.85e+04\n",
      "Epoch 086 Train loss 6.47e+01 Valid loss 6.47e+01 Error: 4.81e+04\n",
      "Epoch 087 Train loss 6.47e+01 Valid loss 6.44e+01 Error: 4.60e+04\n",
      "Epoch 088 Train loss 6.44e+01 Valid loss 6.39e+01 Error: 4.23e+04\n",
      "Epoch 089 Train loss 6.39e+01 Valid loss 6.32e+01 Error: 3.76e+04\n",
      "Epoch 090 Train loss 6.32e+01 Valid loss 6.20e+01 Error: 3.11e+04\n",
      "Epoch 091 Train loss 6.20e+01 Valid loss 6.00e+01 Error: 2.21e+04\n",
      "Epoch 092 Train loss 6.00e+01 Valid loss 5.51e+01 Error: 1.01e+04\n",
      "Epoch 093 Train loss 5.51e+01 Valid loss 5.09e+01 Error: 5.76e+03\n",
      "Epoch 094 Train loss 5.09e+01 Valid loss 5.46e+01 Error: 2.28e+03\n",
      "Epoch 095 Train loss 5.46e+01 Valid loss 5.32e+01 Error: 6.39e+02\n",
      "Epoch 096 Train loss 5.32e+01 Valid loss 5.45e+01 Error: 1.81e+04\n",
      "Epoch 097 Train loss 5.45e+01 Valid loss 6.40e+01 Error: 4.29e+04\n",
      "Epoch 098 Train loss 6.40e+01 Valid loss 6.59e+01 Error: 5.99e+04\n",
      "Epoch 099 Train loss 6.59e+01 Valid loss 6.68e+01 Error: 7.16e+04\n",
      "Epoch 100 Train loss 6.68e+01 Valid loss 6.74e+01 Error: 8.09e+04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-23 19:59:19,980] Trial 22 finished with value: 44.197235107421875 and parameters: {'learning_rate': 5.456692167375115e-07, 'weight_decay': 7.344790588451852e-08, 'n_layers': 3, 'hidden_channels': 256, 'r_link': 191}. Best is trial 10 with value: 35.48434066772461.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 Train loss 4.86e+01 Valid loss 5.57e+01 Error: 2.82e+04 (B)\n",
      "Epoch 002 Train loss 5.57e+01 Valid loss 6.55e+01 Error: 5.71e+04\n",
      "Epoch 003 Train loss 6.55e+01 Valid loss 6.72e+01 Error: 7.30e+04\n",
      "Epoch 004 Train loss 6.72e+01 Valid loss 6.79e+01 Error: 8.26e+04\n",
      "Epoch 005 Train loss 6.79e+01 Valid loss 6.83e+01 Error: 8.83e+04\n",
      "Epoch 006 Train loss 6.83e+01 Valid loss 6.86e+01 Error: 9.17e+04\n",
      "Epoch 007 Train loss 6.86e+01 Valid loss 6.86e+01 Error: 9.31e+04\n",
      "Epoch 008 Train loss 6.86e+01 Valid loss 6.86e+01 Error: 9.30e+04\n",
      "Epoch 009 Train loss 6.86e+01 Valid loss 6.85e+01 Error: 9.13e+04\n",
      "Epoch 010 Train loss 6.85e+01 Valid loss 6.83e+01 Error: 8.78e+04\n",
      "Epoch 011 Train loss 6.83e+01 Valid loss 6.79e+01 Error: 8.28e+04\n",
      "Epoch 012 Train loss 6.79e+01 Valid loss 6.74e+01 Error: 7.61e+04\n",
      "Epoch 013 Train loss 6.74e+01 Valid loss 6.66e+01 Error: 6.77e+04\n",
      "Epoch 014 Train loss 6.66e+01 Valid loss 6.56e+01 Error: 5.77e+04\n",
      "Epoch 015 Train loss 6.56e+01 Valid loss 6.39e+01 Error: 4.60e+04\n",
      "Epoch 016 Train loss 6.39e+01 Valid loss 6.03e+01 Error: 3.19e+04\n",
      "Epoch 017 Train loss 6.03e+01 Valid loss 6.02e+01 Error: 1.45e+04\n",
      "Epoch 018 Train loss 6.02e+01 Valid loss 5.77e+01 Error: 3.25e+03\n",
      "Epoch 019 Train loss 5.77e+01 Valid loss 5.74e+01 Error: 2.96e+03\n",
      "Epoch 020 Train loss 5.74e+01 Valid loss 5.92e+01 Error: 8.04e+03\n",
      "Epoch 021 Train loss 5.92e+01 Valid loss 5.88e+01 Error: 1.33e+04\n",
      "Epoch 022 Train loss 5.88e+01 Valid loss 5.67e+01 Error: 1.67e+04\n",
      "Epoch 023 Train loss 5.67e+01 Valid loss 5.35e+01 Error: 9.55e+03 (B)\n",
      "Epoch 024 Train loss 5.35e+01 Valid loss 4.82e+01 Error: 1.02e+04 (B)\n",
      "Epoch 025 Train loss 4.82e+01 Valid loss 6.26e+01 Error: 4.62e+04\n",
      "Epoch 026 Train loss 6.26e+01 Valid loss 6.01e+01 Error: 7.38e+04\n",
      "Epoch 027 Train loss 6.01e+01 Valid loss 6.76e+01 Error: 8.34e+04\n",
      "Epoch 028 Train loss 6.76e+01 Valid loss 6.92e+01 Error: 8.98e+04\n",
      "Epoch 029 Train loss 6.92e+01 Valid loss 7.01e+01 Error: 9.57e+04\n",
      "Epoch 030 Train loss 7.01e+01 Valid loss 7.08e+01 Error: 1.01e+05\n",
      "Epoch 031 Train loss 7.08e+01 Valid loss 7.13e+01 Error: 1.06e+05\n",
      "Epoch 032 Train loss 7.13e+01 Valid loss 7.17e+01 Error: 1.09e+05\n",
      "Epoch 033 Train loss 7.17e+01 Valid loss 7.19e+01 Error: 1.12e+05\n",
      "Epoch 034 Train loss 7.19e+01 Valid loss 7.20e+01 Error: 1.12e+05\n",
      "Epoch 035 Train loss 7.20e+01 Valid loss 7.20e+01 Error: 1.13e+05\n",
      "Epoch 036 Train loss 7.20e+01 Valid loss 7.19e+01 Error: 1.12e+05\n",
      "Epoch 037 Train loss 7.19e+01 Valid loss 7.18e+01 Error: 1.12e+05\n",
      "Epoch 038 Train loss 7.18e+01 Valid loss 7.16e+01 Error: 1.11e+05\n",
      "Epoch 039 Train loss 7.16e+01 Valid loss 7.13e+01 Error: 1.09e+05\n",
      "Epoch 040 Train loss 7.13e+01 Valid loss 7.09e+01 Error: 1.08e+05\n",
      "Epoch 041 Train loss 7.09e+01 Valid loss 7.02e+01 Error: 1.06e+05\n",
      "Epoch 042 Train loss 7.02e+01 Valid loss 6.92e+01 Error: 1.04e+05\n",
      "Epoch 043 Train loss 6.92e+01 Valid loss 6.71e+01 Error: 1.04e+05\n",
      "Epoch 044 Train loss 6.71e+01 Valid loss 6.78e+01 Error: 1.13e+05\n",
      "Epoch 045 Train loss 6.78e+01 Valid loss 6.81e+01 Error: 1.05e+05\n",
      "Epoch 046 Train loss 6.81e+01 Valid loss 6.71e+01 Error: 8.80e+04\n",
      "Epoch 047 Train loss 6.71e+01 Valid loss 6.34e+01 Error: 6.28e+04\n",
      "Epoch 048 Train loss 6.34e+01 Valid loss 6.45e+01 Error: 2.60e+04\n",
      "Epoch 049 Train loss 6.45e+01 Valid loss 6.35e+01 Error: 1.31e+04\n",
      "Epoch 050 Train loss 6.35e+01 Valid loss 6.49e+01 Error: 3.40e+04\n",
      "Epoch 051 Train loss 6.49e+01 Valid loss 6.25e+01 Error: 5.35e+04\n",
      "Epoch 052 Train loss 6.25e+01 Valid loss 6.81e+01 Error: 8.80e+04\n",
      "Epoch 053 Train loss 6.81e+01 Valid loss 6.98e+01 Error: 1.13e+05\n",
      "Epoch 054 Train loss 6.98e+01 Valid loss 7.06e+01 Error: 1.30e+05\n",
      "Epoch 055 Train loss 7.06e+01 Valid loss 7.08e+01 Error: 1.41e+05\n",
      "Epoch 056 Train loss 7.08e+01 Valid loss 7.07e+01 Error: 1.45e+05\n",
      "Epoch 057 Train loss 7.07e+01 Valid loss 6.99e+01 Error: 1.42e+05\n",
      "Epoch 058 Train loss 6.99e+01 Valid loss 6.65e+01 Error: 1.32e+05\n",
      "Epoch 059 Train loss 6.65e+01 Valid loss 7.01e+01 Error: 1.05e+05\n",
      "Epoch 060 Train loss 7.01e+01 Valid loss 7.10e+01 Error: 8.24e+04\n",
      "Epoch 061 Train loss 7.10e+01 Valid loss 7.09e+01 Error: 5.82e+04\n",
      "Epoch 062 Train loss 7.09e+01 Valid loss 7.00e+01 Error: 3.03e+04\n",
      "Epoch 063 Train loss 7.00e+01 Valid loss 6.62e+01 Error: 4.08e+03\n",
      "Epoch 064 Train loss 6.62e+01 Valid loss 6.98e+01 Error: 2.23e+04\n",
      "Epoch 065 Train loss 6.98e+01 Valid loss 7.09e+01 Error: 3.59e+04\n",
      "Epoch 066 Train loss 7.09e+01 Valid loss 7.13e+01 Error: 4.25e+04\n",
      "Epoch 067 Train loss 7.13e+01 Valid loss 7.13e+01 Error: 4.30e+04\n",
      "Epoch 068 Train loss 7.13e+01 Valid loss 7.10e+01 Error: 3.82e+04\n",
      "Epoch 069 Train loss 7.10e+01 Valid loss 7.03e+01 Error: 2.78e+04\n",
      "Epoch 070 Train loss 7.03e+01 Valid loss 6.79e+01 Error: 9.09e+03\n",
      "Epoch 071 Train loss 6.79e+01 Valid loss 6.99e+01 Error: 2.88e+04\n",
      "Epoch 072 Train loss 6.99e+01 Valid loss 7.08e+01 Error: 5.34e+04\n",
      "Epoch 073 Train loss 7.08e+01 Valid loss 7.09e+01 Error: 7.12e+04\n",
      "Epoch 074 Train loss 7.09e+01 Valid loss 7.05e+01 Error: 8.43e+04\n",
      "Epoch 075 Train loss 7.05e+01 Valid loss 6.96e+01 Error: 9.41e+04\n",
      "Epoch 076 Train loss 6.96e+01 Valid loss 6.71e+01 Error: 1.04e+05\n",
      "Epoch 077 Train loss 6.71e+01 Valid loss 6.96e+01 Error: 1.24e+05\n",
      "Epoch 078 Train loss 6.96e+01 Valid loss 7.05e+01 Error: 1.30e+05\n",
      "Epoch 079 Train loss 7.05e+01 Valid loss 7.04e+01 Error: 1.25e+05\n",
      "Epoch 080 Train loss 7.04e+01 Valid loss 6.96e+01 Error: 1.11e+05\n",
      "Epoch 081 Train loss 6.96e+01 Valid loss 6.73e+01 Error: 8.79e+04\n",
      "Epoch 082 Train loss 6.73e+01 Valid loss 6.62e+01 Error: 5.20e+04\n",
      "Epoch 083 Train loss 6.62e+01 Valid loss 6.57e+01 Error: 2.55e+04\n",
      "Epoch 084 Train loss 6.57e+01 Valid loss 6.34e+01 Error: 9.19e+03\n",
      "Epoch 085 Train loss 6.34e+01 Valid loss 6.20e+01 Error: 7.48e+03\n",
      "Epoch 086 Train loss 6.20e+01 Valid loss 6.18e+01 Error: 2.02e+04\n",
      "Epoch 087 Train loss 6.18e+01 Valid loss 6.29e+01 Error: 3.55e+04\n",
      "Epoch 088 Train loss 6.29e+01 Valid loss 6.09e+01 Error: 2.08e+04\n",
      "Epoch 089 Train loss 6.09e+01 Valid loss 6.03e+01 Error: 1.24e+04\n",
      "Epoch 090 Train loss 6.03e+01 Valid loss 5.55e+01 Error: 7.77e+03\n",
      "Epoch 091 Train loss 5.55e+01 Valid loss 6.17e+01 Error: 1.58e+04\n",
      "Epoch 092 Train loss 6.17e+01 Valid loss 6.33e+01 Error: 1.03e+04\n",
      "Epoch 093 Train loss 6.33e+01 Valid loss 6.48e+01 Error: 1.41e+04\n",
      "Epoch 094 Train loss 6.48e+01 Valid loss 6.51e+01 Error: 1.35e+04\n",
      "Epoch 095 Train loss 6.51e+01 Valid loss 6.35e+01 Error: 5.85e+03\n",
      "Epoch 096 Train loss 6.35e+01 Valid loss 6.56e+01 Error: 1.87e+04\n",
      "Epoch 097 Train loss 6.56e+01 Valid loss 6.58e+01 Error: 2.64e+04\n",
      "Epoch 098 Train loss 6.58e+01 Valid loss 6.48e+01 Error: 2.56e+04\n",
      "Epoch 099 Train loss 6.48e+01 Valid loss 6.18e+01 Error: 1.50e+04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-23 20:00:38,810] Trial 23 finished with value: 48.171871185302734 and parameters: {'learning_rate': 2.3935358414263838e-05, 'weight_decay': 2.713245928455546e-09, 'n_layers': 2, 'hidden_channels': 512, 'r_link': 56}. Best is trial 10 with value: 35.48434066772461.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 Train loss 6.18e+01 Valid loss 5.54e+01 Error: 1.02e+04\n",
      "Epoch 001 Train loss 5.18e+01 Valid loss 4.19e+01 Error: 1.42e+01 (B)\n",
      "Epoch 002 Train loss 4.19e+01 Valid loss 5.32e+01 Error: 4.63e+03\n",
      "Epoch 003 Train loss 5.32e+01 Valid loss 5.29e+01 Error: 7.64e+03\n",
      "Epoch 004 Train loss 5.29e+01 Valid loss 5.03e+01 Error: 1.02e+04\n",
      "Epoch 005 Train loss 5.03e+01 Valid loss 5.19e+01 Error: 1.14e+04\n",
      "Epoch 006 Train loss 5.19e+01 Valid loss 4.97e+01 Error: 1.17e+04\n",
      "Epoch 007 Train loss 4.97e+01 Valid loss 5.05e+01 Error: 1.23e+04\n",
      "Epoch 008 Train loss 5.05e+01 Valid loss 4.95e+01 Error: 1.25e+04\n",
      "Epoch 009 Train loss 4.95e+01 Valid loss 5.24e+01 Error: 1.32e+04\n",
      "Epoch 010 Train loss 5.24e+01 Valid loss 5.33e+01 Error: 1.36e+04\n",
      "Epoch 011 Train loss 5.33e+01 Valid loss 5.35e+01 Error: 1.39e+04\n",
      "Epoch 012 Train loss 5.35e+01 Valid loss 5.30e+01 Error: 1.40e+04\n",
      "Epoch 013 Train loss 5.30e+01 Valid loss 5.08e+01 Error: 1.39e+04\n",
      "Epoch 014 Train loss 5.08e+01 Valid loss 5.32e+01 Error: 1.36e+04\n",
      "Epoch 015 Train loss 5.32e+01 Valid loss 5.42e+01 Error: 1.36e+04\n",
      "Epoch 016 Train loss 5.42e+01 Valid loss 5.46e+01 Error: 1.36e+04\n",
      "Epoch 017 Train loss 5.46e+01 Valid loss 5.49e+01 Error: 1.36e+04\n",
      "Epoch 018 Train loss 5.49e+01 Valid loss 5.50e+01 Error: 1.37e+04\n",
      "Epoch 019 Train loss 5.50e+01 Valid loss 5.50e+01 Error: 1.38e+04\n",
      "Epoch 020 Train loss 5.50e+01 Valid loss 5.49e+01 Error: 1.40e+04\n",
      "Epoch 021 Train loss 5.49e+01 Valid loss 5.47e+01 Error: 1.42e+04\n",
      "Epoch 022 Train loss 5.47e+01 Valid loss 5.44e+01 Error: 1.44e+04\n",
      "Epoch 023 Train loss 5.44e+01 Valid loss 5.38e+01 Error: 1.46e+04\n",
      "Epoch 024 Train loss 5.38e+01 Valid loss 5.23e+01 Error: 1.50e+04\n",
      "Epoch 025 Train loss 5.23e+01 Valid loss 5.21e+01 Error: 1.55e+04\n",
      "Epoch 026 Train loss 5.21e+01 Valid loss 5.32e+01 Error: 1.56e+04\n",
      "Epoch 027 Train loss 5.32e+01 Valid loss 5.32e+01 Error: 1.56e+04\n",
      "Epoch 028 Train loss 5.32e+01 Valid loss 5.20e+01 Error: 1.54e+04\n",
      "Epoch 029 Train loss 5.20e+01 Valid loss 5.20e+01 Error: 1.50e+04\n",
      "Epoch 030 Train loss 5.20e+01 Valid loss 5.28e+01 Error: 1.48e+04\n",
      "Epoch 031 Train loss 5.28e+01 Valid loss 5.22e+01 Error: 1.49e+04\n",
      "Epoch 032 Train loss 5.22e+01 Valid loss 5.01e+01 Error: 1.52e+04\n",
      "Epoch 033 Train loss 5.01e+01 Valid loss 5.28e+01 Error: 1.47e+04\n",
      "Epoch 034 Train loss 5.28e+01 Valid loss 5.37e+01 Error: 1.45e+04\n",
      "Epoch 035 Train loss 5.37e+01 Valid loss 5.40e+01 Error: 1.44e+04\n",
      "Epoch 036 Train loss 5.40e+01 Valid loss 5.39e+01 Error: 1.43e+04\n",
      "Epoch 037 Train loss 5.39e+01 Valid loss 5.35e+01 Error: 1.44e+04\n",
      "Epoch 038 Train loss 5.35e+01 Valid loss 5.23e+01 Error: 1.45e+04\n",
      "Epoch 039 Train loss 5.23e+01 Valid loss 5.15e+01 Error: 1.49e+04\n",
      "Epoch 040 Train loss 5.15e+01 Valid loss 5.17e+01 Error: 1.49e+04\n",
      "Epoch 041 Train loss 5.17e+01 Valid loss 5.03e+01 Error: 1.45e+04\n",
      "Epoch 042 Train loss 5.03e+01 Valid loss 5.24e+01 Error: 1.48e+04\n",
      "Epoch 043 Train loss 5.24e+01 Valid loss 5.32e+01 Error: 1.48e+04\n",
      "Epoch 044 Train loss 5.32e+01 Valid loss 5.29e+01 Error: 1.47e+04\n",
      "Epoch 045 Train loss 5.29e+01 Valid loss 5.09e+01 Error: 1.44e+04\n",
      "Epoch 046 Train loss 5.09e+01 Valid loss 5.34e+01 Error: 1.38e+04\n",
      "Epoch 047 Train loss 5.34e+01 Valid loss 5.47e+01 Error: 1.33e+04\n",
      "Epoch 048 Train loss 5.47e+01 Valid loss 5.53e+01 Error: 1.29e+04\n",
      "Epoch 049 Train loss 5.53e+01 Valid loss 5.56e+01 Error: 1.26e+04\n",
      "Epoch 050 Train loss 5.56e+01 Valid loss 5.58e+01 Error: 1.23e+04\n",
      "Epoch 051 Train loss 5.58e+01 Valid loss 5.60e+01 Error: 1.20e+04\n",
      "Epoch 052 Train loss 5.60e+01 Valid loss 5.60e+01 Error: 1.18e+04\n",
      "Epoch 053 Train loss 5.60e+01 Valid loss 5.61e+01 Error: 1.17e+04\n",
      "Epoch 054 Train loss 5.61e+01 Valid loss 5.61e+01 Error: 1.15e+04\n",
      "Epoch 055 Train loss 5.61e+01 Valid loss 5.61e+01 Error: 1.14e+04\n",
      "Epoch 056 Train loss 5.61e+01 Valid loss 5.61e+01 Error: 1.13e+04\n",
      "Epoch 057 Train loss 5.61e+01 Valid loss 5.60e+01 Error: 1.12e+04\n",
      "Epoch 058 Train loss 5.60e+01 Valid loss 5.60e+01 Error: 1.11e+04\n",
      "Epoch 059 Train loss 5.60e+01 Valid loss 5.59e+01 Error: 1.11e+04\n",
      "Epoch 060 Train loss 5.59e+01 Valid loss 5.58e+01 Error: 1.10e+04\n",
      "Epoch 061 Train loss 5.58e+01 Valid loss 5.57e+01 Error: 1.10e+04\n",
      "Epoch 062 Train loss 5.57e+01 Valid loss 5.55e+01 Error: 1.10e+04\n",
      "Epoch 063 Train loss 5.55e+01 Valid loss 5.54e+01 Error: 1.11e+04\n",
      "Epoch 064 Train loss 5.54e+01 Valid loss 5.51e+01 Error: 1.11e+04\n",
      "Epoch 065 Train loss 5.51e+01 Valid loss 5.49e+01 Error: 1.11e+04\n",
      "Epoch 066 Train loss 5.49e+01 Valid loss 5.45e+01 Error: 1.12e+04\n",
      "Epoch 067 Train loss 5.45e+01 Valid loss 5.40e+01 Error: 1.14e+04\n",
      "Epoch 068 Train loss 5.40e+01 Valid loss 5.32e+01 Error: 1.16e+04\n",
      "Epoch 069 Train loss 5.32e+01 Valid loss 5.09e+01 Error: 1.19e+04\n",
      "Epoch 070 Train loss 5.09e+01 Valid loss 5.32e+01 Error: 1.25e+04\n",
      "Epoch 071 Train loss 5.32e+01 Valid loss 5.46e+01 Error: 1.30e+04\n",
      "Epoch 072 Train loss 5.46e+01 Valid loss 5.52e+01 Error: 1.33e+04\n",
      "Epoch 073 Train loss 5.52e+01 Valid loss 5.56e+01 Error: 1.35e+04\n",
      "Epoch 074 Train loss 5.56e+01 Valid loss 5.57e+01 Error: 1.36e+04\n",
      "Epoch 075 Train loss 5.57e+01 Valid loss 5.58e+01 Error: 1.35e+04\n",
      "Epoch 076 Train loss 5.58e+01 Valid loss 5.59e+01 Error: 1.35e+04\n",
      "Epoch 077 Train loss 5.59e+01 Valid loss 5.59e+01 Error: 1.34e+04\n",
      "Epoch 078 Train loss 5.59e+01 Valid loss 5.58e+01 Error: 1.33e+04\n",
      "Epoch 079 Train loss 5.58e+01 Valid loss 5.58e+01 Error: 1.31e+04\n",
      "Epoch 080 Train loss 5.58e+01 Valid loss 5.57e+01 Error: 1.29e+04\n",
      "Epoch 081 Train loss 5.57e+01 Valid loss 5.56e+01 Error: 1.27e+04\n",
      "Epoch 082 Train loss 5.56e+01 Valid loss 5.55e+01 Error: 1.25e+04\n",
      "Epoch 083 Train loss 5.55e+01 Valid loss 5.53e+01 Error: 1.23e+04\n",
      "Epoch 084 Train loss 5.53e+01 Valid loss 5.51e+01 Error: 1.20e+04\n",
      "Epoch 085 Train loss 5.51e+01 Valid loss 5.49e+01 Error: 1.17e+04\n",
      "Epoch 086 Train loss 5.49e+01 Valid loss 5.47e+01 Error: 1.14e+04\n",
      "Epoch 087 Train loss 5.47e+01 Valid loss 5.43e+01 Error: 1.11e+04\n",
      "Epoch 088 Train loss 5.43e+01 Valid loss 5.39e+01 Error: 1.08e+04\n",
      "Epoch 089 Train loss 5.39e+01 Valid loss 5.34e+01 Error: 1.04e+04\n",
      "Epoch 090 Train loss 5.34e+01 Valid loss 5.25e+01 Error: 9.92e+03\n",
      "Epoch 091 Train loss 5.25e+01 Valid loss 5.08e+01 Error: 9.40e+03\n",
      "Epoch 092 Train loss 5.08e+01 Valid loss 5.01e+01 Error: 8.71e+03\n",
      "Epoch 093 Train loss 5.01e+01 Valid loss 5.13e+01 Error: 8.32e+03\n",
      "Epoch 094 Train loss 5.13e+01 Valid loss 5.15e+01 Error: 8.07e+03\n",
      "Epoch 095 Train loss 5.15e+01 Valid loss 5.12e+01 Error: 7.93e+03\n",
      "Epoch 096 Train loss 5.12e+01 Valid loss 5.04e+01 Error: 7.90e+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-23 20:01:35,292] Trial 24 finished with value: 41.84313201904297 and parameters: {'learning_rate': 0.00012722233993428655, 'weight_decay': 2.2974778802397536e-09, 'n_layers': 2, 'hidden_channels': 64, 'r_link': 142}. Best is trial 10 with value: 35.48434066772461.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 097 Train loss 5.04e+01 Valid loss 4.39e+01 Error: 8.03e+03\n",
      "Epoch 098 Train loss 4.39e+01 Valid loss 5.39e+01 Error: 5.33e+03\n",
      "Epoch 099 Train loss 5.39e+01 Valid loss 5.46e+01 Error: 4.92e+03\n",
      "Epoch 100 Train loss 5.46e+01 Valid loss 5.51e+01 Error: 4.70e+03\n",
      "Epoch 001 Train loss 6.23e+01 Valid loss 6.22e+01 Error: 3.86e+04 (B)\n",
      "Epoch 002 Train loss 6.22e+01 Valid loss 6.20e+01 Error: 3.81e+04 (B)\n",
      "Epoch 003 Train loss 6.20e+01 Valid loss 6.17e+01 Error: 3.74e+04 (B)\n",
      "Epoch 004 Train loss 6.17e+01 Valid loss 6.12e+01 Error: 3.64e+04 (B)\n",
      "Epoch 005 Train loss 6.12e+01 Valid loss 6.05e+01 Error: 3.52e+04 (B)\n",
      "Epoch 006 Train loss 6.05e+01 Valid loss 5.95e+01 Error: 3.39e+04 (B)\n",
      "Epoch 007 Train loss 5.95e+01 Valid loss 5.70e+01 Error: 3.26e+04 (B)\n",
      "Epoch 008 Train loss 5.70e+01 Valid loss 5.74e+01 Error: 3.14e+04\n",
      "Epoch 009 Train loss 5.74e+01 Valid loss 5.82e+01 Error: 3.08e+04\n",
      "Epoch 010 Train loss 5.82e+01 Valid loss 5.82e+01 Error: 3.04e+04\n",
      "Epoch 011 Train loss 5.82e+01 Valid loss 5.75e+01 Error: 3.01e+04\n",
      "Epoch 012 Train loss 5.75e+01 Valid loss 5.39e+01 Error: 3.01e+04 (B)\n",
      "Epoch 013 Train loss 5.39e+01 Valid loss 5.82e+01 Error: 3.08e+04\n",
      "Epoch 014 Train loss 5.82e+01 Valid loss 5.95e+01 Error: 3.13e+04\n",
      "Epoch 015 Train loss 5.95e+01 Valid loss 6.02e+01 Error: 3.18e+04\n",
      "Epoch 016 Train loss 6.02e+01 Valid loss 6.07e+01 Error: 3.21e+04\n",
      "Epoch 017 Train loss 6.07e+01 Valid loss 6.10e+01 Error: 3.24e+04\n",
      "Epoch 018 Train loss 6.10e+01 Valid loss 6.12e+01 Error: 3.24e+04\n",
      "Epoch 019 Train loss 6.12e+01 Valid loss 6.14e+01 Error: 3.24e+04\n",
      "Epoch 020 Train loss 6.14e+01 Valid loss 6.15e+01 Error: 3.24e+04\n",
      "Epoch 021 Train loss 6.15e+01 Valid loss 6.16e+01 Error: 3.22e+04\n",
      "Epoch 022 Train loss 6.16e+01 Valid loss 6.17e+01 Error: 3.20e+04\n",
      "Epoch 023 Train loss 6.17e+01 Valid loss 6.17e+01 Error: 3.16e+04\n",
      "Epoch 024 Train loss 6.17e+01 Valid loss 6.17e+01 Error: 3.12e+04\n",
      "Epoch 025 Train loss 6.17e+01 Valid loss 6.16e+01 Error: 3.07e+04\n",
      "Epoch 026 Train loss 6.16e+01 Valid loss 6.16e+01 Error: 3.01e+04\n",
      "Epoch 027 Train loss 6.16e+01 Valid loss 6.15e+01 Error: 2.94e+04\n",
      "Epoch 028 Train loss 6.15e+01 Valid loss 6.14e+01 Error: 2.87e+04\n",
      "Epoch 029 Train loss 6.14e+01 Valid loss 6.12e+01 Error: 2.79e+04\n",
      "Epoch 030 Train loss 6.12e+01 Valid loss 6.11e+01 Error: 2.69e+04\n",
      "Epoch 031 Train loss 6.11e+01 Valid loss 6.09e+01 Error: 2.59e+04\n",
      "Epoch 032 Train loss 6.09e+01 Valid loss 6.06e+01 Error: 2.49e+04\n",
      "Epoch 033 Train loss 6.06e+01 Valid loss 6.04e+01 Error: 2.37e+04\n",
      "Epoch 034 Train loss 6.04e+01 Valid loss 6.01e+01 Error: 2.24e+04\n",
      "Epoch 035 Train loss 6.01e+01 Valid loss 5.97e+01 Error: 2.10e+04\n",
      "Epoch 036 Train loss 5.97e+01 Valid loss 5.93e+01 Error: 1.95e+04\n",
      "Epoch 037 Train loss 5.93e+01 Valid loss 5.87e+01 Error: 1.79e+04\n",
      "Epoch 038 Train loss 5.87e+01 Valid loss 5.81e+01 Error: 1.61e+04\n",
      "Epoch 039 Train loss 5.81e+01 Valid loss 5.72e+01 Error: 1.40e+04\n",
      "Epoch 040 Train loss 5.72e+01 Valid loss 5.61e+01 Error: 1.16e+04\n",
      "Epoch 041 Train loss 5.61e+01 Valid loss 5.42e+01 Error: 8.87e+03\n",
      "Epoch 042 Train loss 5.42e+01 Valid loss 4.66e+01 Error: 5.34e+03 (B)\n",
      "Epoch 043 Train loss 4.66e+01 Valid loss 5.39e+01 Error: 7.99e+03\n",
      "Epoch 044 Train loss 5.39e+01 Valid loss 5.47e+01 Error: 1.03e+04\n",
      "Epoch 045 Train loss 5.47e+01 Valid loss 5.34e+01 Error: 1.17e+04\n",
      "Epoch 046 Train loss 5.34e+01 Valid loss 5.47e+01 Error: 1.24e+04\n",
      "Epoch 047 Train loss 5.47e+01 Valid loss 5.64e+01 Error: 1.36e+04\n",
      "Epoch 048 Train loss 5.64e+01 Valid loss 5.72e+01 Error: 1.49e+04\n",
      "Epoch 049 Train loss 5.72e+01 Valid loss 5.77e+01 Error: 1.62e+04\n",
      "Epoch 050 Train loss 5.77e+01 Valid loss 5.80e+01 Error: 1.75e+04\n",
      "Epoch 051 Train loss 5.80e+01 Valid loss 5.81e+01 Error: 1.87e+04\n",
      "Epoch 052 Train loss 5.81e+01 Valid loss 5.79e+01 Error: 2.01e+04\n",
      "Epoch 053 Train loss 5.79e+01 Valid loss 5.71e+01 Error: 2.15e+04\n",
      "Epoch 054 Train loss 5.71e+01 Valid loss 5.04e+01 Error: 2.32e+04\n",
      "Epoch 055 Train loss 5.04e+01 Valid loss 5.98e+01 Error: 1.93e+04\n",
      "Epoch 056 Train loss 5.98e+01 Valid loss 6.06e+01 Error: 1.52e+04\n",
      "Epoch 057 Train loss 6.06e+01 Valid loss 6.07e+01 Error: 1.16e+04\n",
      "Epoch 058 Train loss 6.07e+01 Valid loss 6.06e+01 Error: 8.94e+03\n",
      "Epoch 059 Train loss 6.06e+01 Valid loss 6.02e+01 Error: 6.53e+03\n",
      "Epoch 060 Train loss 6.02e+01 Valid loss 5.97e+01 Error: 4.56e+03\n",
      "Epoch 061 Train loss 5.97e+01 Valid loss 5.88e+01 Error: 2.76e+03\n",
      "Epoch 062 Train loss 5.88e+01 Valid loss 5.73e+01 Error: 1.18e+03\n",
      "Epoch 063 Train loss 5.73e+01 Valid loss 5.73e+01 Error: 1.12e+03\n",
      "Epoch 064 Train loss 5.73e+01 Valid loss 5.87e+01 Error: 2.21e+03\n",
      "Epoch 065 Train loss 5.87e+01 Valid loss 5.92e+01 Error: 2.68e+03\n",
      "Epoch 066 Train loss 5.92e+01 Valid loss 5.92e+01 Error: 2.61e+03\n",
      "Epoch 067 Train loss 5.92e+01 Valid loss 5.88e+01 Error: 2.07e+03\n",
      "Epoch 068 Train loss 5.88e+01 Valid loss 5.75e+01 Error: 1.01e+03\n",
      "Epoch 069 Train loss 5.75e+01 Valid loss 5.72e+01 Error: 8.62e+02\n",
      "Epoch 070 Train loss 5.72e+01 Valid loss 5.80e+01 Error: 1.27e+03\n",
      "Epoch 071 Train loss 5.80e+01 Valid loss 5.71e+01 Error: 8.15e+02\n",
      "Epoch 072 Train loss 5.71e+01 Valid loss 5.73e+01 Error: 8.80e+02\n",
      "Epoch 073 Train loss 5.73e+01 Valid loss 5.80e+01 Error: 1.28e+03\n",
      "Epoch 074 Train loss 5.80e+01 Valid loss 5.72e+01 Error: 8.49e+02\n",
      "Epoch 075 Train loss 5.72e+01 Valid loss 5.70e+01 Error: 7.45e+02\n",
      "Epoch 076 Train loss 5.70e+01 Valid loss 5.70e+01 Error: 7.75e+02\n",
      "Epoch 077 Train loss 5.70e+01 Valid loss 5.63e+01 Error: 5.29e+02\n",
      "Epoch 078 Train loss 5.63e+01 Valid loss 5.44e+01 Error: 2.13e+02\n",
      "Epoch 079 Train loss 5.44e+01 Valid loss 5.99e+01 Error: 3.38e+03\n",
      "Epoch 080 Train loss 5.99e+01 Valid loss 6.11e+01 Error: 6.42e+03\n",
      "Epoch 081 Train loss 6.11e+01 Valid loss 6.17e+01 Error: 9.08e+03\n",
      "Epoch 082 Train loss 6.17e+01 Valid loss 6.20e+01 Error: 1.11e+04\n",
      "Epoch 083 Train loss 6.20e+01 Valid loss 6.22e+01 Error: 1.29e+04\n",
      "Epoch 084 Train loss 6.22e+01 Valid loss 6.23e+01 Error: 1.45e+04\n",
      "Epoch 085 Train loss 6.23e+01 Valid loss 6.24e+01 Error: 1.59e+04\n",
      "Epoch 086 Train loss 6.24e+01 Valid loss 6.24e+01 Error: 1.70e+04\n",
      "Epoch 087 Train loss 6.24e+01 Valid loss 6.24e+01 Error: 1.80e+04\n",
      "Epoch 088 Train loss 6.24e+01 Valid loss 6.24e+01 Error: 1.87e+04\n",
      "Epoch 089 Train loss 6.24e+01 Valid loss 6.23e+01 Error: 1.94e+04\n",
      "Epoch 090 Train loss 6.23e+01 Valid loss 6.22e+01 Error: 2.00e+04\n",
      "Epoch 091 Train loss 6.22e+01 Valid loss 6.22e+01 Error: 2.06e+04\n",
      "Epoch 092 Train loss 6.22e+01 Valid loss 6.21e+01 Error: 2.11e+04\n",
      "Epoch 093 Train loss 6.21e+01 Valid loss 6.19e+01 Error: 2.15e+04\n",
      "Epoch 094 Train loss 6.19e+01 Valid loss 6.18e+01 Error: 2.19e+04\n",
      "Epoch 095 Train loss 6.18e+01 Valid loss 6.16e+01 Error: 2.22e+04\n",
      "Epoch 096 Train loss 6.16e+01 Valid loss 6.15e+01 Error: 2.25e+04\n",
      "Epoch 097 Train loss 6.15e+01 Valid loss 6.12e+01 Error: 2.27e+04\n",
      "Epoch 098 Train loss 6.12e+01 Valid loss 6.10e+01 Error: 2.29e+04\n",
      "Epoch 099 Train loss 6.10e+01 Valid loss 6.07e+01 Error: 2.31e+04\n",
      "Epoch 100 Train loss 6.07e+01 Valid loss 6.03e+01 Error: 2.34e+04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-23 20:02:32,754] Trial 25 finished with value: 46.63982009887695 and parameters: {'learning_rate': 6.063975376570418e-06, 'weight_decay': 1.8688041703846132e-08, 'n_layers': 3, 'hidden_channels': 64, 'r_link': 266}. Best is trial 10 with value: 35.48434066772461.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 Train loss 6.08e+01 Valid loss 6.07e+01 Error: 2.49e+04 (B)\n",
      "Epoch 002 Train loss 6.07e+01 Valid loss 6.06e+01 Error: 2.46e+04 (B)\n",
      "Epoch 003 Train loss 6.06e+01 Valid loss 6.05e+01 Error: 2.42e+04 (B)\n",
      "Epoch 004 Train loss 6.05e+01 Valid loss 6.04e+01 Error: 2.38e+04 (B)\n",
      "Epoch 005 Train loss 6.04e+01 Valid loss 6.03e+01 Error: 2.32e+04 (B)\n",
      "Epoch 006 Train loss 6.03e+01 Valid loss 6.01e+01 Error: 2.26e+04 (B)\n",
      "Epoch 007 Train loss 6.01e+01 Valid loss 5.99e+01 Error: 2.19e+04 (B)\n",
      "Epoch 008 Train loss 5.99e+01 Valid loss 5.97e+01 Error: 2.11e+04 (B)\n",
      "Epoch 009 Train loss 5.97e+01 Valid loss 5.95e+01 Error: 2.02e+04 (B)\n",
      "Epoch 010 Train loss 5.95e+01 Valid loss 5.91e+01 Error: 1.93e+04 (B)\n",
      "Epoch 011 Train loss 5.91e+01 Valid loss 5.88e+01 Error: 1.82e+04 (B)\n",
      "Epoch 012 Train loss 5.88e+01 Valid loss 5.83e+01 Error: 1.70e+04 (B)\n",
      "Epoch 013 Train loss 5.83e+01 Valid loss 5.78e+01 Error: 1.57e+04 (B)\n",
      "Epoch 014 Train loss 5.78e+01 Valid loss 5.73e+01 Error: 1.45e+04 (B)\n",
      "Epoch 015 Train loss 5.73e+01 Valid loss 5.66e+01 Error: 1.32e+04 (B)\n",
      "Epoch 016 Train loss 5.66e+01 Valid loss 5.59e+01 Error: 1.19e+04 (B)\n",
      "Epoch 017 Train loss 5.59e+01 Valid loss 5.48e+01 Error: 1.05e+04 (B)\n",
      "Epoch 018 Train loss 5.48e+01 Valid loss 5.32e+01 Error: 8.96e+03 (B)\n",
      "Epoch 019 Train loss 5.32e+01 Valid loss 4.73e+01 Error: 7.36e+03 (B)\n",
      "Epoch 020 Train loss 4.73e+01 Valid loss 5.13e+01 Error: 6.44e+03\n",
      "Epoch 021 Train loss 5.13e+01 Valid loss 5.23e+01 Error: 5.63e+03\n",
      "Epoch 022 Train loss 5.23e+01 Valid loss 5.26e+01 Error: 4.88e+03\n",
      "Epoch 023 Train loss 5.26e+01 Valid loss 5.26e+01 Error: 4.18e+03\n",
      "Epoch 024 Train loss 5.26e+01 Valid loss 5.25e+01 Error: 3.53e+03\n",
      "Epoch 025 Train loss 5.25e+01 Valid loss 5.21e+01 Error: 2.90e+03\n",
      "Epoch 026 Train loss 5.21e+01 Valid loss 5.16e+01 Error: 2.20e+03\n",
      "Epoch 027 Train loss 5.16e+01 Valid loss 5.06e+01 Error: 1.44e+03\n",
      "Epoch 028 Train loss 5.06e+01 Valid loss 4.84e+01 Error: 5.30e+02\n",
      "Epoch 029 Train loss 4.84e+01 Valid loss 4.82e+01 Error: 5.53e+02\n",
      "Epoch 030 Train loss 4.82e+01 Valid loss 4.93e+01 Error: 1.04e+03\n",
      "Epoch 031 Train loss 4.93e+01 Valid loss 4.96e+01 Error: 1.37e+03\n",
      "Epoch 032 Train loss 4.96e+01 Valid loss 4.96e+01 Error: 1.59e+03\n",
      "Epoch 033 Train loss 4.96e+01 Valid loss 4.94e+01 Error: 1.71e+03\n",
      "Epoch 034 Train loss 4.94e+01 Valid loss 4.89e+01 Error: 1.75e+03\n",
      "Epoch 035 Train loss 4.89e+01 Valid loss 4.83e+01 Error: 1.72e+03\n",
      "Epoch 036 Train loss 4.83e+01 Valid loss 4.74e+01 Error: 1.64e+03\n",
      "Epoch 037 Train loss 4.74e+01 Valid loss 4.60e+01 Error: 1.51e+03 (B)\n",
      "Epoch 038 Train loss 4.60e+01 Valid loss 4.35e+01 Error: 1.34e+03 (B)\n",
      "Epoch 039 Train loss 4.35e+01 Valid loss 4.23e+01 Error: 1.29e+03 (B)\n",
      "Epoch 040 Train loss 4.23e+01 Valid loss 3.78e+01 Error: 5.53e+02 (B)\n",
      "Epoch 041 Train loss 3.78e+01 Valid loss 3.27e+01 Error: 8.53e+02 (B)\n",
      "Epoch 042 Train loss 3.28e+01 Valid loss 4.62e+01 Error: 2.22e+03\n",
      "Epoch 043 Train loss 4.62e+01 Valid loss 4.82e+01 Error: 3.28e+03\n",
      "Epoch 044 Train loss 4.82e+01 Valid loss 4.94e+01 Error: 4.23e+03\n",
      "Epoch 045 Train loss 4.94e+01 Valid loss 5.02e+01 Error: 5.14e+03\n",
      "Epoch 046 Train loss 5.02e+01 Valid loss 5.05e+01 Error: 5.85e+03\n",
      "Epoch 047 Train loss 5.05e+01 Valid loss 5.05e+01 Error: 6.50e+03\n",
      "Epoch 048 Train loss 5.05e+01 Valid loss 5.03e+01 Error: 7.10e+03\n",
      "Epoch 049 Train loss 5.03e+01 Valid loss 4.90e+01 Error: 7.59e+03\n",
      "Epoch 050 Train loss 4.90e+01 Valid loss 4.93e+01 Error: 7.88e+03\n",
      "Epoch 051 Train loss 4.93e+01 Valid loss 5.08e+01 Error: 8.27e+03\n",
      "Epoch 052 Train loss 5.08e+01 Valid loss 5.13e+01 Error: 8.70e+03\n",
      "Epoch 053 Train loss 5.13e+01 Valid loss 5.13e+01 Error: 9.19e+03\n",
      "Epoch 054 Train loss 5.13e+01 Valid loss 5.03e+01 Error: 9.73e+03\n",
      "Epoch 055 Train loss 5.03e+01 Valid loss 5.00e+01 Error: 1.04e+04\n",
      "Epoch 056 Train loss 5.00e+01 Valid loss 5.04e+01 Error: 1.08e+04\n",
      "Epoch 057 Train loss 5.04e+01 Valid loss 4.75e+01 Error: 1.10e+04\n",
      "Epoch 058 Train loss 4.75e+01 Valid loss 5.29e+01 Error: 1.19e+04\n",
      "Epoch 059 Train loss 5.29e+01 Valid loss 5.43e+01 Error: 1.26e+04\n",
      "Epoch 060 Train loss 5.43e+01 Valid loss 5.51e+01 Error: 1.33e+04\n",
      "Epoch 061 Train loss 5.51e+01 Valid loss 5.57e+01 Error: 1.39e+04\n",
      "Epoch 062 Train loss 5.57e+01 Valid loss 5.61e+01 Error: 1.44e+04\n",
      "Epoch 063 Train loss 5.61e+01 Valid loss 5.64e+01 Error: 1.49e+04\n",
      "Epoch 064 Train loss 5.64e+01 Valid loss 5.67e+01 Error: 1.54e+04\n",
      "Epoch 065 Train loss 5.67e+01 Valid loss 5.69e+01 Error: 1.57e+04\n",
      "Epoch 066 Train loss 5.69e+01 Valid loss 5.70e+01 Error: 1.61e+04\n",
      "Epoch 067 Train loss 5.70e+01 Valid loss 5.72e+01 Error: 1.64e+04\n",
      "Epoch 068 Train loss 5.72e+01 Valid loss 5.73e+01 Error: 1.66e+04\n",
      "Epoch 069 Train loss 5.73e+01 Valid loss 5.73e+01 Error: 1.68e+04\n",
      "Epoch 070 Train loss 5.73e+01 Valid loss 5.74e+01 Error: 1.70e+04\n",
      "Epoch 071 Train loss 5.74e+01 Valid loss 5.74e+01 Error: 1.71e+04\n",
      "Epoch 072 Train loss 5.74e+01 Valid loss 5.75e+01 Error: 1.72e+04\n",
      "Epoch 073 Train loss 5.75e+01 Valid loss 5.75e+01 Error: 1.73e+04\n",
      "Epoch 074 Train loss 5.75e+01 Valid loss 5.74e+01 Error: 1.73e+04\n",
      "Epoch 075 Train loss 5.74e+01 Valid loss 5.74e+01 Error: 1.73e+04\n",
      "Epoch 076 Train loss 5.74e+01 Valid loss 5.74e+01 Error: 1.73e+04\n",
      "Epoch 077 Train loss 5.74e+01 Valid loss 5.73e+01 Error: 1.73e+04\n",
      "Epoch 078 Train loss 5.73e+01 Valid loss 5.73e+01 Error: 1.73e+04\n",
      "Epoch 079 Train loss 5.73e+01 Valid loss 5.72e+01 Error: 1.72e+04\n",
      "Epoch 080 Train loss 5.72e+01 Valid loss 5.71e+01 Error: 1.72e+04\n",
      "Epoch 081 Train loss 5.71e+01 Valid loss 5.70e+01 Error: 1.71e+04\n",
      "Epoch 082 Train loss 5.70e+01 Valid loss 5.68e+01 Error: 1.70e+04\n",
      "Epoch 083 Train loss 5.68e+01 Valid loss 5.66e+01 Error: 1.69e+04\n",
      "Epoch 084 Train loss 5.66e+01 Valid loss 5.64e+01 Error: 1.67e+04\n",
      "Epoch 085 Train loss 5.64e+01 Valid loss 5.62e+01 Error: 1.65e+04\n",
      "Epoch 086 Train loss 5.62e+01 Valid loss 5.58e+01 Error: 1.64e+04\n",
      "Epoch 087 Train loss 5.58e+01 Valid loss 5.54e+01 Error: 1.61e+04\n",
      "Epoch 088 Train loss 5.54e+01 Valid loss 5.48e+01 Error: 1.59e+04\n",
      "Epoch 089 Train loss 5.48e+01 Valid loss 5.38e+01 Error: 1.56e+04\n",
      "Epoch 090 Train loss 5.38e+01 Valid loss 5.09e+01 Error: 1.53e+04\n",
      "Epoch 091 Train loss 5.09e+01 Valid loss 5.40e+01 Error: 1.46e+04\n",
      "Epoch 092 Train loss 5.40e+01 Valid loss 5.53e+01 Error: 1.40e+04\n",
      "Epoch 093 Train loss 5.53e+01 Valid loss 5.59e+01 Error: 1.36e+04\n",
      "Epoch 094 Train loss 5.59e+01 Valid loss 5.63e+01 Error: 1.32e+04\n",
      "Epoch 095 Train loss 5.63e+01 Valid loss 5.66e+01 Error: 1.28e+04\n",
      "Epoch 096 Train loss 5.66e+01 Valid loss 5.68e+01 Error: 1.25e+04\n",
      "Epoch 097 Train loss 5.68e+01 Valid loss 5.69e+01 Error: 1.22e+04\n",
      "Epoch 098 Train loss 5.69e+01 Valid loss 5.70e+01 Error: 1.19e+04\n",
      "Epoch 099 Train loss 5.70e+01 Valid loss 5.70e+01 Error: 1.16e+04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-23 20:03:27,880] Trial 26 finished with value: 32.76902770996094 and parameters: {'learning_rate': 3.783175782197047e-06, 'weight_decay': 5.068840881372831e-09, 'n_layers': 1, 'hidden_channels': 64, 'r_link': 245}. Best is trial 26 with value: 32.76902770996094.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 Train loss 5.70e+01 Valid loss 5.71e+01 Error: 1.14e+04\n",
      "Epoch 001 Train loss 6.24e+01 Valid loss 6.24e+01 Error: 3.31e+04 (B)\n",
      "Epoch 002 Train loss 6.24e+01 Valid loss 6.23e+01 Error: 3.27e+04 (B)\n",
      "Epoch 003 Train loss 6.23e+01 Valid loss 6.22e+01 Error: 3.21e+04 (B)\n",
      "Epoch 004 Train loss 6.22e+01 Valid loss 6.21e+01 Error: 3.14e+04 (B)\n",
      "Epoch 005 Train loss 6.21e+01 Valid loss 6.19e+01 Error: 3.06e+04 (B)\n",
      "Epoch 006 Train loss 6.19e+01 Valid loss 6.17e+01 Error: 2.97e+04 (B)\n",
      "Epoch 007 Train loss 6.17e+01 Valid loss 6.15e+01 Error: 2.87e+04 (B)\n",
      "Epoch 008 Train loss 6.15e+01 Valid loss 6.13e+01 Error: 2.76e+04 (B)\n",
      "Epoch 009 Train loss 6.13e+01 Valid loss 6.10e+01 Error: 2.64e+04 (B)\n",
      "Epoch 010 Train loss 6.10e+01 Valid loss 6.07e+01 Error: 2.50e+04 (B)\n",
      "Epoch 011 Train loss 6.07e+01 Valid loss 6.03e+01 Error: 2.36e+04 (B)\n",
      "Epoch 012 Train loss 6.03e+01 Valid loss 5.99e+01 Error: 2.21e+04 (B)\n",
      "Epoch 013 Train loss 5.99e+01 Valid loss 5.94e+01 Error: 2.05e+04 (B)\n",
      "Epoch 014 Train loss 5.94e+01 Valid loss 5.89e+01 Error: 1.89e+04 (B)\n",
      "Epoch 015 Train loss 5.89e+01 Valid loss 5.83e+01 Error: 1.71e+04 (B)\n",
      "Epoch 016 Train loss 5.83e+01 Valid loss 5.74e+01 Error: 1.50e+04 (B)\n",
      "Epoch 017 Train loss 5.74e+01 Valid loss 5.63e+01 Error: 1.26e+04 (B)\n",
      "Epoch 018 Train loss 5.63e+01 Valid loss 5.47e+01 Error: 1.02e+04 (B)\n",
      "Epoch 019 Train loss 5.47e+01 Valid loss 5.24e+01 Error: 7.75e+03 (B)\n",
      "Epoch 020 Train loss 5.24e+01 Valid loss 4.41e+01 Error: 5.32e+03 (B)\n",
      "Epoch 021 Train loss 4.41e+01 Valid loss 5.04e+01 Error: 5.80e+03\n",
      "Epoch 022 Train loss 5.04e+01 Valid loss 5.17e+01 Error: 6.16e+03\n",
      "Epoch 023 Train loss 5.17e+01 Valid loss 5.23e+01 Error: 6.40e+03\n",
      "Epoch 024 Train loss 5.23e+01 Valid loss 5.26e+01 Error: 6.49e+03\n",
      "Epoch 025 Train loss 5.26e+01 Valid loss 5.26e+01 Error: 6.45e+03\n",
      "Epoch 026 Train loss 5.26e+01 Valid loss 5.24e+01 Error: 6.28e+03\n",
      "Epoch 027 Train loss 5.24e+01 Valid loss 5.19e+01 Error: 5.97e+03\n",
      "Epoch 028 Train loss 5.19e+01 Valid loss 5.07e+01 Error: 5.50e+03\n",
      "Epoch 029 Train loss 5.07e+01 Valid loss 4.41e+01 Error: 4.80e+03 (B)\n",
      "Epoch 030 Train loss 4.41e+01 Valid loss 5.17e+01 Error: 6.35e+03\n",
      "Epoch 031 Train loss 5.17e+01 Valid loss 5.35e+01 Error: 7.75e+03\n",
      "Epoch 032 Train loss 5.35e+01 Valid loss 5.45e+01 Error: 8.94e+03\n",
      "Epoch 033 Train loss 5.45e+01 Valid loss 5.52e+01 Error: 9.97e+03\n",
      "Epoch 034 Train loss 5.52e+01 Valid loss 5.58e+01 Error: 1.09e+04\n",
      "Epoch 035 Train loss 5.58e+01 Valid loss 5.62e+01 Error: 1.17e+04\n",
      "Epoch 036 Train loss 5.62e+01 Valid loss 5.66e+01 Error: 1.25e+04\n",
      "Epoch 037 Train loss 5.66e+01 Valid loss 5.69e+01 Error: 1.31e+04\n",
      "Epoch 038 Train loss 5.69e+01 Valid loss 5.71e+01 Error: 1.37e+04\n",
      "Epoch 039 Train loss 5.71e+01 Valid loss 5.73e+01 Error: 1.42e+04\n",
      "Epoch 040 Train loss 5.73e+01 Valid loss 5.75e+01 Error: 1.46e+04\n",
      "Epoch 041 Train loss 5.75e+01 Valid loss 5.76e+01 Error: 1.49e+04\n",
      "Epoch 042 Train loss 5.76e+01 Valid loss 5.76e+01 Error: 1.51e+04\n",
      "Epoch 043 Train loss 5.76e+01 Valid loss 5.77e+01 Error: 1.53e+04\n",
      "Epoch 044 Train loss 5.77e+01 Valid loss 5.77e+01 Error: 1.54e+04\n",
      "Epoch 045 Train loss 5.77e+01 Valid loss 5.78e+01 Error: 1.55e+04\n",
      "Epoch 046 Train loss 5.78e+01 Valid loss 5.78e+01 Error: 1.55e+04\n",
      "Epoch 047 Train loss 5.78e+01 Valid loss 5.78e+01 Error: 1.55e+04\n",
      "Epoch 048 Train loss 5.78e+01 Valid loss 5.77e+01 Error: 1.55e+04\n",
      "Epoch 049 Train loss 5.77e+01 Valid loss 5.77e+01 Error: 1.54e+04\n",
      "Epoch 050 Train loss 5.77e+01 Valid loss 5.77e+01 Error: 1.54e+04\n",
      "Epoch 051 Train loss 5.77e+01 Valid loss 5.76e+01 Error: 1.52e+04\n",
      "Epoch 052 Train loss 5.76e+01 Valid loss 5.76e+01 Error: 1.51e+04\n",
      "Epoch 053 Train loss 5.76e+01 Valid loss 5.75e+01 Error: 1.49e+04\n",
      "Epoch 054 Train loss 5.75e+01 Valid loss 5.74e+01 Error: 1.47e+04\n",
      "Epoch 055 Train loss 5.74e+01 Valid loss 5.73e+01 Error: 1.45e+04\n",
      "Epoch 056 Train loss 5.73e+01 Valid loss 5.71e+01 Error: 1.42e+04\n",
      "Epoch 057 Train loss 5.71e+01 Valid loss 5.70e+01 Error: 1.39e+04\n",
      "Epoch 058 Train loss 5.70e+01 Valid loss 5.69e+01 Error: 1.36e+04\n",
      "Epoch 059 Train loss 5.69e+01 Valid loss 5.67e+01 Error: 1.33e+04\n",
      "Epoch 060 Train loss 5.67e+01 Valid loss 5.66e+01 Error: 1.30e+04\n",
      "Epoch 061 Train loss 5.66e+01 Valid loss 5.64e+01 Error: 1.27e+04\n",
      "Epoch 062 Train loss 5.64e+01 Valid loss 5.62e+01 Error: 1.23e+04\n",
      "Epoch 063 Train loss 5.62e+01 Valid loss 5.60e+01 Error: 1.19e+04\n",
      "Epoch 064 Train loss 5.60e+01 Valid loss 5.57e+01 Error: 1.15e+04\n",
      "Epoch 065 Train loss 5.57e+01 Valid loss 5.54e+01 Error: 1.10e+04\n",
      "Epoch 066 Train loss 5.54e+01 Valid loss 5.51e+01 Error: 1.05e+04\n",
      "Epoch 067 Train loss 5.51e+01 Valid loss 5.47e+01 Error: 9.92e+03\n",
      "Epoch 068 Train loss 5.47e+01 Valid loss 5.43e+01 Error: 9.30e+03\n",
      "Epoch 069 Train loss 5.43e+01 Valid loss 5.38e+01 Error: 8.65e+03\n",
      "Epoch 070 Train loss 5.38e+01 Valid loss 5.31e+01 Error: 7.94e+03\n",
      "Epoch 071 Train loss 5.31e+01 Valid loss 5.24e+01 Error: 7.24e+03\n",
      "Epoch 072 Train loss 5.24e+01 Valid loss 5.14e+01 Error: 6.53e+03\n",
      "Epoch 073 Train loss 5.14e+01 Valid loss 4.94e+01 Error: 5.73e+03\n",
      "Epoch 074 Train loss 4.94e+01 Valid loss 4.84e+01 Error: 4.79e+03\n",
      "Epoch 075 Train loss 4.84e+01 Valid loss 4.84e+01 Error: 4.21e+03\n",
      "Epoch 076 Train loss 4.84e+01 Valid loss 4.43e+01 Error: 3.86e+03\n",
      "Epoch 077 Train loss 4.43e+01 Valid loss 5.08e+01 Error: 4.87e+03\n",
      "Epoch 078 Train loss 5.08e+01 Valid loss 5.16e+01 Error: 5.58e+03\n",
      "Epoch 079 Train loss 5.16e+01 Valid loss 5.14e+01 Error: 6.23e+03\n",
      "Epoch 080 Train loss 5.14e+01 Valid loss 4.85e+01 Error: 6.61e+03\n",
      "Epoch 081 Train loss 4.85e+01 Valid loss 5.28e+01 Error: 6.19e+03\n",
      "Epoch 082 Train loss 5.28e+01 Valid loss 5.43e+01 Error: 5.79e+03\n",
      "Epoch 083 Train loss 5.43e+01 Valid loss 5.51e+01 Error: 5.48e+03\n",
      "Epoch 084 Train loss 5.51e+01 Valid loss 5.56e+01 Error: 5.32e+03\n",
      "Epoch 085 Train loss 5.56e+01 Valid loss 5.59e+01 Error: 5.18e+03\n",
      "Epoch 086 Train loss 5.59e+01 Valid loss 5.60e+01 Error: 5.07e+03\n",
      "Epoch 087 Train loss 5.60e+01 Valid loss 5.61e+01 Error: 4.90e+03\n",
      "Epoch 088 Train loss 5.61e+01 Valid loss 5.62e+01 Error: 4.69e+03\n",
      "Epoch 089 Train loss 5.62e+01 Valid loss 5.62e+01 Error: 4.41e+03\n",
      "Epoch 090 Train loss 5.62e+01 Valid loss 5.61e+01 Error: 4.03e+03\n",
      "Epoch 091 Train loss 5.61e+01 Valid loss 5.59e+01 Error: 3.52e+03\n",
      "Epoch 092 Train loss 5.59e+01 Valid loss 5.56e+01 Error: 2.91e+03\n",
      "Epoch 093 Train loss 5.56e+01 Valid loss 5.51e+01 Error: 2.22e+03\n",
      "Epoch 094 Train loss 5.51e+01 Valid loss 5.41e+01 Error: 1.30e+03\n",
      "Epoch 095 Train loss 5.41e+01 Valid loss 5.10e+01 Error: 2.68e+02\n",
      "Epoch 096 Train loss 5.10e+01 Valid loss 5.35e+01 Error: 1.03e+03\n",
      "Epoch 097 Train loss 5.35e+01 Valid loss 5.38e+01 Error: 1.27e+03\n",
      "Epoch 098 Train loss 5.38e+01 Valid loss 5.32e+01 Error: 9.96e+02\n",
      "Epoch 099 Train loss 5.32e+01 Valid loss 4.93e+01 Error: 1.48e+02\n",
      "Epoch 100 Train loss 4.93e+01 Valid loss 5.54e+01 Error: 3.48e+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-23 20:04:24,311] Trial 27 finished with value: 44.07023620605469 and parameters: {'learning_rate': 5.758175379898966e-06, 'weight_decay': 1.773895853855709e-09, 'n_layers': 2, 'hidden_channels': 64, 'r_link': 257}. Best is trial 26 with value: 32.76902770996094.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 Train loss 6.53e+01 Valid loss 6.53e+01 Error: 5.48e+04 (B)\n",
      "Epoch 002 Train loss 6.53e+01 Valid loss 6.53e+01 Error: 5.45e+04 (B)\n",
      "Epoch 003 Train loss 6.53e+01 Valid loss 6.52e+01 Error: 5.42e+04 (B)\n",
      "Epoch 004 Train loss 6.52e+01 Valid loss 6.52e+01 Error: 5.36e+04 (B)\n",
      "Epoch 005 Train loss 6.52e+01 Valid loss 6.51e+01 Error: 5.30e+04 (B)\n",
      "Epoch 006 Train loss 6.51e+01 Valid loss 6.50e+01 Error: 5.22e+04 (B)\n",
      "Epoch 007 Train loss 6.50e+01 Valid loss 6.49e+01 Error: 5.12e+04 (B)\n",
      "Epoch 008 Train loss 6.49e+01 Valid loss 6.47e+01 Error: 5.01e+04 (B)\n",
      "Epoch 009 Train loss 6.47e+01 Valid loss 6.46e+01 Error: 4.89e+04 (B)\n",
      "Epoch 010 Train loss 6.46e+01 Valid loss 6.44e+01 Error: 4.76e+04 (B)\n",
      "Epoch 011 Train loss 6.44e+01 Valid loss 6.42e+01 Error: 4.62e+04 (B)\n",
      "Epoch 012 Train loss 6.42e+01 Valid loss 6.40e+01 Error: 4.47e+04 (B)\n",
      "Epoch 013 Train loss 6.40e+01 Valid loss 6.38e+01 Error: 4.31e+04 (B)\n",
      "Epoch 014 Train loss 6.38e+01 Valid loss 6.35e+01 Error: 4.14e+04 (B)\n",
      "Epoch 015 Train loss 6.35e+01 Valid loss 6.32e+01 Error: 3.96e+04 (B)\n",
      "Epoch 016 Train loss 6.32e+01 Valid loss 6.29e+01 Error: 3.76e+04 (B)\n",
      "Epoch 017 Train loss 6.29e+01 Valid loss 6.25e+01 Error: 3.55e+04 (B)\n",
      "Epoch 018 Train loss 6.25e+01 Valid loss 6.21e+01 Error: 3.34e+04 (B)\n",
      "Epoch 019 Train loss 6.21e+01 Valid loss 6.16e+01 Error: 3.10e+04 (B)\n",
      "Epoch 020 Train loss 6.16e+01 Valid loss 6.11e+01 Error: 2.88e+04 (B)\n",
      "Epoch 021 Train loss 6.11e+01 Valid loss 6.05e+01 Error: 2.66e+04 (B)\n",
      "Epoch 022 Train loss 6.05e+01 Valid loss 5.98e+01 Error: 2.44e+04 (B)\n",
      "Epoch 023 Train loss 5.98e+01 Valid loss 5.89e+01 Error: 2.23e+04 (B)\n",
      "Epoch 024 Train loss 5.89e+01 Valid loss 5.78e+01 Error: 2.01e+04 (B)\n",
      "Epoch 025 Train loss 5.78e+01 Valid loss 5.58e+01 Error: 1.80e+04 (B)\n",
      "Epoch 026 Train loss 5.58e+01 Valid loss 5.37e+01 Error: 1.60e+04 (B)\n",
      "Epoch 027 Train loss 5.37e+01 Valid loss 5.30e+01 Error: 1.54e+04 (B)\n",
      "Epoch 028 Train loss 5.30e+01 Valid loss 5.36e+01 Error: 1.56e+04\n",
      "Epoch 029 Train loss 5.36e+01 Valid loss 5.39e+01 Error: 1.54e+04\n",
      "Epoch 030 Train loss 5.39e+01 Valid loss 5.28e+01 Error: 1.48e+04 (B)\n",
      "Epoch 031 Train loss 5.28e+01 Valid loss 5.28e+01 Error: 1.40e+04 (B)\n",
      "Epoch 032 Train loss 5.28e+01 Valid loss 5.33e+01 Error: 1.36e+04\n",
      "Epoch 033 Train loss 5.33e+01 Valid loss 5.21e+01 Error: 1.34e+04 (B)\n",
      "Epoch 034 Train loss 5.21e+01 Valid loss 5.27e+01 Error: 1.36e+04\n",
      "Epoch 035 Train loss 5.27e+01 Valid loss 5.33e+01 Error: 1.34e+04\n",
      "Epoch 036 Train loss 5.33e+01 Valid loss 5.27e+01 Error: 1.29e+04\n",
      "Epoch 037 Train loss 5.27e+01 Valid loss 4.67e+01 Error: 1.20e+04 (B)\n",
      "Epoch 038 Train loss 4.67e+01 Valid loss 5.46e+01 Error: 1.30e+04\n",
      "Epoch 039 Train loss 5.46e+01 Valid loss 5.60e+01 Error: 1.38e+04\n",
      "Epoch 040 Train loss 5.60e+01 Valid loss 5.68e+01 Error: 1.46e+04\n",
      "Epoch 041 Train loss 5.68e+01 Valid loss 5.73e+01 Error: 1.52e+04\n",
      "Epoch 042 Train loss 5.73e+01 Valid loss 5.77e+01 Error: 1.57e+04\n",
      "Epoch 043 Train loss 5.77e+01 Valid loss 5.79e+01 Error: 1.61e+04\n",
      "Epoch 044 Train loss 5.79e+01 Valid loss 5.82e+01 Error: 1.64e+04\n",
      "Epoch 045 Train loss 5.82e+01 Valid loss 5.83e+01 Error: 1.67e+04\n",
      "Epoch 046 Train loss 5.83e+01 Valid loss 5.84e+01 Error: 1.69e+04\n",
      "Epoch 047 Train loss 5.84e+01 Valid loss 5.85e+01 Error: 1.70e+04\n",
      "Epoch 048 Train loss 5.85e+01 Valid loss 5.85e+01 Error: 1.71e+04\n",
      "Epoch 049 Train loss 5.85e+01 Valid loss 5.85e+01 Error: 1.72e+04\n",
      "Epoch 050 Train loss 5.85e+01 Valid loss 5.85e+01 Error: 1.72e+04\n",
      "Epoch 051 Train loss 5.85e+01 Valid loss 5.85e+01 Error: 1.72e+04\n",
      "Epoch 052 Train loss 5.85e+01 Valid loss 5.84e+01 Error: 1.71e+04\n",
      "Epoch 053 Train loss 5.84e+01 Valid loss 5.83e+01 Error: 1.70e+04\n",
      "Epoch 054 Train loss 5.83e+01 Valid loss 5.82e+01 Error: 1.68e+04\n",
      "Epoch 055 Train loss 5.82e+01 Valid loss 5.81e+01 Error: 1.66e+04\n",
      "Epoch 056 Train loss 5.81e+01 Valid loss 5.79e+01 Error: 1.62e+04\n",
      "Epoch 057 Train loss 5.79e+01 Valid loss 5.76e+01 Error: 1.57e+04\n",
      "Epoch 058 Train loss 5.76e+01 Valid loss 5.73e+01 Error: 1.51e+04\n",
      "Epoch 059 Train loss 5.73e+01 Valid loss 5.68e+01 Error: 1.45e+04\n",
      "Epoch 060 Train loss 5.68e+01 Valid loss 5.62e+01 Error: 1.38e+04\n",
      "Epoch 061 Train loss 5.62e+01 Valid loss 5.53e+01 Error: 1.29e+04\n",
      "Epoch 062 Train loss 5.53e+01 Valid loss 5.37e+01 Error: 1.20e+04\n",
      "Epoch 063 Train loss 5.37e+01 Valid loss 5.14e+01 Error: 1.09e+04\n",
      "Epoch 064 Train loss 5.14e+01 Valid loss 4.99e+01 Error: 1.07e+04\n",
      "Epoch 065 Train loss 4.99e+01 Valid loss 5.43e+01 Error: 1.16e+04\n",
      "Epoch 066 Train loss 5.43e+01 Valid loss 5.56e+01 Error: 1.23e+04\n",
      "Epoch 067 Train loss 5.56e+01 Valid loss 5.62e+01 Error: 1.28e+04\n",
      "Epoch 068 Train loss 5.62e+01 Valid loss 5.66e+01 Error: 1.32e+04\n",
      "Epoch 069 Train loss 5.66e+01 Valid loss 5.69e+01 Error: 1.35e+04\n",
      "Epoch 070 Train loss 5.69e+01 Valid loss 5.70e+01 Error: 1.36e+04\n",
      "Epoch 071 Train loss 5.70e+01 Valid loss 5.70e+01 Error: 1.34e+04\n",
      "Epoch 072 Train loss 5.70e+01 Valid loss 5.69e+01 Error: 1.32e+04\n",
      "Epoch 073 Train loss 5.69e+01 Valid loss 5.68e+01 Error: 1.29e+04\n",
      "Epoch 074 Train loss 5.68e+01 Valid loss 5.66e+01 Error: 1.25e+04\n",
      "Epoch 075 Train loss 5.66e+01 Valid loss 5.64e+01 Error: 1.21e+04\n",
      "Epoch 076 Train loss 5.64e+01 Valid loss 5.61e+01 Error: 1.16e+04\n",
      "Epoch 077 Train loss 5.61e+01 Valid loss 5.59e+01 Error: 1.11e+04\n",
      "Epoch 078 Train loss 5.59e+01 Valid loss 5.55e+01 Error: 1.04e+04\n",
      "Epoch 079 Train loss 5.55e+01 Valid loss 5.51e+01 Error: 9.74e+03\n",
      "Epoch 080 Train loss 5.51e+01 Valid loss 5.46e+01 Error: 8.99e+03\n",
      "Epoch 081 Train loss 5.46e+01 Valid loss 5.39e+01 Error: 8.11e+03\n",
      "Epoch 082 Train loss 5.39e+01 Valid loss 5.31e+01 Error: 7.10e+03\n",
      "Epoch 083 Train loss 5.31e+01 Valid loss 5.19e+01 Error: 5.98e+03\n",
      "Epoch 084 Train loss 5.19e+01 Valid loss 5.02e+01 Error: 4.72e+03\n",
      "Epoch 085 Train loss 5.02e+01 Valid loss 4.57e+01 Error: 3.34e+03 (B)\n",
      "Epoch 086 Train loss 4.57e+01 Valid loss 4.79e+01 Error: 1.13e+03\n",
      "Epoch 087 Train loss 4.79e+01 Valid loss 4.86e+01 Error: 9.62e+02\n",
      "Epoch 088 Train loss 4.86e+01 Valid loss 5.08e+01 Error: 2.17e+03\n",
      "Epoch 089 Train loss 5.08e+01 Valid loss 5.14e+01 Error: 2.60e+03\n",
      "Epoch 090 Train loss 5.14e+01 Valid loss 5.14e+01 Error: 2.61e+03\n",
      "Epoch 091 Train loss 5.14e+01 Valid loss 5.08e+01 Error: 2.19e+03\n",
      "Epoch 092 Train loss 5.08e+01 Valid loss 5.00e+01 Error: 1.65e+03\n",
      "Epoch 093 Train loss 5.00e+01 Valid loss 4.81e+01 Error: 7.73e+02\n",
      "Epoch 094 Train loss 4.81e+01 Valid loss 4.80e+01 Error: 1.04e+03\n",
      "Epoch 095 Train loss 4.80e+01 Valid loss 4.75e+01 Error: 1.81e+03\n",
      "Epoch 096 Train loss 4.75e+01 Valid loss 4.31e+01 Error: 2.27e+03 (B)\n",
      "Epoch 097 Train loss 4.31e+01 Valid loss 4.18e+01 Error: 1.15e+02 (B)\n",
      "Epoch 098 Train loss 4.18e+01 Valid loss 4.90e+01 Error: 2.69e+03\n",
      "Epoch 099 Train loss 4.90e+01 Valid loss 4.92e+01 Error: 4.95e+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-23 20:05:19,410] Trial 28 finished with value: 41.8164176940918 and parameters: {'learning_rate': 1.0764718604842538e-06, 'weight_decay': 3.94730212635345e-09, 'n_layers': 1, 'hidden_channels': 64, 'r_link': 295}. Best is trial 26 with value: 32.76902770996094.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 Train loss 4.92e+01 Valid loss 5.04e+01 Error: 6.97e+03\n",
      "Epoch 001 Train loss 5.36e+01 Valid loss 5.35e+01 Error: 8.14e+03 (B)\n",
      "Epoch 002 Train loss 5.35e+01 Valid loss 5.33e+01 Error: 7.87e+03 (B)\n",
      "Epoch 003 Train loss 5.33e+01 Valid loss 5.27e+01 Error: 7.37e+03 (B)\n",
      "Epoch 004 Train loss 5.27e+01 Valid loss 5.19e+01 Error: 6.64e+03 (B)\n",
      "Epoch 005 Train loss 5.19e+01 Valid loss 5.02e+01 Error: 5.69e+03 (B)\n",
      "Epoch 006 Train loss 5.02e+01 Valid loss 4.40e+01 Error: 4.61e+03 (B)\n",
      "Epoch 007 Train loss 4.40e+01 Valid loss 4.80e+01 Error: 3.92e+03\n",
      "Epoch 008 Train loss 4.80e+01 Valid loss 4.90e+01 Error: 3.26e+03\n",
      "Epoch 009 Train loss 4.90e+01 Valid loss 4.92e+01 Error: 2.59e+03\n",
      "Epoch 010 Train loss 4.92e+01 Valid loss 4.90e+01 Error: 1.89e+03\n",
      "Epoch 011 Train loss 4.90e+01 Valid loss 4.83e+01 Error: 1.15e+03\n",
      "Epoch 012 Train loss 4.83e+01 Valid loss 4.60e+01 Error: 3.36e+02\n",
      "Epoch 013 Train loss 4.60e+01 Valid loss 4.74e+01 Error: 6.97e+02\n",
      "Epoch 014 Train loss 4.74e+01 Valid loss 4.89e+01 Error: 1.53e+03\n",
      "Epoch 015 Train loss 4.89e+01 Valid loss 4.95e+01 Error: 2.23e+03\n",
      "Epoch 016 Train loss 4.95e+01 Valid loss 4.97e+01 Error: 2.73e+03\n",
      "Epoch 017 Train loss 4.97e+01 Valid loss 4.97e+01 Error: 3.13e+03\n",
      "Epoch 018 Train loss 4.97e+01 Valid loss 4.97e+01 Error: 3.52e+03\n",
      "Epoch 019 Train loss 4.97e+01 Valid loss 4.94e+01 Error: 3.91e+03\n",
      "Epoch 020 Train loss 4.94e+01 Valid loss 4.87e+01 Error: 4.32e+03\n",
      "Epoch 021 Train loss 4.87e+01 Valid loss 4.64e+01 Error: 4.80e+03\n",
      "Epoch 022 Train loss 4.64e+01 Valid loss 4.87e+01 Error: 5.40e+03\n",
      "Epoch 023 Train loss 4.87e+01 Valid loss 5.02e+01 Error: 5.84e+03\n",
      "Epoch 024 Train loss 5.02e+01 Valid loss 5.10e+01 Error: 6.20e+03\n",
      "Epoch 025 Train loss 5.10e+01 Valid loss 5.16e+01 Error: 6.47e+03\n",
      "Epoch 026 Train loss 5.16e+01 Valid loss 5.19e+01 Error: 6.65e+03\n",
      "Epoch 027 Train loss 5.19e+01 Valid loss 5.21e+01 Error: 6.76e+03\n",
      "Epoch 028 Train loss 5.21e+01 Valid loss 5.22e+01 Error: 6.81e+03\n",
      "Epoch 029 Train loss 5.22e+01 Valid loss 5.22e+01 Error: 6.78e+03\n",
      "Epoch 030 Train loss 5.22e+01 Valid loss 5.21e+01 Error: 6.69e+03\n",
      "Epoch 031 Train loss 5.21e+01 Valid loss 5.20e+01 Error: 6.53e+03\n",
      "Epoch 032 Train loss 5.20e+01 Valid loss 5.17e+01 Error: 6.30e+03\n",
      "Epoch 033 Train loss 5.17e+01 Valid loss 5.14e+01 Error: 6.00e+03\n",
      "Epoch 034 Train loss 5.14e+01 Valid loss 5.08e+01 Error: 5.63e+03\n",
      "Epoch 035 Train loss 5.08e+01 Valid loss 5.01e+01 Error: 5.17e+03\n",
      "Epoch 036 Train loss 5.01e+01 Valid loss 4.90e+01 Error: 4.63e+03\n",
      "Epoch 037 Train loss 4.90e+01 Valid loss 4.64e+01 Error: 3.94e+03\n",
      "Epoch 038 Train loss 4.64e+01 Valid loss 4.71e+01 Error: 2.95e+03\n",
      "Epoch 039 Train loss 4.71e+01 Valid loss 4.76e+01 Error: 2.06e+03\n",
      "Epoch 040 Train loss 4.76e+01 Valid loss 4.69e+01 Error: 1.18e+03\n",
      "Epoch 041 Train loss 4.69e+01 Valid loss 4.35e+01 Error: 2.15e+02 (B)\n",
      "Epoch 042 Train loss 4.35e+01 Valid loss 4.64e+01 Error: 1.37e+03\n",
      "Epoch 043 Train loss 4.64e+01 Valid loss 3.89e+01 Error: 2.78e+03 (B)\n",
      "Epoch 044 Train loss 3.89e+01 Valid loss 4.54e+01 Error: 3.25e+02\n",
      "Epoch 045 Train loss 4.54e+01 Valid loss 5.02e+01 Error: 2.18e+03\n",
      "Epoch 046 Train loss 5.02e+01 Valid loss 5.20e+01 Error: 4.43e+03\n",
      "Epoch 047 Train loss 5.20e+01 Valid loss 5.29e+01 Error: 6.17e+03\n",
      "Epoch 048 Train loss 5.29e+01 Valid loss 5.34e+01 Error: 7.56e+03\n",
      "Epoch 049 Train loss 5.34e+01 Valid loss 5.37e+01 Error: 8.79e+03\n",
      "Epoch 050 Train loss 5.37e+01 Valid loss 5.38e+01 Error: 9.97e+03\n",
      "Epoch 051 Train loss 5.38e+01 Valid loss 5.36e+01 Error: 1.11e+04\n",
      "Epoch 052 Train loss 5.36e+01 Valid loss 5.31e+01 Error: 1.23e+04\n",
      "Epoch 053 Train loss 5.31e+01 Valid loss 5.03e+01 Error: 1.34e+04\n",
      "Epoch 054 Train loss 5.03e+01 Valid loss 5.43e+01 Error: 1.49e+04\n",
      "Epoch 055 Train loss 5.43e+01 Valid loss 5.59e+01 Error: 1.60e+04\n",
      "Epoch 056 Train loss 5.59e+01 Valid loss 5.67e+01 Error: 1.70e+04\n",
      "Epoch 057 Train loss 5.67e+01 Valid loss 5.73e+01 Error: 1.78e+04\n",
      "Epoch 058 Train loss 5.73e+01 Valid loss 5.76e+01 Error: 1.83e+04\n",
      "Epoch 059 Train loss 5.76e+01 Valid loss 5.79e+01 Error: 1.87e+04\n",
      "Epoch 060 Train loss 5.79e+01 Valid loss 5.81e+01 Error: 1.91e+04\n",
      "Epoch 061 Train loss 5.81e+01 Valid loss 5.82e+01 Error: 1.93e+04\n",
      "Epoch 062 Train loss 5.82e+01 Valid loss 5.83e+01 Error: 1.95e+04\n",
      "Epoch 063 Train loss 5.83e+01 Valid loss 5.84e+01 Error: 1.96e+04\n",
      "Epoch 064 Train loss 5.84e+01 Valid loss 5.84e+01 Error: 1.97e+04\n",
      "Epoch 065 Train loss 5.84e+01 Valid loss 5.84e+01 Error: 1.97e+04\n",
      "Epoch 066 Train loss 5.84e+01 Valid loss 5.84e+01 Error: 1.95e+04\n",
      "Epoch 067 Train loss 5.84e+01 Valid loss 5.83e+01 Error: 1.94e+04\n",
      "Epoch 068 Train loss 5.83e+01 Valid loss 5.82e+01 Error: 1.93e+04\n",
      "Epoch 069 Train loss 5.82e+01 Valid loss 5.81e+01 Error: 1.91e+04\n",
      "Epoch 070 Train loss 5.81e+01 Valid loss 5.80e+01 Error: 1.88e+04\n",
      "Epoch 071 Train loss 5.80e+01 Valid loss 5.79e+01 Error: 1.85e+04\n",
      "Epoch 072 Train loss 5.79e+01 Valid loss 5.77e+01 Error: 1.82e+04\n",
      "Epoch 073 Train loss 5.77e+01 Valid loss 5.75e+01 Error: 1.78e+04\n",
      "Epoch 074 Train loss 5.75e+01 Valid loss 5.72e+01 Error: 1.73e+04\n",
      "Epoch 075 Train loss 5.72e+01 Valid loss 5.69e+01 Error: 1.68e+04\n",
      "Epoch 076 Train loss 5.69e+01 Valid loss 5.65e+01 Error: 1.62e+04\n",
      "Epoch 077 Train loss 5.65e+01 Valid loss 5.60e+01 Error: 1.56e+04\n",
      "Epoch 078 Train loss 5.60e+01 Valid loss 5.53e+01 Error: 1.49e+04\n",
      "Epoch 079 Train loss 5.53e+01 Valid loss 5.42e+01 Error: 1.41e+04\n",
      "Epoch 080 Train loss 5.42e+01 Valid loss 5.16e+01 Error: 1.31e+04\n",
      "Epoch 081 Train loss 5.16e+01 Valid loss 5.31e+01 Error: 1.19e+04\n",
      "Epoch 082 Train loss 5.31e+01 Valid loss 5.43e+01 Error: 1.09e+04\n",
      "Epoch 083 Train loss 5.43e+01 Valid loss 5.48e+01 Error: 1.01e+04\n",
      "Epoch 084 Train loss 5.48e+01 Valid loss 5.50e+01 Error: 9.39e+03\n",
      "Epoch 085 Train loss 5.50e+01 Valid loss 5.50e+01 Error: 8.80e+03\n",
      "Epoch 086 Train loss 5.50e+01 Valid loss 5.51e+01 Error: 8.22e+03\n",
      "Epoch 087 Train loss 5.51e+01 Valid loss 5.50e+01 Error: 7.68e+03\n",
      "Epoch 088 Train loss 5.50e+01 Valid loss 5.49e+01 Error: 7.16e+03\n",
      "Epoch 089 Train loss 5.49e+01 Valid loss 5.48e+01 Error: 6.66e+03\n",
      "Epoch 090 Train loss 5.48e+01 Valid loss 5.46e+01 Error: 6.16e+03\n",
      "Epoch 091 Train loss 5.46e+01 Valid loss 5.44e+01 Error: 5.63e+03\n",
      "Epoch 092 Train loss 5.44e+01 Valid loss 5.42e+01 Error: 5.00e+03\n",
      "Epoch 093 Train loss 5.42e+01 Valid loss 5.39e+01 Error: 4.34e+03\n",
      "Epoch 094 Train loss 5.39e+01 Valid loss 5.35e+01 Error: 3.65e+03\n",
      "Epoch 095 Train loss 5.35e+01 Valid loss 5.30e+01 Error: 2.90e+03\n",
      "Epoch 096 Train loss 5.30e+01 Valid loss 5.22e+01 Error: 2.07e+03\n",
      "Epoch 097 Train loss 5.22e+01 Valid loss 5.09e+01 Error: 1.13e+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-23 20:06:16,056] Trial 29 finished with value: 38.87424850463867 and parameters: {'learning_rate': 4.0784385310347617e-07, 'weight_decay': 1.0177069431523034e-08, 'n_layers': 2, 'hidden_channels': 64, 'r_link': 208}. Best is trial 26 with value: 32.76902770996094.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 098 Train loss 5.09e+01 Valid loss 4.43e+01 Error: 4.30e+01\n",
      "Epoch 099 Train loss 4.43e+01 Valid loss 5.24e+01 Error: 3.99e+03\n",
      "Epoch 100 Train loss 5.24e+01 Valid loss 5.07e+01 Error: 6.93e+03\n",
      "Epoch 001 Train loss 5.08e+01 Valid loss 5.06e+01 Error: 1.25e+03 (B)\n",
      "Epoch 002 Train loss 5.06e+01 Valid loss 5.00e+01 Error: 9.94e+02 (B)\n",
      "Epoch 003 Train loss 5.00e+01 Valid loss 4.90e+01 Error: 6.40e+02 (B)\n",
      "Epoch 004 Train loss 4.90e+01 Valid loss 4.65e+01 Error: 2.00e+02 (B)\n",
      "Epoch 005 Train loss 4.65e+01 Valid loss 4.69e+01 Error: 2.57e+02\n",
      "Epoch 006 Train loss 4.69e+01 Valid loss 4.78e+01 Error: 4.39e+02\n",
      "Epoch 007 Train loss 4.78e+01 Valid loss 4.80e+01 Error: 5.30e+02\n",
      "Epoch 008 Train loss 4.80e+01 Valid loss 4.78e+01 Error: 5.04e+02\n",
      "Epoch 009 Train loss 4.78e+01 Valid loss 4.71e+01 Error: 3.72e+02\n",
      "Epoch 010 Train loss 4.71e+01 Valid loss 4.46e+01 Error: 1.08e+02 (B)\n",
      "Epoch 011 Train loss 4.46e+01 Valid loss 4.73e+01 Error: 4.25e+02\n",
      "Epoch 012 Train loss 4.73e+01 Valid loss 4.86e+01 Error: 8.30e+02\n",
      "Epoch 013 Train loss 4.86e+01 Valid loss 4.92e+01 Error: 1.16e+03\n",
      "Epoch 014 Train loss 4.92e+01 Valid loss 4.94e+01 Error: 1.44e+03\n",
      "Epoch 015 Train loss 4.94e+01 Valid loss 4.95e+01 Error: 1.66e+03\n",
      "Epoch 016 Train loss 4.95e+01 Valid loss 4.94e+01 Error: 1.84e+03\n",
      "Epoch 017 Train loss 4.94e+01 Valid loss 4.92e+01 Error: 1.98e+03\n",
      "Epoch 018 Train loss 4.92e+01 Valid loss 4.89e+01 Error: 2.09e+03\n",
      "Epoch 019 Train loss 4.89e+01 Valid loss 4.84e+01 Error: 2.16e+03\n",
      "Epoch 020 Train loss 4.84e+01 Valid loss 4.76e+01 Error: 2.21e+03\n",
      "Epoch 021 Train loss 4.76e+01 Valid loss 4.63e+01 Error: 2.27e+03\n",
      "Epoch 022 Train loss 4.63e+01 Valid loss 4.36e+01 Error: 2.54e+03 (B)\n",
      "Epoch 023 Train loss 4.36e+01 Valid loss 3.86e+01 Error: 2.23e+03 (B)\n",
      "Epoch 024 Train loss 3.86e+01 Valid loss 4.73e+01 Error: 3.02e+03\n",
      "Epoch 025 Train loss 4.73e+01 Valid loss 4.91e+01 Error: 3.76e+03\n",
      "Epoch 026 Train loss 4.91e+01 Valid loss 5.01e+01 Error: 4.31e+03\n",
      "Epoch 027 Train loss 5.01e+01 Valid loss 5.08e+01 Error: 4.73e+03\n",
      "Epoch 028 Train loss 5.08e+01 Valid loss 5.12e+01 Error: 5.10e+03\n",
      "Epoch 029 Train loss 5.12e+01 Valid loss 5.16e+01 Error: 5.42e+03\n",
      "Epoch 030 Train loss 5.16e+01 Valid loss 5.19e+01 Error: 5.69e+03\n",
      "Epoch 031 Train loss 5.19e+01 Valid loss 5.20e+01 Error: 5.88e+03\n",
      "Epoch 032 Train loss 5.20e+01 Valid loss 5.21e+01 Error: 6.03e+03\n",
      "Epoch 033 Train loss 5.21e+01 Valid loss 5.21e+01 Error: 6.12e+03\n",
      "Epoch 034 Train loss 5.21e+01 Valid loss 5.21e+01 Error: 6.13e+03\n",
      "Epoch 035 Train loss 5.21e+01 Valid loss 5.20e+01 Error: 6.12e+03\n",
      "Epoch 036 Train loss 5.20e+01 Valid loss 5.18e+01 Error: 6.08e+03\n",
      "Epoch 037 Train loss 5.18e+01 Valid loss 5.15e+01 Error: 5.98e+03\n",
      "Epoch 038 Train loss 5.15e+01 Valid loss 5.11e+01 Error: 5.81e+03\n",
      "Epoch 039 Train loss 5.11e+01 Valid loss 5.03e+01 Error: 5.56e+03\n",
      "Epoch 040 Train loss 5.03e+01 Valid loss 4.89e+01 Error: 5.25e+03\n",
      "Epoch 041 Train loss 4.89e+01 Valid loss 4.49e+01 Error: 4.82e+03\n",
      "Epoch 042 Train loss 4.49e+01 Valid loss 4.90e+01 Error: 5.28e+03\n",
      "Epoch 043 Train loss 4.90e+01 Valid loss 5.03e+01 Error: 5.55e+03\n",
      "Epoch 044 Train loss 5.03e+01 Valid loss 5.08e+01 Error: 5.72e+03\n",
      "Epoch 045 Train loss 5.08e+01 Valid loss 5.11e+01 Error: 5.81e+03\n",
      "Epoch 046 Train loss 5.11e+01 Valid loss 5.12e+01 Error: 5.79e+03\n",
      "Epoch 047 Train loss 5.12e+01 Valid loss 5.12e+01 Error: 5.69e+03\n",
      "Epoch 048 Train loss 5.12e+01 Valid loss 5.10e+01 Error: 5.52e+03\n",
      "Epoch 049 Train loss 5.10e+01 Valid loss 5.07e+01 Error: 5.27e+03\n",
      "Epoch 050 Train loss 5.07e+01 Valid loss 5.02e+01 Error: 4.95e+03\n",
      "Epoch 051 Train loss 5.02e+01 Valid loss 4.94e+01 Error: 4.55e+03\n",
      "Epoch 052 Train loss 4.94e+01 Valid loss 4.81e+01 Error: 4.06e+03\n",
      "Epoch 053 Train loss 4.81e+01 Valid loss 4.37e+01 Error: 3.44e+03\n",
      "Epoch 054 Train loss 4.37e+01 Valid loss 4.84e+01 Error: 2.23e+03\n",
      "Epoch 055 Train loss 4.84e+01 Valid loss 4.86e+01 Error: 1.30e+03\n",
      "Epoch 056 Train loss 4.86e+01 Valid loss 4.68e+01 Error: 3.64e+02\n",
      "Epoch 057 Train loss 4.68e+01 Valid loss 4.85e+01 Error: 6.88e+02\n",
      "Epoch 058 Train loss 4.85e+01 Valid loss 5.00e+01 Error: 1.32e+03\n",
      "Epoch 059 Train loss 5.00e+01 Valid loss 5.08e+01 Error: 1.93e+03\n",
      "Epoch 060 Train loss 5.08e+01 Valid loss 5.13e+01 Error: 2.42e+03\n",
      "Epoch 061 Train loss 5.13e+01 Valid loss 5.16e+01 Error: 2.82e+03\n",
      "Epoch 062 Train loss 5.16e+01 Valid loss 5.18e+01 Error: 3.13e+03\n",
      "Epoch 063 Train loss 5.18e+01 Valid loss 5.18e+01 Error: 3.29e+03\n",
      "Epoch 064 Train loss 5.18e+01 Valid loss 5.16e+01 Error: 3.12e+03\n",
      "Epoch 065 Train loss 5.16e+01 Valid loss 5.13e+01 Error: 2.84e+03\n",
      "Epoch 066 Train loss 5.13e+01 Valid loss 5.08e+01 Error: 2.45e+03\n",
      "Epoch 067 Train loss 5.08e+01 Valid loss 5.01e+01 Error: 1.89e+03\n",
      "Epoch 068 Train loss 5.01e+01 Valid loss 4.87e+01 Error: 1.14e+03\n",
      "Epoch 069 Train loss 4.87e+01 Valid loss 4.30e+01 Error: 8.66e+01\n",
      "Epoch 070 Train loss 4.30e+01 Valid loss 4.51e+01 Error: 2.88e+03\n",
      "Epoch 071 Train loss 4.51e+01 Valid loss 5.23e+01 Error: 6.11e+03\n",
      "Epoch 072 Train loss 5.23e+01 Valid loss 5.46e+01 Error: 9.07e+03\n",
      "Epoch 073 Train loss 5.46e+01 Valid loss 5.62e+01 Error: 1.21e+04\n",
      "Epoch 074 Train loss 5.62e+01 Valid loss 5.73e+01 Error: 1.48e+04\n",
      "Epoch 075 Train loss 5.73e+01 Valid loss 5.80e+01 Error: 1.68e+04\n",
      "Epoch 076 Train loss 5.80e+01 Valid loss 5.83e+01 Error: 1.79e+04\n",
      "Epoch 077 Train loss 5.83e+01 Valid loss 5.85e+01 Error: 1.89e+04\n",
      "Epoch 078 Train loss 5.85e+01 Valid loss 5.86e+01 Error: 1.94e+04\n",
      "Epoch 079 Train loss 5.86e+01 Valid loss 5.87e+01 Error: 1.98e+04\n",
      "Epoch 080 Train loss 5.87e+01 Valid loss 5.87e+01 Error: 2.00e+04\n",
      "Epoch 081 Train loss 5.87e+01 Valid loss 5.87e+01 Error: 2.00e+04\n",
      "Epoch 082 Train loss 5.87e+01 Valid loss 5.86e+01 Error: 1.99e+04\n",
      "Epoch 083 Train loss 5.86e+01 Valid loss 5.84e+01 Error: 1.97e+04\n",
      "Epoch 084 Train loss 5.84e+01 Valid loss 5.82e+01 Error: 1.93e+04\n",
      "Epoch 085 Train loss 5.82e+01 Valid loss 5.79e+01 Error: 1.88e+04\n",
      "Epoch 086 Train loss 5.79e+01 Valid loss 5.75e+01 Error: 1.82e+04\n",
      "Epoch 087 Train loss 5.75e+01 Valid loss 5.69e+01 Error: 1.76e+04\n",
      "Epoch 088 Train loss 5.69e+01 Valid loss 5.64e+01 Error: 1.73e+04\n",
      "Epoch 089 Train loss 5.64e+01 Valid loss 5.57e+01 Error: 1.70e+04\n",
      "Epoch 090 Train loss 5.57e+01 Valid loss 5.40e+01 Error: 1.65e+04\n",
      "Epoch 091 Train loss 5.40e+01 Valid loss 5.29e+01 Error: 1.59e+04\n",
      "Epoch 092 Train loss 5.29e+01 Valid loss 5.33e+01 Error: 1.55e+04\n",
      "Epoch 093 Train loss 5.33e+01 Valid loss 5.02e+01 Error: 1.54e+04\n",
      "Epoch 094 Train loss 5.02e+01 Valid loss 5.52e+01 Error: 1.59e+04\n",
      "Epoch 095 Train loss 5.52e+01 Valid loss 5.62e+01 Error: 1.61e+04\n",
      "Epoch 096 Train loss 5.62e+01 Valid loss 5.68e+01 Error: 1.63e+04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-23 20:07:11,170] Trial 30 finished with value: 38.614173889160156 and parameters: {'learning_rate': 2.9953062957508875e-06, 'weight_decay': 3.817732782471611e-09, 'n_layers': 1, 'hidden_channels': 64, 'r_link': 163}. Best is trial 26 with value: 32.76902770996094.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 097 Train loss 5.68e+01 Valid loss 5.72e+01 Error: 1.64e+04\n",
      "Epoch 098 Train loss 5.72e+01 Valid loss 5.74e+01 Error: 1.65e+04\n",
      "Epoch 099 Train loss 5.74e+01 Valid loss 5.75e+01 Error: 1.66e+04\n",
      "Epoch 100 Train loss 5.75e+01 Valid loss 5.76e+01 Error: 1.66e+04\n",
      "Epoch 001 Train loss 6.02e+01 Valid loss 5.29e+01 Error: 3.22e+03 (B)\n",
      "Epoch 002 Train loss 5.29e+01 Valid loss 5.61e+01 Error: 7.56e+03\n",
      "Epoch 003 Train loss 5.61e+01 Valid loss 5.75e+01 Error: 1.38e+04\n",
      "Epoch 004 Train loss 5.75e+01 Valid loss 5.97e+01 Error: 2.31e+04\n",
      "Epoch 005 Train loss 5.97e+01 Valid loss 5.88e+01 Error: 1.98e+04\n",
      "Epoch 006 Train loss 5.88e+01 Valid loss 5.45e+01 Error: 7.47e+03\n",
      "Epoch 007 Train loss 5.45e+01 Valid loss 3.82e+01 Error: 6.26e+02 (B)\n",
      "Epoch 008 Train loss 3.82e+01 Valid loss 6.02e+01 Error: 2.31e+04\n",
      "Epoch 009 Train loss 6.02e+01 Valid loss 6.39e+01 Error: 4.29e+04\n",
      "Epoch 010 Train loss 6.39e+01 Valid loss 6.57e+01 Error: 5.87e+04\n",
      "Epoch 011 Train loss 6.57e+01 Valid loss 6.68e+01 Error: 7.08e+04\n",
      "Epoch 012 Train loss 6.68e+01 Valid loss 6.75e+01 Error: 8.06e+04\n",
      "Epoch 013 Train loss 6.75e+01 Valid loss 6.80e+01 Error: 8.78e+04\n",
      "Epoch 014 Train loss 6.80e+01 Valid loss 6.83e+01 Error: 9.24e+04\n",
      "Epoch 015 Train loss 6.83e+01 Valid loss 6.84e+01 Error: 9.51e+04\n",
      "Epoch 016 Train loss 6.84e+01 Valid loss 6.83e+01 Error: 9.61e+04\n",
      "Epoch 017 Train loss 6.83e+01 Valid loss 6.81e+01 Error: 9.50e+04\n",
      "Epoch 018 Train loss 6.81e+01 Valid loss 6.77e+01 Error: 9.23e+04\n",
      "Epoch 019 Train loss 6.77e+01 Valid loss 6.69e+01 Error: 8.82e+04\n",
      "Epoch 020 Train loss 6.69e+01 Valid loss 6.53e+01 Error: 8.25e+04\n",
      "Epoch 021 Train loss 6.53e+01 Valid loss 6.37e+01 Error: 7.44e+04\n",
      "Epoch 022 Train loss 6.37e+01 Valid loss 6.24e+01 Error: 7.43e+04\n",
      "Epoch 023 Train loss 6.24e+01 Valid loss 6.64e+01 Error: 8.46e+04\n",
      "Epoch 024 Train loss 6.64e+01 Valid loss 6.77e+01 Error: 9.10e+04\n",
      "Epoch 025 Train loss 6.77e+01 Valid loss 6.82e+01 Error: 9.46e+04\n",
      "Epoch 026 Train loss 6.82e+01 Valid loss 6.85e+01 Error: 9.58e+04\n",
      "Epoch 027 Train loss 6.85e+01 Valid loss 6.86e+01 Error: 9.55e+04\n",
      "Epoch 028 Train loss 6.86e+01 Valid loss 6.85e+01 Error: 9.29e+04\n",
      "Epoch 029 Train loss 6.85e+01 Valid loss 6.82e+01 Error: 8.77e+04\n",
      "Epoch 030 Train loss 6.82e+01 Valid loss 6.77e+01 Error: 8.04e+04\n",
      "Epoch 031 Train loss 6.77e+01 Valid loss 6.70e+01 Error: 7.09e+04\n",
      "Epoch 032 Train loss 6.70e+01 Valid loss 6.59e+01 Error: 5.89e+04\n",
      "Epoch 033 Train loss 6.59e+01 Valid loss 6.42e+01 Error: 4.45e+04\n",
      "Epoch 034 Train loss 6.42e+01 Valid loss 6.12e+01 Error: 2.69e+04\n",
      "Epoch 035 Train loss 6.12e+01 Valid loss 4.94e+01 Error: 4.02e+03\n",
      "Epoch 036 Train loss 4.94e+01 Valid loss 6.31e+01 Error: 3.83e+04\n",
      "Epoch 037 Train loss 6.31e+01 Valid loss 6.66e+01 Error: 6.84e+04\n",
      "Epoch 038 Train loss 6.66e+01 Valid loss 6.82e+01 Error: 9.08e+04\n",
      "Epoch 039 Train loss 6.82e+01 Valid loss 6.91e+01 Error: 1.07e+05\n",
      "Epoch 040 Train loss 6.91e+01 Valid loss 6.94e+01 Error: 1.15e+05\n",
      "Epoch 041 Train loss 6.94e+01 Valid loss 6.92e+01 Error: 1.16e+05\n",
      "Epoch 042 Train loss 6.92e+01 Valid loss 6.86e+01 Error: 1.13e+05\n",
      "Epoch 043 Train loss 6.86e+01 Valid loss 6.67e+01 Error: 1.05e+05\n",
      "Epoch 044 Train loss 6.67e+01 Valid loss 6.67e+01 Error: 8.99e+04\n",
      "Epoch 045 Train loss 6.67e+01 Valid loss 6.75e+01 Error: 8.11e+04\n",
      "Epoch 046 Train loss 6.75e+01 Valid loss 6.73e+01 Error: 7.53e+04\n",
      "Epoch 047 Train loss 6.73e+01 Valid loss 6.62e+01 Error: 7.27e+04\n",
      "Epoch 048 Train loss 6.62e+01 Valid loss 6.32e+01 Error: 7.61e+04\n",
      "Epoch 049 Train loss 6.32e+01 Valid loss 6.60e+01 Error: 4.98e+04\n",
      "Epoch 050 Train loss 6.60e+01 Valid loss 6.56e+01 Error: 2.57e+04\n",
      "Epoch 051 Train loss 6.56e+01 Valid loss 5.87e+01 Error: 8.01e+02\n",
      "Epoch 052 Train loss 5.87e+01 Valid loss 6.63e+01 Error: 6.00e+04\n",
      "Epoch 053 Train loss 6.63e+01 Valid loss 6.80e+01 Error: 1.12e+05\n",
      "Epoch 054 Train loss 6.80e+01 Valid loss 7.08e+01 Error: 1.52e+05\n",
      "Epoch 055 Train loss 7.08e+01 Valid loss 7.20e+01 Error: 1.81e+05\n",
      "Epoch 056 Train loss 7.20e+01 Valid loss 7.28e+01 Error: 2.02e+05\n",
      "Epoch 057 Train loss 7.28e+01 Valid loss 7.32e+01 Error: 2.14e+05\n",
      "Epoch 058 Train loss 7.32e+01 Valid loss 7.33e+01 Error: 2.20e+05\n",
      "Epoch 059 Train loss 7.33e+01 Valid loss 7.32e+01 Error: 2.18e+05\n",
      "Epoch 060 Train loss 7.32e+01 Valid loss 7.28e+01 Error: 2.08e+05\n",
      "Epoch 061 Train loss 7.28e+01 Valid loss 7.20e+01 Error: 1.93e+05\n",
      "Epoch 062 Train loss 7.20e+01 Valid loss 7.06e+01 Error: 1.69e+05\n",
      "Epoch 063 Train loss 7.06e+01 Valid loss 6.43e+01 Error: 1.37e+05\n",
      "Epoch 064 Train loss 6.43e+01 Valid loss 7.05e+01 Error: 8.29e+04\n",
      "Epoch 065 Train loss 7.05e+01 Valid loss 6.97e+01 Error: 3.18e+04\n",
      "Epoch 066 Train loss 6.97e+01 Valid loss 6.95e+01 Error: 2.44e+04\n",
      "Epoch 067 Train loss 6.95e+01 Valid loss 7.15e+01 Error: 5.95e+04\n",
      "Epoch 068 Train loss 7.15e+01 Valid loss 7.23e+01 Error: 8.40e+04\n",
      "Epoch 069 Train loss 7.23e+01 Valid loss 7.27e+01 Error: 1.01e+05\n",
      "Epoch 070 Train loss 7.27e+01 Valid loss 7.29e+01 Error: 1.14e+05\n",
      "Epoch 071 Train loss 7.29e+01 Valid loss 7.29e+01 Error: 1.23e+05\n",
      "Epoch 072 Train loss 7.29e+01 Valid loss 7.28e+01 Error: 1.29e+05\n",
      "Epoch 073 Train loss 7.28e+01 Valid loss 7.24e+01 Error: 1.33e+05\n",
      "Epoch 074 Train loss 7.24e+01 Valid loss 7.18e+01 Error: 1.37e+05\n",
      "Epoch 075 Train loss 7.18e+01 Valid loss 7.02e+01 Error: 1.45e+05\n",
      "Epoch 076 Train loss 7.02e+01 Valid loss 6.99e+01 Error: 1.65e+05\n",
      "Epoch 077 Train loss 6.99e+01 Valid loss 6.95e+01 Error: 1.51e+05\n",
      "Epoch 078 Train loss 6.95e+01 Valid loss 6.60e+01 Error: 1.17e+05\n",
      "Epoch 079 Train loss 6.60e+01 Valid loss 7.02e+01 Error: 1.35e+05\n",
      "Epoch 080 Train loss 7.02e+01 Valid loss 7.10e+01 Error: 1.40e+05\n",
      "Epoch 081 Train loss 7.10e+01 Valid loss 7.08e+01 Error: 1.33e+05\n",
      "Epoch 082 Train loss 7.08e+01 Valid loss 6.97e+01 Error: 1.14e+05\n",
      "Epoch 083 Train loss 6.97e+01 Valid loss 6.63e+01 Error: 8.47e+04\n",
      "Epoch 084 Train loss 6.63e+01 Valid loss 6.69e+01 Error: 3.43e+04\n",
      "Epoch 085 Train loss 6.69e+01 Valid loss 6.65e+01 Error: 1.82e+04\n",
      "Epoch 086 Train loss 6.65e+01 Valid loss 6.77e+01 Error: 3.03e+04\n",
      "Epoch 087 Train loss 6.77e+01 Valid loss 6.73e+01 Error: 2.69e+04\n",
      "Epoch 088 Train loss 6.73e+01 Valid loss 6.48e+01 Error: 9.10e+03\n",
      "Epoch 089 Train loss 6.48e+01 Valid loss 6.69e+01 Error: 3.83e+04\n",
      "Epoch 090 Train loss 6.69e+01 Valid loss 6.20e+01 Error: 6.76e+04\n",
      "Epoch 091 Train loss 6.20e+01 Valid loss 7.01e+01 Error: 1.18e+05\n",
      "Epoch 092 Train loss 7.01e+01 Valid loss 7.09e+01 Error: 1.47e+05\n",
      "Epoch 093 Train loss 7.09e+01 Valid loss 6.95e+01 Error: 1.53e+05\n",
      "Epoch 094 Train loss 6.95e+01 Valid loss 7.08e+01 Error: 1.35e+05\n",
      "Epoch 095 Train loss 7.08e+01 Valid loss 7.19e+01 Error: 1.29e+05\n",
      "Epoch 096 Train loss 7.19e+01 Valid loss 7.21e+01 Error: 1.24e+05\n",
      "Epoch 097 Train loss 7.21e+01 Valid loss 7.19e+01 Error: 1.19e+05\n",
      "Epoch 098 Train loss 7.19e+01 Valid loss 7.12e+01 Error: 1.15e+05\n",
      "Epoch 099 Train loss 7.12e+01 Valid loss 6.95e+01 Error: 1.12e+05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-23 20:08:44,263] Trial 31 finished with value: 38.24867248535156 and parameters: {'learning_rate': 1.1020352987549754e-05, 'weight_decay': 3.920232827246835e-08, 'n_layers': 3, 'hidden_channels': 512, 'r_link': 234}. Best is trial 26 with value: 32.76902770996094.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 Train loss 6.95e+01 Valid loss 6.87e+01 Error: 1.22e+05\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning: multiple trials\n",
    "\n",
    "name = base_name+\"_trials\"\n",
    "storage = f\"sqlite:///{os.getcwd()}/Databases/optuna_{name}\"\n",
    "n_trials = 10\n",
    "sampler = optuna.samplers.TPESampler(n_startup_trials=n_trials//3)\n",
    "study = optuna.create_study(study_name=name, sampler=sampler, storage=storage, load_if_exists=True)\n",
    "\n",
    "study.optimize(objective, n_trials, gc_after_trial=True)\n",
    "\n",
    "trials = study.trials\n",
    "losses = [el.value for el in trials]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "036b035a-9894-49fe-a726-16e7651a687c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses: [46.37158966064453, 43.337669372558594, 45.3077507019043, 52.475128173828125, 46.68367385864258, 51.591941833496094, 35.48434066772461, 39.49705505371094, 40.103271484375, 37.063941955566406, 49.92986297607422, 36.61855697631836, 40.57113265991211, 44.958953857421875, 40.88899230957031, 39.731834411621094, 35.907962799072266, 44.197235107421875, 48.171871185302734, 41.84313201904297, 46.63982009887695, 32.76902770996094, 44.07023620605469, 41.8164176940918, 38.87424850463867, 38.614173889160156, 38.24867248535156]\n",
      "Best trial: FrozenTrial(number=21, state=1, values=[35.907962799072266], datetime_start=datetime.datetime(2025, 6, 23, 19, 40, 28, 35660), datetime_complete=datetime.datetime(2025, 6, 23, 19, 41, 22, 970302), params={'learning_rate': 4.776881941892208e-06, 'weight_decay': 3.3820429538675104e-09, 'n_layers': 2, 'hidden_channels': 64, 'r_link': 247}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=0.001, log=True, low=1e-07, step=None), 'weight_decay': FloatDistribution(high=1e-06, log=True, low=1e-09, step=None), 'n_layers': IntDistribution(high=5, log=False, low=1, step=1), 'hidden_channels': CategoricalDistribution(choices=(64, 128, 256, 512)), 'r_link': IntDistribution(high=300, log=False, low=0, step=1)}, trial_id=22, value=None)\n"
     ]
    }
   ],
   "source": [
    "# Show results and save as a dictionary\n",
    "\n",
    "trials = study.trials\n",
    "losses = [el.value for el in trials]\n",
    "print(\"Losses:\",[x for x in losses if x != None])\n",
    "best_idx = np.argsort([x for x in losses if x != None])[0]\n",
    "best_trial = trials[best_idx]\n",
    "print(\"Best trial:\",best_trial)\n",
    "\n",
    "hparams.learning_rate = best_trial.params[\"learning_rate\"]\n",
    "hparams.n_layers = best_trial.params[\"n_layers\"]\n",
    "hparams.hidden_channels = best_trial.params[\"hidden_channels\"]\n",
    "hparams.r_link = best_trial.params[\"r_link\"]\n",
    "hparams.weight_decay = best_trial.params['weight_decay']\n",
    "\n",
    "outputs = np.load(\"Outputs/outputs_\"+hparams.name_model()+\".npy\")\n",
    "trues = np.load(\"Outputs/trues_\"+hparams.name_model()+\".npy\")\n",
    "errors = np.load(\"Outputs/errors_\"+hparams.name_model()+\".npy\")\n",
    "\n",
    "true, pred, err = graph.denormalize(trues, outputs, errors, type=\"WDM\") # change type to the parameter you're using\n",
    "true = true[:,0]\n",
    "pred = pred[:,0]\n",
    "err = np.abs(err[:,0])\n",
    "graph_dict = dict(true = true, pred = pred, err = err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a09757c-bff9-4d6e-904b-5f12cf4e8deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_234687/1423474408.py:8: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"k--\" (-> color='k'). The keyword argument will take precedence.\n",
      "  ax.plot([0,30],[0,30],'k--', color='orange')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAIVCAYAAAApuQ6uAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABJGElEQVR4nO3de1yUZf7/8feIgIgyaihgIWGax/IAyaE07YC6a6a2qbWSumRZWRm5bXZatTaqXc1OapmGtm5akmm/rJXKY4J5ADMrs9LwAJGmg4cEhfv3B19nHQFhhoFhbl7Px+N+yFxzzTWf++5+7Lz3uk8WwzAMAQAAeLkGni4AAADAHQg1AADAFAg1AADAFAg1AADAFAg1AADAFAg1AADAFAg1AADAFAg1AADAFAg1AADAFAg1AADAFLw21Kxbt0433XSTWrduLYvFog8++KDSz6xdu1ZRUVFq1KiR2rZtqzlz5pTpk5aWps6dO8vf31+dO3fWsmXLaqB6AADgbl4bak6cOKFu3brp1VdfrVL/PXv26A9/+IN69+6trKwsPfbYY3rggQeUlpZm75ORkaERI0YoMTFR27dvV2JiooYPH65NmzbV1GoAAAA3sZjhgZYWi0XLli3TkCFDKuzzt7/9TStWrNC3335rbxs/fry2b9+ujIwMSdKIESNUUFCgjz/+2N5nwIABat68ud55550aqx8AAFRfQ08XUFsyMjKUkJDg0Na/f3/NmzdPp0+flq+vrzIyMvTQQw+V6TNz5swKxy0sLFRhYaH9dUlJiX777TdddNFFslgsbl0HAADMxjAMHTt2TK1bt1aDBtU7gFRvQk1eXp5CQkIc2kJCQnTmzBkdOnRIYWFhFfbJy8urcNyUlBRNnTq1RmoGAKC+2Ldvny655JJqjVFvQo2kMjMnZ4+8ndteXp8LzbhMnjxZycnJ9tc2m01t2rTRvn37FBQU5I6yAQAwrYKCAoWHh6tp06bVHqvehJrQ0NAyMy75+flq2LChLrroogv2OX/25lz+/v7y9/cv0x4UFORcqCkulE7kSP4tJP+Lqv45AABMwB2nbHjt1U/OiouLU3p6ukPbqlWrFB0dLV9f3wv2iY+Pr/kCC76T/t/l0keda/67AAAwIa+dqTl+/Lh++OEH++s9e/YoOztbLVq0UJs2bTR58mQdOHBACxculFR6pdOrr76q5ORkjRs3ThkZGZo3b57DVU0PPvig+vTpo+eff14333yzli9frk8//VQbNmyo+RUqOlL6r1/zmv8uAABMyGtnarZs2aIePXqoR48ekqTk5GT16NFDTz31lCQpNzdXOTk59v6RkZFauXKl1qxZo+7du+vpp5/Wyy+/rFtuucXeJz4+XosXL9Zbb72lK6+8UqmpqVqyZIliYmJqfoXOhhpfQg0AAK4wxX1q6pKCggJZrVbZbDbnzqn5cb60KUkKGyj1W1lzBQIAUIe4/LtZDq+dqTEdDj8BAFAthJq6glADAEC1EGrqiqLfSv8l1AAA4BKvvfrJdFr1Lf03OM6jZQAA4K0INXVFxPDSBQAAuITDTwAAwBQINXXFiZ+lU4cko8TTlQAA4JUINXXFJ1dJ77eUbDs9XQkAAF6JUFMXGAaXdAMAUE2EmrrgzHHJOFP6N6EGAACXEGrqgrOzNA18JZ/Gnq0FAAAvRaipC8499GSxeLYWAAC8FKGmLuB8GgAAqo1QUxecDTW+hBoAAFzFHYXrgsA2Uvt7pMAIT1cCAIDXItTUBS2iShcAAOAyDj8BAABTYKamLjj1qySL5NdMasB/EgAAXMFMTV2w9cHSRyR8/4qnKwEAwGsRauoCLukGAKDaCDV1AaEGAIBqI9TUBUW/lf5LqAEAwGWEmrqAmRoAAKqNUONphnFOqGnh2VoAAPBihBpPO3NcMopL/2amBgAAl3FTFI8zSh+RcLpA8gnwdDEAAHgtQo2n+QZJV83ydBUAAHg9Dj8BAABTINR42pkT0qlDUskZT1cCAIBXI9R42s9LSh+RsO5mT1cCAIBXI9R4GveoAQDALQg1nkaoAQDALQg1nkaoAQDALQg1nkaoAQDALQg1nsbDLAEAcAtCjacxUwMAgFtwR2FPa/1HqcllUpO2nq4EAACvRqjxtCue8nQFAACYAoefAACAKRBqPKmkmEckAADgJoQaT/r9YOkjEt4NlAzD09UAAODVCDWeZL+cu5lksXi0FAAAvB2hxpPsl3O38GwdAACYAKHGk7hHDQAAbkOo8SRCDQAAbkOo8SRCDQAAbkOo8SRCDQAAbuPVoWbWrFmKjIxUo0aNFBUVpfXr11fYd8yYMbJYLGWWLl262PukpqaW2+fUqVM1swLNrpAibpMu6lUz4wMAUI94bahZsmSJJk6cqMcff1xZWVnq3bu3Bg4cqJycnHL7v/TSS8rNzbUv+/btU4sWLXTrrbc69AsKCnLol5ubq0aNGtXMSkSMkK7+jxSZWDPjAwBQj3htqJkxY4aSkpJ05513qlOnTpo5c6bCw8M1e/bscvtbrVaFhobaly1btujIkSMaO3asQz+LxeLQLzQ0tDZWBwAAVJNXhpqioiJt3bpVCQkJDu0JCQnauHFjlcaYN2+ebrjhBkVERDi0Hz9+XBEREbrkkks0aNAgZWVlXXCcwsJCFRQUOCxVXxEbj0gAAMBNvDLUHDp0SMXFxQoJCXFoDwkJUV5eXqWfz83N1ccff6w777zTob1jx45KTU3VihUr9M4776hRo0a6+uqrtXv37grHSklJkdVqtS/h4eFVX5H/9pIW+0r5G6r+GQAAUC6vDDVnWc57tIBhGGXaypOamqpmzZppyJAhDu2xsbEaNWqUunXrpt69e+vdd9/V5ZdfrldeeaXCsSZPniybzWZf9u3bV/UVsF/9ZK36ZwAAQLkaeroAVwQHB8vHx6fMrEx+fn6Z2ZvzGYah+fPnKzExUX5+fhfs26BBA1111VUXnKnx9/eXv79/1Yv/XyFc0g0AgBt55UyNn5+foqKilJ6e7tCenp6u+Pj4C3527dq1+uGHH5SUlFTp9xiGoezsbIWFhVWr3nKdOS4Z/3c+DaEGAIBq88qZGklKTk5WYmKioqOjFRcXpzfeeEM5OTkaP368pNLDQgcOHNDChQsdPjdv3jzFxMSoa9euZcacOnWqYmNj1b59exUUFOjll19Wdna2XnvtNfevwNlZmgZ+kk9j948PAEA947WhZsSIETp8+LCmTZum3Nxcde3aVStXrrRfzZSbm1vmnjU2m01paWl66aWXyh3z6NGjuuuuu5SXlyer1aoePXpo3bp16tWrBm6Od+6hpyqcBwQAAC7MYhiG4ekizKSgoEBWq1U2m01BQUEVd/xljfRZPymoozTo21qrDwCAuqTKv5tV4LUzNV7Pr3npIxIacXM/AADcgVDjKc27lT4iAQAAuIVXXv0EAABwPkKNp5z5nUckAADgRoQaT8l6uPQRCTue9nQlAACYAqHGU85e0u3bxLN1AABgEoQaT+ERCQAAuBWhxlMINQAAuBWhxlMINQAAuBWhxlMINQAAuBWhxhMM45xQ08KztQAAYBLcUdgTSk5LbYaXBhtCDQAAbkGo8QQfPx6RAACAm3H4CQAAmAKhxhNKzvCIBAAA3IxQ4wkHVpQ+IuGz6z1dCQAApkGo8YSzVz418PdsHQAAmAihxhPOhhp/rnwCAMBdCDWewI33AABwO0KNJxBqAABwO0KNJxBqAABwO0KNJxBqAABwO+4o7AkX9ZJkkZq09XQlAACYBqHGE66c5ukKAAAwHQ4/AQAAUyDU1DbD4BEJAADUAEJNbTtzvPQRCe8GSWdOeroaAABMg1BT285e+VRySvIJ8GwtAACYCKGmtp17ObfF4tlaAAAwEUJNbbOHGp77BACAOxFqahs33gMAoEYQamoboQYAgBpBqKlthBoAAGoEoaa2NQ6XwgZIzXt6uhIAAEyFxyTUtojhpQsAAHArZmoAAIApEGpqm1Hi6QoAADAlQk1tWxUvvdtUOvhfT1cCAICpEGpqW9Fvpc9/asgjEgAAcCdCTW3jkm4AAGoEoaY2GQahBgCAGkKoqU1njktGcenfhBoAANyKUFObzs7SNPCTfBp7thYAAEyGUFObzj30ZLF4thYAAEyGOwrXpgZ+pY9I8LV6uhIAAEyHUFObrJ2kfh97ugoAAEzJqw8/zZo1S5GRkWrUqJGioqK0fv36CvuuWbNGFoulzPLdd9859EtLS1Pnzp3l7++vzp07a9myZTW9GgAAwA28NtQsWbJEEydO1OOPP66srCz17t1bAwcOVE5OzgU/t2vXLuXm5tqX9u3b29/LyMjQiBEjlJiYqO3btysxMVHDhw/Xpk2b3FO0YbhnHAAAUIbFMLzzlzYmJkY9e/bU7Nmz7W2dOnXSkCFDlJKSUqb/mjVr1K9fPx05ckTNmjUrd8wRI0aooKBAH3/8v0NEAwYMUPPmzfXOO+9Uqa6CggJZrVbZbDYFBQU5vrn9CWnXS1KHiVK3p6s0HgAAZnbB300neeVMTVFRkbZu3aqEhASH9oSEBG3cuPGCn+3Ro4fCwsJ0/fXXa/Xq1Q7vZWRklBmzf//+FxyzsLBQBQUFDkvFhf/fIxK48gkAALfzylBz6NAhFRcXKyQkxKE9JCREeXl55X4mLCxMb7zxhtLS0vT++++rQ4cOuv7667Vu3Tp7n7y8PKfGlKSUlBRZrVb7Eh4eXnHh3E0YAIAa49VXP1nOm/EwDKNM21kdOnRQhw4d7K/j4uK0b98+/etf/1KfPn1cGlOSJk+erOTkZPvrgoKCioMNoQYAgBrjlTM1wcHB8vHxKTODkp+fX2am5UJiY2O1e/du++vQ0FCnx/T391dQUJDDUiFCDQAANcYrQ42fn5+ioqKUnp7u0J6enq74+Pgqj5OVlaWwsDD767i4uDJjrlq1yqkxL4hQAwBAjfHaw0/JyclKTExUdHS04uLi9MYbbygnJ0fjx4+XVHpY6MCBA1q4cKEkaebMmbr00kvVpUsXFRUV6d///rfS0tKUlpZmH/PBBx9Unz599Pzzz+vmm2/W8uXL9emnn2rDhg3uKZpQAwBAjfHaUDNixAgdPnxY06ZNU25urrp27aqVK1cqIiJCkpSbm+twz5qioiJNmjRJBw4cUEBAgLp06aKPPvpIf/jDH+x94uPjtXjxYj3xxBN68sknddlll2nJkiWKiYlxT9HBcdKpfMm/pXvGAwAAdl57n5q6yp3X2wMAYHb1/j41AAAA5yPUAAAAUyDU1JZf1krvNpU+u87TlQAAYEqEmtpy9hEJxac8XQkAAKZEqKktXM4NAECNItTUFkINAAA1ilBTWwg1AADUKEJNbSn6rfRfQg0AADWCUFNbmKkBAKBGEWpqS5N20kUxUuClnq4EAABT8tpnP3mdbk+XLgAAoEYwUwMAAEyBUAMAAEyBUFMbDENa2kL6oI10Kt/T1QAAYEqcU1Mbzhwvvfqp6IjUsImnqwEAwJSYqakNZy/nbuAn+QR4thYAAEyKUFMbzr1HjcXi2VoAADApQk1t4MZ7AADUOEJNbSDUAABQ4wg1tYFQAwBAjSPU1Aa/ZtJFsZK1i6crAQDAtLikuzaEDytdAABAjWGmBgAAmAKhBgAAmAKhpjas/1PpIxL2L/d0JQAAmBahpjac3Fe6iBvvAQBQUwg1tYFLugEAqHGEmtpAqAEAoMY5dUn3ihUrnP6CG2+8UQEB9fghjoZBqAEAoBY4FWqGDBni1OAWi0W7d+9W27ZtnfqcqZw5LhnFpX8TagAAqDFOH37Ky8tTSUlJlZbGjRvXRM3e5ewsTQM/yacez1gBAFDDnJqpGT16tFOHkkaNGqWgoCCnizIVo1i6KEay+EgWrn4CAKCmWAzDMDxdhJkUFBTIarXKZrMR6AAAqIQ7fzedPvyUnZ1drS8EAACoCU6Hmp49eyoqKkqzZ8+WzWariZoAAACc5nSo+eKLL9SzZ089+uijCgsL06hRo7R69eqaqM0cdr1a+oiE7MmergQAAFNzOtTExcVp7ty5ysvL0+zZs7V//37dcMMNuuyyy/SPf/xD+/fvr4k6vdep3NJHJJw57ulKAAAwNZfvKBwQEKDRo0drzZo1+v7773Xbbbfp9ddfV2RkpP7whz+4s0bvxo33AACoFW55TMJll12mRx99VI8//riCgoL03//+1x3DmgOhBgCAWuHUfWrKs3btWs2fP19paWny8fHR8OHDlZSU5I7azIFQAwBArXAp1Ozbt0+pqalKTU3Vnj17FB8fr1deeUXDhw9XYGCgu2v0boQaAABqhdOh5sYbb9Tq1avVsmVL3XHHHfrLX/6iDh061ERt5kCoAQCgVjgdagICApSWlqZBgwbJx8enJmoyl6btJUsDyb+lpysBAMDUeEyCm/GYBAAAqs6jj0k41/r16zVq1CjFxcXpwIEDkqS3335bGzZsqFZRAAAAznI51KSlpal///4KCAhQVlaWCgsLJUnHjh3Ts88+67YCAQAAqsLlUPPMM89ozpw5mjt3rnx9fe3t8fHx2rZtm1uK83pHv5Y+CJc+u97TlQAAYHouh5pdu3apT58+ZdqDgoJ09OjR6tRUZbNmzVJkZKQaNWqkqKgorV+/vsK+77//vm688Ua1bNlSQUFBiouLK3OTwNTUVFksljLLqVOnXCuw8JB0cr/0+0HXPg8AAKrM5VATFhamH374oUz7hg0b1LZt22oVVRVLlizRxIkT9fjjjysrK0u9e/fWwIEDlZOTU27/devW6cYbb9TKlSu1detW9evXTzfddJOysrIc+gUFBSk3N9dhadSokWtFcjk3AAC1xuU7Ct9999168MEHNX/+fFksFh08eFAZGRmaNGmSnnrqKXfWWK4ZM2YoKSlJd955pyRp5syZ+u9//6vZs2crJSWlTP+ZM2c6vH722We1fPlyffjhh+rRo4e93WKxKDQ01D1FEmoAAKg1LoeaRx55RDabTf369dOpU6fUp08f+fv7a9KkSZowYYI7ayyjqKhIW7du1aOPPurQnpCQoI0bN1ZpjJKSEh07dkwtWrRwaD9+/LgiIiJUXFys7t276+mnn3YIPecrLCy0nyQtlV6a9r9CCTUAANSWal3S/Y9//EOHDh3Sl19+qczMTP366696+umn3VVbhQ4dOqTi4mKFhIQ4tIeEhCgvL69KY0yfPl0nTpzQ8OHD7W0dO3ZUamqqVqxYoXfeeUeNGjXS1Vdfrd27d1c4TkpKiqxWq30JDw//35uEGgAAao3Loebf//63JKlx48aKjo5Wr1691KRJE0nSX//6V/dUVwmLxeLw2jCMMm3leeeddzRlyhQtWbJErVq1srfHxsZq1KhR6tatm3r37q13331Xl19+uV555ZUKx5o8ebJsNpt92bdv3//eJNQAAFBrXA41EyZM0P/7f/+vTPtDDz1kDzw1JTg4WD4+PmVmZfLz88vM3pxvyZIlSkpK0rvvvqsbbrjhgn0bNGigq6666oIzNf7+/goKCnJY/vdmsBTUQWp8SeUrBQAAqsXlULN48WKNGjVK69ats7fdf//9evfdd7V69Wq3FFcRPz8/RUVFKT093aE9PT1d8fHxFX7unXfe0ZgxY/Sf//xHf/zjHyv9HsMwlJ2drbCwMNcKvXKqNOg7qd1drn0eAABUmcsnCg8YMEBz5szRkCFDtGrVKs2fP1/Lly/X6tWrdfnll7uzxnIlJycrMTFR0dHRiouL0xtvvKGcnByNHz9eUulhoQMHDmjhwoWSSgPNHXfcoZdeekmxsbH2WZ6AgABZrVZJ0tSpUxUbG6v27duroKBAL7/8srKzs/Xaa6/V+PoAAIDqcTnUSNLIkSN15MgRXXPNNWrZsqXWrl2rdu3auau2CxoxYoQOHz6sadOmKTc3V127dtXKlSsVEREhScrNzXW4Z83rr7+uM2fO6L777tN9991nbx89erRSU1MlSUePHtVdd92lvLw8Wa1W9ejRQ+vWrVOvXr1qZZ0AAIDrnHpKd3JycrntS5cuVY8ePXTZZZfZ22bMmFH96ryQw9NGv7hWMoqlPsulJpGeLg0AgDrHnU/pdmqm5vy775512WWXqaCgwP5+Va5AMj3DkI7uKA01Dfw8XQ0AAKbnVKip6ROATeXM8dJAI3FJNwAAtcCpq5+++uorlZSUVLn/zp07debMGaeLMoWio6X/NvCTfAI8WgoAAPWBU6GmR48eOnz4cJX7x8XFVfiASdM7G2r8mkscjgMAoMY5dfjJMAw9+eSTaty4cZX6FxUVuVSUKZw+Wvovh54AAKgVToWaPn36aNeuXVXuHxcXp4CAenro5dyZGgAAUOOcCjVr1qypoTJMyNKw9BEJTdp6uhIAAOqFat18Dxdw8UCp0whPVwEAQL3h8rOfAAAA6hJCDQAAMAVCTU3Z9oi0spuU856nKwEAoF7gnJqacvwHqeAr6cwJT1cCAEC9UO1QM23atAu+/9RTT1X3K7wTl3QDAFCr3BJqunbtqmHDhik4OFhOPPTb3E4fKf2XUAMAQK2odqjZv3+/li5dqmXLlsnPz0+33nqrhg4dqubN6/mPedFRyUeEGgAAakm1TxQODQ3VhAkT9Nlnn2n+/Pmy2Wzq1KmTFixY4I76vBeHnwAAqFVuOVHYMAytXbtWS5Ys0ZdffqnbbrtNV199tTuG9l7G/z3NnFADAECtqHaomTBhgjIzM9W7d2/dcccdmj17tjvq8n5B7STf3yWfqj38EwAAVI/FqOaZvQ0aNFCLFi1ksVhKB/y/fw3DkMViUX5+fvWr9CIFBQWyWq2y2WwKCgqqle88ceKEmjRpIkk6fvy4AgMDy22rT+r7+gOAt3Dn72a1Z2pKSkqqOwQAAEC1cUdhAABgCm4JNW+++aZ69Oihxo0bKzAwUFFRUUpNTXXH0N5rVZyU/ainqwAAoN6o9uGnN998U7Nnz9aLL76onj17yjAMZWVl6a9//asMw9DYsWPdUaf3OfqNdLKHp6sAAKDeqPZMzeuvv64PPvhAffv2VVBQkKxWq/r27av3339fs2bNckeN3ovLuQEAqDXVDjUnT55UeHh4mfbw8HCdPHmyusN7N0INAAC1ptqhplGjRhW+5+/vX93hvRuhBgCAWlPtc2q2b9+uVq1aOTzI0mKxyDAM2Wy26g7v3Qg1AADUmmqHmqysLHXt2tV+0z2cg1ADAECtqXaoGTdunH766Sd1795dcXFxiouLU0xMDE/pbhQs+Qd7ugoAAOqNap9Tk5mZqf3792vatGlq3ry5UlNTFRUVpc6dOyspKckdNXqnwT9KLev5Qz0BAKhFbnlKt5+fn2JjY3XJJZeodevWat26tVauXKkdO3a4Y3gAAIBKVTvUvPjii8rIyNDBgwcVHh6umJgY/elPf1JKSgpXPwEAgFpT7VDz+uuvKzAwUIMHD1ZcXJx69eqlZs2auaE0L7d2iHTT556uAgCAeqPaoea7777T0aNHlZmZqY0bN2rmzJk6cuSIOnTooNjYWI0fP94ddXof29eergAAgHrFLefUNGvWTAMGDFBcXJwyMzOVkZGht99+W4sXL66/ocavmacrAACgXql2qJk7d64yMjKUkZGh48ePKyYmRjExMVqwYIGio6PdUaN38m3m6QoAAKhXqh1qvvnmG/Xv319TpkxRmzZt3FGTOTBTAwBArXLL1U8oB3cTBgCgVrkcao4ePap58+YpLy9PkZGR6t69u7p166bAwEB31ue9OPwEAECtcjnUDBs2TDt27NBVV12ljz/+WN9//71KSkrUtm1bde/eXe+++6476/Q+PCIBAIBa5XKo2bRpk9auXWs/GbiwsFA7d+7U9u3btX37drcV6LW6/M3TFQAAUK+4HGq6du2qBg3+9+gof39/9ezZUz179nRLYQAAAM5w+YGWzz//vJ588kmdOnXKnfUAAAC4xOVQExkZqWPHjqlTp0567LHHtHz5cuXk5LizNu9m+87TFQAAUK+4HGpuueUW7du3T/369dOXX36ppKQkRUZG6qKLLtJ1113nzhq9k1Hska89cOBAldrqk/q+/gBQX7h8Ts0333yjzMxMXXnllfa2nJwcZWVlKTs72x21ebdavPneggUL7H936tRJb7zxhgoLC8u0JSUl1VpNnlbeNqlP6w8A9ZHFMAzDlQ9ee+21SklJUXx8vLtr8moFBQWyWq2yHc5VUIvQGv++/fv3KyIiQiUlJfY2Hx8flZSU6Nz/tD4+Ptq7d68uueSSGq/J0yraJvVl/QHAm9h/N202BQUFVWsslw8/TZw4UVOmTNGRI0eqVUB1zJo1S5GRkWrUqJGioqK0fv36C/Zfu3atoqKi1KhRI7Vt21Zz5swp0yctLU2dO3eWv7+/OnfurGXLlrlWnE+Aa59z0u7dux1+vCWpuLhY52fV4uJi/fDDD7VSk6dVtE3qy/oDQH1VrXNqPv30U7Vv315/+ctf9MYbb2jz5s0Ohz1q0pIlSzRx4kQ9/vjjysrKUu/evTVw4MAKT1bes2eP/vCHP6h3797KysrSY489pgceeEBpaWn2PhkZGRoxYoQSExO1fft2JSYmavjw4dq0aZPzBVosrq6aU9q3b+9wab1UOithOe/7fXx81K5du1qpydMq2ib1Zf0BoN4yXLR3717jgw8+MKZOnWoMHTrUaNu2rdGgQQPD19fXuOKKK1wdtsp69epljB8/3qGtY8eOxqOPPlpu/0ceecTo2LGjQ9vdd99txMbG2l8PHz7cGDBggEOf/v37GyNHjqxyXTabzZBk2Gy2Kn+mul577TVDkiHJ8PHxMd58881y2+qT+r7+AOAt3Pm76fI5NeU5duyYsrOz9dVXX+m+++5z17BlFBUVqXHjxnrvvfc0dOhQe/uDDz6o7OxsrV27tsxn+vTpox49euill16yty1btkzDhw/XyZMn5evrqzZt2uihhx7SQw89ZO/z4osvaubMmfr555/LraWwsNBhdqqgoEDh4eFuOTZYVSdOnFCTJk0kSbt27dLll19eblt9Ut/XHwC8hUfPqXnsscf05Zdflvte06ZN1bt37xoNNJJ06NAhFRcXKyQkxKE9JCREeXl55X4mLy+v3P5nzpzRoUOHLtinojElKSUlRVar1b6Eh4e7skpuc/HFF1eprT6p7+sPAPWF06EmNzdXgwYNUlhYmO666y599NFHtXYezfnOP2/EMIwybZX1P7/d2TEnT54sm81mX/bt21fl+gEAgPs4HWreeust/fLLL3r33XfVrFkzPfzwwwoODtawYcOUmppqn/WoScHBwfLx8Skzg5Kfn19mpuWs0NDQcvs3bNhQF1100QX7VDSmVPrMq6CgIIcFAADUPpeufrJYLOrdu7deeOEFfffdd/ryyy8VGxuruXPn6uKLL1afPn30r3/9q8bu5Orn56eoqCilp6c7tKenp1d435y4uLgy/VetWqXo6Gj5+vpesA/34gEAoO5z+Y7C5+rUqZM6deqkRx55RL/++qs+/PBDLV++XJI0adIkd3xFGcnJyUpMTFR0dLTi4uL0xhtvKCcnR+PHj5dUeljowIEDWrhwoSRp/PjxevXVV5WcnKxx48YpIyND8+bN0zvvvGMf88EHH1SfPn30/PPP6+abb9by5cv16aefasOGDTWyDgAAwH3cEmrOderUKW3YsMEeamrKiBEjdPjwYU2bNk25ubnq2rWrVq5cqYiICEml5/6ce8+ayMhIrVy5Ug899JBee+01tW7dWi+//LJuueUWe5/4+HgtXrxYTzzxhJ588klddtllWrJkiWJiYmp0XQAAQPW59ZJuSdq+fbt69uyp4mLPPNDR09x5aVpVnXv58vHjxxUYGFhuW31S39cfALyFO383nZ6pWbFixQXf/+mnn1wuBgAAwFVOh5ohQ4bIYrGUebbQuS50CTQAAEBNcPrqp7CwMKWlpamkpKTcZdu2bTVRJwAAwAU5HWqioqIuGFwqm8UBAACoCU4ffvrrX/+qEydOVPh+u3bttHr16moVBQAA4CynQ03v3r0v+H5gYKCuvfZalwsCAABwhUt3FAYAAKhrCDUAAMAUCDUAAMAUCDUAAMAUCDUAAMAUCDUAAMAUnLqkOzk5ucp9Z8yY4XQxAAAArnIq1GRlZTm83rp1q4qLi9WhQwdJ0vfffy8fHx9FRUW5r0IAAIAqcCrUnHun4BkzZqhp06ZasGCBmjdvLkk6cuSIxo4dW+kN+gAAANzN5XNqpk+frpSUFHugkaTmzZvrmWee0fTp091SHAAAQFW5HGoKCgr0yy+/lGnPz8/XsWPHqlUUAACAs1wONUOHDtXYsWO1dOlS7d+/X/v379fSpUuVlJSkYcOGubNGAACASjn9QMuz5syZo0mTJmnUqFE6ffp06WANGyopKUn//Oc/3VYgKhcYGCjDMCptq0/q+/oDQH1kMar5v/wnTpzQjz/+KMMw1K5dOwUGBrqrNq9UUFAgq9Uqm82moKAgT5cDAECd5s7fzWrdfG/9+vW6++67NX78eAUHByswMFBvv/22NmzYUK2iAAAAnOVyqElLS1P//v0VEBCgbdu2qbCwUJJ07NgxPfvss24rEAAAoCpcDjXPPPOM5syZo7lz58rX19feHh8fr23btrmlOAAAgKpyOdTs2rVLffr0KdMeFBSko0ePVqcmAAAAp7kcasLCwvTDDz+Uad+wYYPatm1braIAAACc5XKoufvuu/Xggw9q06ZNslgsOnjwoBYtWqRJkybp3nvvdWeNAAAAlXL5PjWPPPKIbDab+vXrp1OnTqlPnz7y9/fXpEmTNGHCBHfWCAAAUKlq36fm5MmT+uabb1RSUqLOnTurSZMm7qrNK3GfGgAAqs6dv5suz9Tk5OQoPDxcjRs3VnR0dJn32rRpU63CAAAAnOHyOTWRkZH69ddfy7QfPnxYkZGR1SoKAADAWS6HGsMwZLFYyrQfP35cjRo1qlZRAAAAznL68FNycrIkyWKx6Mknn1Tjxo3t7xUXF2vTpk3q3r272woEAACoCqdDTVZWlqTSmZodO3bIz8/P/p6fn5+6deumSZMmua9CAACAKnA61KxevVqSNHbsWL388stq2rSp24sCAABwlsvn1LRv317vvfdemfb58+fr+eefr1ZRAAAAznI51Lzxxhvq2LFjmfYuXbpozpw51SoKAADAWS6Hmry8PIWFhZVpb9mypXJzc6tVFAAAgLNcDjXh4eH64osvyrR/8cUXat26dbWKAgAAcJbLdxS+8847NXHiRJ0+fVrXXXedJOmzzz7TI488oocffthtBQIAAFRFtR5o+dtvv+nee+9VUVGRJKlRo0b629/+psmTJ7utQAAAgKqo9gMtjx8/rm+//VYBAQFq3769/P393VWbV+KBlgAAVF2deKDlWU2aNNFVV11V3WEAAACqxalQk5ycrKefflqBgYH2xyVUZMaMGdUqDAAAwBlOhZqsrCydPn3a/ndFynvQJQAAQE2q9jk1cMQ5NQAAVJ07fzddvk8NAABAXeL0OTVVxTk1AACgNjl9Ts25tm7dquLiYnXo0EGS9P3338vHx0dRUVHuq7AcR44c0QMPPKAVK1ZIkgYPHqxXXnlFzZo1K7f/6dOn9cQTT2jlypX66aefZLVadcMNN+i5555zuPtx3759tXbtWofPjhgxQosXL66xdQEAAO7hVKhZvXq1/e8ZM2aoadOmWrBggZo3by6pNGyMHTtWvXv3dm+V57n99tu1f/9+ffLJJ5Kku+66S4mJifrwww/L7X/y5Elt27ZNTz75pLp166YjR45o4sSJGjx4sLZs2eLQd9y4cZo2bZr9dUBAQM2tCAAAcBuXTxS++OKLtWrVKnXp0sWh/euvv1ZCQoIOHjzolgLP9+2336pz587KzMxUTEyMJCkzM1NxcXH67rvv7LNGldm8ebN69eqln3/+WW3atJFUOlPTvXt3zZw50+X6OFEYAICqqxMnChcUFOiXX34p056fn69jx45Vq6gLycjIkNVqtQcaSYqNjZXVatXGjRurPI7NZpPFYilzyGrRokUKDg5Wly5dNGnSpErXpbCwUAUFBQ4LAACofS7fUXjo0KEaO3aspk+frtjYWEmlMyZ//etfNWzYMLcVeL68vDy1atWqTHurVq2Ul5dXpTFOnTqlRx99VLfffrtDKvzzn/+syMhIhYaG6uuvv9bkyZO1fft2paenVzhWSkqKpk6d6vyKAAAAt3I51MyZM0eTJk3SqFGj7Dfka9iwoZKSkvTPf/7T6fGmTJlSaTjYvHmzpPJv7mcYRpVu+nf69GmNHDlSJSUlmjVrlsN748aNs//dtWtXtW/fXtHR0dq2bZt69uxZ7niTJ092uCqsoKBA4eHhldYBAADcy+VQ07hxY82aNUv//Oc/9eOPP8owDLVr106BgYEujTdhwgSNHDnygn0uvfRSffXVV+Ue9vr1118VEhJywc+fPn1aw4cP1549e/T5559XeuyuZ8+e8vX11e7duysMNf7+/vX+IZ4AANQF1Xqg5fr16/X666/rp59+0nvvvafAwEC9/fbbioyM1DXXXOPUWMHBwQoODq60X1xcnGw2m7788kv16tVLkrRp0ybZbDbFx8dX+LmzgWb37t1avXq1Lrrookq/a+fOnTp9+rTCwsKqviIAAMAjXD5ROC0tTf3791dAQIC2bdumwsJCSdKxY8f07LPPuq3A83Xq1EkDBgzQuHHjlJmZqczMTI0bN06DBg1yuPKpY8eOWrZsmSTpzJkz+tOf/qQtW7Zo0aJFKi4uVl5envLy8lRUVCRJ+vHHHzVt2jRt2bJFe/fu1cqVK3XrrbeqR48euvrqq2tsfQAAgHu4HGqeeeYZzZkzR3PnzpWvr6+9PT4+Xtu2bXNLcRVZtGiRrrjiCiUkJCghIUFXXnml3n77bYc+u3btks1mkyTt379fK1as0P79+9W9e3eFhYXZl7NXTPn5+emzzz5T//791aFDBz3wwANKSEjQp59+Kh8fnxpdHwAAUH0uH37atWuX+vTpU6Y9KChIR48erU5NlWrRooX+/e9/X7DPubffufTSS1XZ7XjCw8PL3E0YAAB4D5dnasLCwvTDDz+Uad+wYYPatm1braIAAACc5XKoufvuu/Xggw9q06ZNslgsOnjwoBYtWqRJkybp3nvvdWeNAAAAlXL58NMjjzwim82mfv366dSpU+rTp4/8/f01adIkTZgwwZ01AgAAVMrlZz+ddfLkSX3zzTcqKSlR586d1aRJE3fV5pV49hMAAFXn8Wc/nT59Wv369dP333+vxo0bKzo6Wr169ar3gQYAAHiOS6HG19dXX3/9dZUeSwAAAFAbXD5R+I477tC8efPcWQsAAIDLXD5RuKioSG+++abS09MVHR1d5plPM2bMqHZxAAAAVeVyqPn666/tD3n8/vvvHd7jsBQAAKhtLoea1atXu7MOAACAanH6nJqTJ0/qvvvu08UXX6xWrVrp9ttv16FDh2qiNgAAgCpzOtT8/e9/V2pqqv74xz9q5MiRSk9P1z333FMTtQEAAFSZ04ef3n//fc2bN08jR46UJI0aNUpXX321iouLeZo1AADwGKdnavbt26fevXvbX/fq1UsNGzbUwYMH3VoYAACAM5wONcXFxfLz83Noa9iwoc6cOeO2ogAAAJzl9OEnwzA0ZswY+fv729tOnTql8ePHO9yr5v3333dPhQAAAFXgdKgZPXp0mbZRo0a5pRgAAABXOR1q3nrrrZqoAwAAoFpcfvYTAABAXUKoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApkCoAQAApuCVoebIkSNKTEyU1WqV1WpVYmKijh49esHPjBkzRhaLxWGJjY116FNYWKj7779fwcHBCgwM1ODBg7V///4aXBMAAOAuXhlqbr/9dmVnZ+uTTz7RJ598ouzsbCUmJlb6uQEDBig3N9e+rFy50uH9iRMnatmyZVq8eLE2bNig48ePa9CgQSouLq6pVQEAAG7S0NMFOOvbb7/VJ598oszMTMXExEiS5s6dq7i4OO3atUsdOnSo8LP+/v4KDQ0t9z2bzaZ58+bp7bff1g033CBJ+ve//63w8HB9+umn6t+/f7mfKywsVGFhof11QUGBq6sGAACqwetmajIyMmS1Wu2BRpJiY2NltVq1cePGC352zZo1atWqlS6//HKNGzdO+fn59ve2bt2q06dPKyEhwd7WunVrde3a9YLjpqSk2A+DWa1WhYeHV2PtAACAq7wu1OTl5alVq1Zl2lu1aqW8vLwKPzdw4EAtWrRIn3/+uaZPn67Nmzfruuuus8+y5OXlyc/PT82bN3f4XEhIyAXHnTx5smw2m33Zt2+fi2sGAACqo84cfpoyZYqmTp16wT6bN2+WJFksljLvGYZRbvtZI0aMsP/dtWtXRUdHKyIiQh999JGGDRtW4ecqG9ff31/+/v4XrBsAANS8OhNqJkyYoJEjR16wz6WXXqqvvvpKv/zyS5n3fv31V4WEhFT5+8LCwhQREaHdu3dLkkJDQ1VUVKQjR444zNbk5+crPj6+yuMCAADPqDOhJjg4WMHBwZX2i4uLk81m05dffqlevXpJkjZt2iSbzeZU+Dh8+LD27dunsLAwSVJUVJR8fX2Vnp6u4cOHS5Jyc3P19ddf64UXXnBhjQAAQG3yunNqOnXqpAEDBmjcuHHKzMxUZmamxo0bp0GDBjlc+dSxY0ctW7ZMknT8+HFNmjRJGRkZ2rt3r9asWaObbrpJwcHBGjp0qCTJarUqKSlJDz/8sD777DNlZWVp1KhRuuKKK+xXQwEAgLqrzszUOGPRokV64IEH7FcqDR48WK+++qpDn127dslms0mSfHx8tGPHDi1cuFBHjx5VWFiY+vXrpyVLlqhp06b2z7z44otq2LChhg8frt9//13XX3+9UlNT5ePjU3srBwAAXGIxDMPwdBFmUlBQIKvVKpvNpqCgIE+XAwBAnebO302vO/wEAABQHkINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBa8MNUeOHFFiYqKsVqusVqsSExN19OjRC37GYrGUu/zzn/+09+nbt2+Z90eOHFnDawMAANyhoacLcMXtt9+u/fv365NPPpEk3XXXXUpMTNSHH35Y4Wdyc3MdXn/88cdKSkrSLbfc4tA+btw4TZs2zf46ICDAjZUDAICa4nWh5ttvv9Unn3yizMxMxcTESJLmzp2ruLg47dq1Sx06dCj3c6GhoQ6vly9frn79+qlt27YO7Y0bNy7TFwAA1H1ed/gpIyNDVqvVHmgkKTY2VlarVRs3bqzSGL/88os++ugjJSUllXlv0aJFCg4OVpcuXTRp0iQdO3bsgmMVFhaqoKDAYQEAALXP62Zq8vLy1KpVqzLtrVq1Ul5eXpXGWLBggZo2baphw4Y5tP/5z39WZGSkQkND9fXXX2vy5Mnavn270tPTKxwrJSVFU6dOdW4lAACA29WZmZopU6ZUeDLv2WXLli2SSk/6PZ9hGOW2l2f+/Pn685//rEaNGjm0jxs3TjfccIO6du2qkSNHaunSpfr000+1bdu2CseaPHmybDabfdm3b58Taw0AANylzszUTJgwodIrjS699FJ99dVX+uWXX8q89+uvvyokJKTS71m/fr127dqlJUuWVNq3Z8+e8vX11e7du9WzZ89y+/j7+8vf37/SsQAAQM2qM6EmODhYwcHBlfaLi4uTzWbTl19+qV69ekmSNm3aJJvNpvj4+Eo/P2/ePEVFRalbt26V9t25c6dOnz6tsLCwylcAAAB4VJ05/FRVnTp10oABAzRu3DhlZmYqMzNT48aN06BBgxyufOrYsaOWLVvm8NmCggK99957uvPOO8uM++OPP2ratGnasmWL9u7dq5UrV+rWW29Vjx49dPXVV9f4egEAgOrxulAjlV6hdMUVVyghIUEJCQm68sor9fbbbzv02bVrl2w2m0Pb4sWLZRiGbrvttjJj+vn56bPPPlP//v3VoUMHPfDAA0pISNCnn34qHx+fGl0fAABQfRbDMAxPF2EmBQUFslqtstlsCgoK8nQ5AADUae783fTKmRoAAIDzEWoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApEGoAAIApeGWo+cc//qH4+Hg1btxYzZo1q9JnDMPQlClT1Lp1awUEBKhv377auXOnQ5/CwkLdf//9Cg4OVmBgoAYPHqz9+/fXwBoAAAB388pQU1RUpFtvvVX33HNPlT/zwgsvaMaMGXr11Ve1efNmhYaG6sYbb9SxY8fsfSZOnKhly5Zp8eLF2rBhg44fP65BgwapuLi4JlYDAAC4kcUwDMPTRbgqNTVVEydO1NGjRy/YzzAMtW7dWhMnTtTf/vY3SaWzMiEhIXr++ed19913y2azqWXLlnr77bc1YsQISdLBgwcVHh6ulStXqn///lWqqaCgQFarVTabTUFBQdVaPwAAzM6dv5sN3VRTnbZnzx7l5eUpISHB3ubv769rr71WGzdu1N13362tW7fq9OnTDn1at26trl27auPGjRWGmsLCQhUWFtpf22w2SaX/kQAAwIWd/b10xxxLvQg1eXl5kqSQkBCH9pCQEP3888/2Pn5+fmrevHmZPmc/X56UlBRNnTq1THt4eHh1ywYAoN44fPiwrFZrtcaoM6FmypQp5YaDc23evFnR0dEuf4fFYnF4bRhGmbbzVdZn8uTJSk5Otr8+evSoIiIilJOTU+3/OPVFQUGBwsPDtW/fPg7ZVRHbzHlsM+exzZzHNnOezWZTmzZt1KJFi2qPVWdCzYQJEzRy5MgL9rn00ktdGjs0NFRS6WxMWFiYvT0/P98+exMaGqqioiIdOXLEYbYmPz9f8fHxFY7t7+8vf3//Mu1Wq5Ud2klBQUFsMyexzZzHNnMe28x5bDPnNWhQ/WuX6kyoCQ4OVnBwcI2MHRkZqdDQUKWnp6tHjx6SSq+gWrt2rZ5//nlJUlRUlHx9fZWenq7hw4dLknJzc/X111/rhRdeqJG6AACA+9SZUOOMnJwc/fbbb8rJyVFxcbGys7MlSe3atVOTJk0kSR07dlRKSoqGDh0qi8WiiRMn6tlnn1X79u3Vvn17Pfvss2rcuLFuv/12SaUzK0lJSXr44Yd10UUXqUWLFpo0aZKuuOIK3XDDDZ5aVQAAUEVeGWqeeuopLViwwP767OzL6tWr1bdvX0nSrl277FciSdIjjzyi33//Xffee6+OHDmimJgYrVq1Sk2bNrX3efHFF9WwYUMNHz5cv//+u66//nqlpqbKx8enyrX5+/vr73//e7mHpFA+tpnz2GbOY5s5j23mPLaZ89y5zbz6PjUAAABneeUdhQEAAM5HqAEAAKZAqAEAAKZAqAEAAKZAqHGjWbNmKTIyUo0aNVJUVJTWr1/v6ZLqrClTpshisTgsZ2+SiFLr1q3TTTfdpNatW8tiseiDDz5weN8wDE2ZMkWtW7dWQECA+vbtq507d3qm2Dqism02ZsyYMvtdbGysZ4qtI1JSUnTVVVepadOmatWqlYYMGaJdu3Y59GFfc1SVbca+5mj27Nm68sor7TcljIuL08cff2x/3137GKHGTZYsWaKJEyfq8ccfV1ZWlnr37q2BAwcqJyfH06XVWV26dFFubq592bFjh6dLqlNOnDihbt266dVXXy33/RdeeEEzZszQq6++qs2bNys0NFQ33nijjh07VsuV1h2VbTNJGjBggMN+t3LlylqssO5Zu3at7rvvPmVmZio9PV1nzpxRQkKCTpw4Ye/DvuaoKttMYl871yWXXKLnnntOW7Zs0ZYtW3Tdddfp5ptvtgcXt+1jBtyiV69exvjx4x3aOnbsaDz66KMeqqhu+/vf/25069bN02V4DUnGsmXL7K9LSkqM0NBQ47nnnrO3nTp1yrBarcacOXM8UGHdc/42MwzDGD16tHHzzTd7pB5vkZ+fb0gy1q5daxgG+1pVnL/NDIN9rSqaN29uvPnmm27dx5ipcYOioiJt3bpVCQkJDu0JCQnauHGjh6qq+3bv3q3WrVsrMjJSI0eO1E8//eTpkrzGnj17lJeX57DP+fv769prr2Wfq8SaNWvUqlUrXX755Ro3bpzy8/M9XVKdcvampWcfLsi+Vrnzt9lZ7GvlKy4u1uLFi3XixAnFxcW5dR8j1LjBoUOHVFxcbH845lkhISHKy8vzUFV1W0xMjBYuXKj//ve/mjt3rvLy8hQfH6/Dhw97ujSvcHa/Yp9zzsCBA7Vo0SJ9/vnnmj59ujZv3qzrrrtOhYWFni6tTjAMQ8nJybrmmmvUtWtXSexrlSlvm0nsa+XZsWOHmjRpIn9/f40fP17Lli1T586d3bqPeeVjEuoqi8Xi8NowjDJtKDVw4ED731dccYXi4uJ02WWXacGCBUpOTvZgZd6Ffc45I0aMsP/dtWtXRUdHKyIiQh999JGGDRvmwcrqhgkTJuirr77Shg0byrzHvla+irYZ+1pZHTp0UHZ2to4ePaq0tDSNHj1aa9eutb/vjn2MmRo3CA4Olo+PT5lEmZ+fXyZ5onyBgYG64oortHv3bk+X4hXOXinGPlc9YWFhioiIYL+TdP/992vFihVavXq1LrnkEns7+1rFKtpm5WFfk/z8/NSuXTtFR0crJSVF3bp100svveTWfYxQ4wZ+fn6KiopSenq6Q3t6erri4+M9VJV3KSws1LfffquwsDBPl+IVIiMjFRoa6rDPFRUVae3atexzTjh8+LD27dtXr/c7wzA0YcIEvf/++/r8888VGRnp8D77WlmVbbPysK+VZRiGCgsL3buPuekk5npv8eLFhq+vrzFv3jzjm2++MSZOnGgEBgYae/fu9XRpddLDDz9srFmzxvjpp5+MzMxMY9CgQUbTpk3ZXuc4duyYkZWVZWRlZRmSjBkzZhhZWVnGzz//bBiGYTz33HOG1Wo13n//fWPHjh3GbbfdZoSFhRkFBQUertxzLrTNjh07Zjz88MPGxo0bjT179hirV6824uLijIsvvrheb7N77rnHsFqtxpo1a4zc3Fz7cvLkSXsf9jVHlW0z9rWyJk+ebKxbt87Ys2eP8dVXXxmPPfaY0aBBA2PVqlWGYbhvHyPUuNFrr71mREREGH5+fkbPnj0dLu+DoxEjRhhhYWGGr6+v0bp1a2PYsGHGzp07PV1WnbJ69WpDUpll9OjRhmGUXmr797//3QgNDTX8/f2NPn36GDt27PBs0R52oW128uRJIyEhwWjZsqXh6+trtGnTxhg9erSRk5Pj6bI9qrztJcl466237H3Y1xxVts3Y18r6y1/+Yv99bNmypXH99dfbA41huG8fsxiGYbg4cwQAAFBncE4NAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINAAAwBUINANSgvn37ymKxyGKxKDs72942ceLEGv/uMWPG2L/7gw8+qPHvAzyNUAPUE2d/3CpaxowZU6v1rFu3TjfddJNat25d6Y/umDFj9Oijj9ZecW42btw45ebmqmvXrm4Z76abbtINN9xQ7nsZGRmyWCzatm2bXnrpJeXm5rrlOwFv0NDTBQCoHef+uC1ZskRPPfWUdu3aZW8LCAgo85mioiL5+fnVSD0nTpxQt27dNHbsWN1yyy0V9ispKdFHH32kFStW1EgdtaFx48YKDQ1123hJSUkaNmyYfv75Z0VERDi8N3/+fHXv3l09e/aUJFmtVrd9L1DXMVMD1BOhoaH2xWq1ymKxlGnr27evJkyYoOTkZAUHB+vGG2+UJF166aWaOXOmw3jdu3fXlClTJEmGYeiFF15Q27ZtFRAQoG7dumnp0qUXrGfgwIF65plnNGzYsAv2++KLL9SgQQPFxMRIkvbu3SuLxaL3339fffr0UUBAgKKiorR3716tWbNGvXr1UuPGjdWvXz/99ttvrm2sWvbJJ5/IarVq4cKFkirfnoMGDVKrVq2UmprqMM7Jkye1ZMkSJSUl1Wb5QJ1BqAHgYMGCBWrYsKG++OILvf7661X6zBNPPKG33npLs2fP1s6dO/XQQw9p1KhRWrt2bbXrWbFihW666SY1aFD6P1dnz0uZNWuWnn32WWVkZOjw4cNKTEzU888/r9dee01r1qzRjh07NG/evGp/f01bvHixhg8froULF+qOO+6QVPn2bNiwoe644w6lpqbKMAz7WO+9956Kior05z//2SPrAngaoQaAg3bt2umFF15Qhw4d1LFjx0r7nzhxQjNmzND8+fPVv39/tW3bVmPGjNGoUaOqHIouZMWKFbr55pvtr7dv367mzZtr8eLFuuaaa9S9e3f169dPOTk5Wrp0qa666ir16tVLV111lfLy8iRJEydOdKilQ4cOmj59uv11TEyMvvrqKzVs2FA9evRQ586dFRUVpblz5zrUYrFYdO+999pf5+bmysfHxz5j5axZs2Zp/PjxWr58uX0dq7o9//KXv9hnp86aP3++hg0bpubNm7tUD+DtOKcGgIPo6Gin+n/zzTc6deqU/VDVWUVFRerRo0e1avn222+1f/9+h5Nis7OzNXjwYAUHB9vbcnJydNtttykwMNCh7Y9//KMkKTY2Vp988onuvvtuHTlyRIGBgcrIyJAkFRYW6scff1SXLl3UrFkzZWVlSZJ+/vlnDR06VIZh6K677pIktWjRQpmZmSouLpaPj4+WLl2qLl26uLRuaWlp+uWXX7Rhwwb16tXL3l7V7dmxY0fFx8dr/vz56tevn3788UetX79eq1atcqkewAyYqQHg4NxgcFaDBg0cDnNI0unTpyWVnsgrSR999JGys7PtyzfffFPpeTWVWbFihW688UaHk5i3b9+u2NhYh37Z2dn2c24k6dSpU/r+++/VvXt3SVJcXJwyMzMlSZmZmRo8eLAOHjwoSdq2bZu6d+8uHx8fhzEjIiI0ffp0zZo1y95msVjUu3dv+2GgZcuWVXpOUEW6d++uli1b6q233nLYts5sz6SkJKWlpamgoEBvvfWWIiIidP3117tUD2AGhBoAlWrZsqXD1VMFBQXas2ePJKlz587y9/dXTk6O2rVr57CEh4dX63uXL1+uwYMHO3zv3r17HWYsfv75Z/32228ObTt37lRxcbG6desmqTSgHDt2TEeOHFFmZqZiY2MVHh6unJwcZWZmKi4urtzv79mzp8MVYpI0fPhwvfvuuzp48KD8/PwcZoyccdlll2n16tVavny57r//fnu7M9tz+PDh8vHx0X/+8x8tWLBAY8eOlcVicakewAw4/ASgUtddd51SU1N10003qXnz5nryySftMxtNmzbVpEmT9NBDD6mkpETXXHONCgoKtHHjRjVp0kSjR48ud8zjx4/rhx9+sL/es2ePsrOz1aJFC7Vp00b5+fnavHmzw/1rtm/frgYNGujKK6+0t2VnZ6tZs2a69NJLHfq1bdtWTZs2tbfFxMRo06ZN2rRpkx588EF98803ysjIUGZmZoU1nj87JUnx8fG6//77tXjxYv3pT3/SqVOnqrQNy3P55Zdr9erV6tu3rxo2bKiZM2c6tT2bNGmiESNG6LHHHpPNZqv1ew0BdQ2hBkClJk+erJ9++kmDBg2S1WrV008/bZ+pkaSnn35arVq1UkpKin766Sc1a9ZMPXv21GOPPVbhmFu2bFG/fv3sr5OTkyVJo0ePVmpqqj788EPFxMSoVatW9j7bt29Xx44dHQ5HZWVl2Wdkzu139tDTWbGxscrIyNCRI0fUokULxcTEaOnSpdq0aZPDIaZzZWdnlzlZ2mKxqE+fPnruuef07bff6p133qlwHauiQ4cO+vzzz9W3b1/5+Pho+vTpTm3PpKQkzZs3TwkJCWrTpk21agG8ncUo7/+KAICHDR48WNdcc40eeeQRt4y3fv16jR49Wtdcc40WLlyoU6dOqVu3brJYLPruu+8kScHBwTp06JAkad++fRo6dKjuuece+31fzr6/e/dubdiwQWPHjtWrr76qQ4cOVXgFVN++fdW9e/cy9/mpTRaLRcuWLdOQIUM8VgNQGzinBkCddM011+i2225z23jR0dHav3+//YTiRo0aqWnTpg4nHR89elTdu3dX586dNWTIEI0fP77cG9m1b99eY8eOrfJ3z5o1S02aNNGOHTuqvyJOGD9+vJo0aVKr3wl4EjM1AFCDDhw4oN9//12S1KZNmxp77ER58vPzVVBQIEkKCwsr98o2wEwINQAAwBQ4/AQAAEyBUAMAAEyBUAMAAEyBUAMAAEyBUAMAAEyBUAMAAEyBUAMAAEyBUAMAAEyBUAMAAEzh/wPLVEpFFMPx4wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting results (for WDM model).\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.set_box_aspect(1)\n",
    "ax.set_xlim(0,30)\n",
    "ax.set_ylim(-1,1)\n",
    "\n",
    "ax.plot([0,30],[0,30],'k--', color='orange')\n",
    "ax.errorbar(1/graph_dict[\"true\"], 1/graph_dict[\"pred\"], yerr=graph_dict[\"err\"]/(graph_dict[\"pred\"]**2), fmt='k.')\n",
    "\n",
    "ax.set(xlabel=\"True \"+\"$1/m_\\mathrm{WDM}$\"+\" [keV]\")\n",
    "ax.set(ylabel=\"Predicted \"+\"$1/m_\\mathrm{WDM}$\"+\" [keV]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c98a523-d9c2-400f-ba94-e2fde0b9dfa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
